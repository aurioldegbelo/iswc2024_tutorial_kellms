@prefix bibtex: <http://purl.org/net/nknouf/ns/bibtex#> .
@prefix datacite: <http://purl.org/spar/datacite/> .
@prefix dblp: <https://dblp.org/rdf/schema#> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix ex: <http://example.org/> .
@prefix litre: <http://purl.org/spar/literal/> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix schema1: <http://schema.org/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<https://dblp.org/rec/journals/corr/abs-2401-07237.ttl> rdfs:label "provenance information for RDF data of dblp record 'journals/corr/abs-2401-07237'" ;
    dcterms:creator <https://dblp.org> ;
    dcterms:isPartOf <https://dblp.org/rdf/dblp.ttl> ;
    dcterms:license <https://creativecommons.org/publicdomain/zero/1.0/> ;
    dcterms:modified "2024-10-06T21:24:37+0200" .

<https://dblp.org/rec/journals/corr/abs-2407-10430.ttl> rdfs:label "provenance information for RDF data of dblp record 'journals/corr/abs-2407-10430'" ;
    dcterms:creator <https://dblp.org> ;
    dcterms:isPartOf <https://dblp.org/rdf/dblp.ttl> ;
    dcterms:license <https://creativecommons.org/publicdomain/zero/1.0/> ;
    dcterms:modified "2024-08-15T11:21:01+0200" .

<https://dblp.org/rec/journals/corr/abs-2407-16127.ttl> rdfs:label "provenance information for RDF data of dblp record 'journals/corr/abs-2407-16127'" ;
    dcterms:creator <https://dblp.org> ;
    dcterms:isPartOf <https://dblp.org/rdf/dblp.ttl> ;
    dcterms:license <https://creativecommons.org/publicdomain/zero/1.0/> ;
    dcterms:modified "2024-08-19T21:11:18+0200" .

<https://dblp.org/rec/journals/corr/abs-2407-18752.ttl> rdfs:label "provenance information for RDF data of dblp record 'journals/corr/abs-2407-18752'" ;
    dcterms:creator <https://dblp.org> ;
    dcterms:isPartOf <https://dblp.org/rdf/dblp.ttl> ;
    dcterms:license <https://creativecommons.org/publicdomain/zero/1.0/> ;
    dcterms:modified "2024-08-24T12:32:22+0200" .

<https://dblp.org/rec/journals/corr/abs-2407-19998.ttl> rdfs:label "provenance information for RDF data of dblp record 'journals/corr/abs-2407-19998'" ;
    dcterms:creator <https://dblp.org> ;
    dcterms:isPartOf <https://dblp.org/rdf/dblp.ttl> ;
    dcterms:license <https://creativecommons.org/publicdomain/zero/1.0/> ;
    dcterms:modified "2024-08-21T20:53:19+0200" .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk1> ex:chunkContent """# Distilling Event Sequence Knowledge From Large Language Models

Somin Wadhwa^1,2^, Oktie Hassanzadeh^1^, Debarun Bhattacharjya^1^,
Ken Barker^1^, and Jian Ni^1^

^1^ IBM Research, USA
^2^ Northeastern University, USA
wadhwa.s@northeastern.edu,{hassanzadeh, debarunb, kjbarker}@us.ibm.com

## Abstract

Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex structured knowledge from pattern mining and probabilistic event models. We release our sequence generation code and evaluation framework, as well as corpus of event sequence data.

**Keywords:** Knowledge Graphs, Large Language Models, Knowledge Distillation

## 1 Introduction""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk2> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk1> ex:chunkContent """# Expanding the Scope:
# Inductive Knowledge Graph Reasoning with
# Multi-Starting Progressive Propagation

Zhoutian Shao¹, Yuanning Cui¹, and Wei Hu¹,²(✉)

¹ State Key Laboratory for Novel Software Technology,
  Nanjing University, Nanjing 210023, China
² National Institute of Healthcare Data Science,
  Nanjing University, Nanjing 210093, China
ztshao.nju@gmail.com, yncui.nju@gmail.com, whu@nju.edu.cn

## Abstract. 
Knowledge graphs (KGs) are widely acknowledged as incomplete, and new entities are constantly emerging in the real world. Inductive KG reasoning aims to predict missing facts for these new entities. Among existing models, graph neural networks (GNNs) based ones have shown promising performance for this task. However, they are still challenged by inefficient message propagation due to the distance and scalability issues. In this paper, we propose a new inductive KG reasoning model, MStar, by leveraging conditional message passing neural networks (C-MPNNs). Our key insight is to select multiple query-specific starting entities to expand the scope of progressive propagation. To propagate query-related messages to a farther area within limited steps, we subsequently design a highway layer to propagate information toward these selected starting entities. Moreover, we introduce a training strategy called LinkVerify to mitigate the impact of noisy training samples. Experimental results validate that MStar achieves superior performance compared with state-of-the-art models, especially for distant entities.

**Keywords:** Knowledge graphs · Inductive reasoning · Conditional message passing.

## 1 Introduction""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk2> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk1> ex:chunkContent """# Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion

Yang Liu¹, Xiaobin Tian¹, Zequn Sun¹(✉), and Wei Hu¹,²

¹ State Key Laboratory for Novel Software Technology,
  Nanjing University, Nanjing 210023, China
² National Institute of Healthcare Data Science,
  Nanjing University, Nanjing 210093, China
{yliu20, xbtian}.nju@gmail.com, {sunzq, whu}@nju.edu.cn

## Abstract

Traditional knowledge graph (KG) completion models learn embeddings to predict missing facts. Recent works attempt to complete KGs in a text-generation manner with large language models (LLMs). However, they need to ground the output of LLMs to KG entities, which inevitably brings errors. In this paper, we present a finetuning framework, DIFT, aiming to unleash the KG completion ability of LLMs and avoid grounding errors. Given an incomplete fact, DIFT employs a lightweight model to obtain candidate entities and finetunes an LLM with discrimination instructions to select the correct one from the given candidates. To improve performance while reducing instruction data, DIFT uses a truncated sampling method to select useful facts for finetuning and injects KG embeddings into the LLM. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed framework.

**Keywords:** Knowledge graph completion · Large language model · Instruction tuning.

## 1 Introduction""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk2> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk1> ex:chunkContent """# Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery

Yuni Susanti1* and Michael Färber2

1 Artificial Intelligence Lab., Fujitsu Ltd., Japan
yuni.susanti@fujitsu.com
2 ScaDS.AI & TU Dresden, Germany
michael.faerber@tu-dresden.de

## Abstract

Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present "KG Structure as Prompt", a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub.1

**Keywords:** causal relation · language model · knowledge graph

## 1 Introduction""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk2> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk1> ex:chunkContent """# Do LLMs Really Adapt to Domains?
## An Ontology Learning Perspective

Huu Tan Mai1,2[0009-0003-6584-4212], Cuong Xuan Chu2, and Heiko
Paulheim1[0000-0003-4386-8195]

1 University of Mannheim, Data and Web Science Group, 68161 Mannheim, Germany
2 Bosch Center for AI, 71272 Renningen, Germany
{huutan.mai,cuongxuan.chu}@de.bosch.com
{huu.tan.mai,heiko.paulheim}@uni-mannheim.de""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk2> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk10> ex:chunkContent """### Label of Interest (LoI): Rise in Unemployment

| Zero-Shot | |
|-----------|--|
| Q: What usually comes after <LoI>? | Complex post-processing of LM output required. |
| A: | |
| Model Output: unemployment follows a economic decline in... | |

| Iterative ICL Few-Shot | |
|------------------------|--|
| Vocabulary: {set of event concepts} | Minimal to no post processing necessary. |
| Using the vocabulary above answer the questions below: | |
| Q: What usually comes after an earthquake? | |
| A: Tsunami | |
| Q: What usually comes after criminal offense? | |
| A: Trial | |
| Q: What usually comes after <LoI>? | |
| A: | |
| Model Output: economic decline | |

Fig. 2. Illustration of our approach to elicit event sequence knowledge given a label of interest. Use of instructional in-context exemplars substantially reduces the need for post-processing LLM output in addition to constraining the output label space.

Number of exemplars We varied the number of in-context exemplars between 1-12. This is in addition to the zero-shot experiments described in the main paper. We evaluated recall (i.e. proportion of references captured by the resulting output sequences) for all generated outputs in an automated way through matching lexical alignment with Wikidata references. We observed none to marginal improvements by varying the number of exemplars beyond 3 in the initial trigger prompt, and beyond 5 in the second iterative prompt.

Selection of specific exemplars Selecting in-context examples is an incredibly noisy process [44]. We started with a static set of randomly selected examples, however owing to Wikidata's inherent label imbalance (political and economic events dominate newsworthy concepts), this led to erratic results, i.e. high recall on similar concepts but not otherwise.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk11> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk11> ex:chunkContent """We then tested a dynamic selection method where every instance of the prompt would contain examples similar to target_label. To achieve this, we used BERT embeddings [12] to retrieve (using cosine distance) examples from the reference set. This approach however led to a lower macro-recall and, upon
---
Distilling Event Sequence Knowledge From Large Language Models           7

manual inspection of outputs, we observed a high degree of redundancy, i.e.
generated outputs were copied from the ICL exemplars.

To solve for these issues, we reverted to the static prompt examples but man-
ually selected a mix of event topics to be included in the prompt. Our current
selection of the prompt yields higher macro-recall than both of the aforemen-
tioned techniques tested. While we believe there may be better techniques to
select ideal candidates for ICL exemplars [4,1] and understand how their com-
positionality affects specific outputs, we consider such an analysis to be beyond
the scope of our work.

We repeat this sequence generation procedure on all 202 event concepts, gen-
erating 2, 276 event sequences with an average length of 5.7 labels per sequence.

## 4 Assessing the Quality of LLM-Generated Event Sequences""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk12> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk12> ex:chunkContent """## 4 Assessing the Quality of LLM-Generated Event Sequences

Open-ended text generation in a task like event sequencing, with extremely
sparse reference data, poses unique challenges to the evaluation of model out-
puts [37]. The traditional scheme to evaluate discrete model outputs has been
to calculate precision and recall for the generated outputs against a predefined
reference test set. However, under open-world settings and particularly when
a sparse KG like Wikidata is used as reference data, a missing causal relation
between two event classes in a sequence may very well be a valid relation. There-
fore, it is not possible to automatically derive an accurate measure of precision
and recall purely using Wikidata as reference data. We take a multi-pronged
evaluation approach to assess the quality and usefulness of the generated event
sequences across multiple tasks. This section presents our approach in evalu-
ating the quality of generated event sequences, along with the results of this
evaluation. An evaluation of the usefulness of the generated event sequences is
presented in Section 5.

### 4.1 Human Evaluation of Cause-Effect Prediction Accuracy of LLMs

To quantify how well model outputs correlate with human assessments, we
first conduct a small-scale human evaluation on a different, independent event-
commonsense reasoning dataset: ATOMIC [21]. ATOMIC consists of event-
centered pairs of instances of the form IsAfter (Y comes after X) and Causes (X
causes Y). We use these instances as input prompts to the model and then ask
human annotators to evaluate model outputs. Specifically, we show human anno-
tators anonymized model outputs and true references and elicit their preferences
given a trigger event.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk13> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk13> ex:chunkContent """To generate outputs, we follow the same strategy as above for a set of 200 ran-
domly selected event-centered input instances (100 each of the type 'X Causes
---
Y' and 'Y IsAfter X')³. We then present the model output and the true refer-
ence from ATOMIC to three human annotators. For example, a typical instance
presented to human evaluators was of the form:

```
Input Instance: PersonX drops out of high school
Response 1: PersonX gets a job
Response 2: PersonX turns PersonX's life around
Type: IsAfter
```

One of the responses above is the LLM output, while the other is the true ref-
erence. The human evaluators are then asked to answer the following questions–

- Are both responses functionally similar?
- Which response do you prefer?
- Which response, if any, is completely irrelevant?

We find that in an overwhelming majority of the cases, the models generate
output that is semantically equivalent to the reference (even though there is no
direct lexical alignment), or output that the humans prefer over the true refer-
ence. Based on the responses from human evaluators⁴, we observe that humans
found 65.82% of response pairs functionally equivalent. That is, even though
not lexically aligned, they meant the same thing. In 27.64% of the instances
the humans preferred the model-generated event instance over the true refer-
ence. In only 6.55% of instances did the humans prefer the true reference over
the model-generated output.⁵ These results reinforce our underlying assertion
that LLMs are capable of event-centered reasoning, and therefore could produce
high-quality event sequence collections.

## 4.2 Evaluation of Recall""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk14> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk14> ex:chunkContent """## 4.2 Evaluation of Recall

Despite the sparsity of causal relations in Wikidata, one can still reason-
ably estimate recall through pairwise comparisons of generated event classes
to the existing causal relations in Wikidata. We build a reference set of
causal relations in Wikidata by curating a list of all the pairs of event
concepts that have any of the several causal relations⁶ in Wikidata in
any direction, including has_cause (P828), has_immediate_cause (P1478),
has_contributing_factor (P1479), and has_effect(P1542). Our results
show that this is an effective mechanism of comparing recall across methods,
as larger models that are expected to have better accuracy yield higher recall
scores.

³ Complete details about ATOMIC are available in Table 1 in [21].
⁴ We observe strong inter-rater agreement with a Fleiss kappa, κ = 0.81; conflicting
  response labels were aggregated through majority vote.
⁵ Additional details on these experiments are available in the supplementary material
  (Appendix).
⁶ www.wikidata.org/wiki/Wikidata:List_of_properties/causality
---
Distilling Event Sequence Knowledge From Large Language Models           9

## 4.3     Evaluation of Precision""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk15> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk15> ex:chunkContent """## 4.3     Evaluation of Precision

To overcome sparsity in reference data, prior work has generally relied on human evaluations [10] for estimating precision, which entails looking at pairs of events and asking annotators whether or not the events in question have a causal relationship. Such a process for potentially thousands of event pairs (like in our case) can be very cost prohibitive. Furthermore, recent research [46,19,47] indicates that pre-trained language models themselves might outperform lay human annotators such as those found on Amazon Mechanical Turk. For instance, [19] demonstrated that labeling data under a few-shot chain-of-thought prompt ("explain-then-annotate" setting) surpasses crowdworker annotations on relevance assessments. Following these results, we use LLMs for evaluating precision and for selecting the most optimal model. We propose evaluating precision for model selection as a binary classification task. Given two events e1, e2, an evaluator model must evaluate whether e1 reasonably leads to e2 under a has_cause or has_effect relationship. To do this, we start with a large instruction-tuned model (in our case, Flan-T5-XXL (11B) [11]) and adopt instructional in-context few shot prompting to classify whether or not e1 reasonably leads to e2, and for the model to provide a justification for its results. While we hypothesize that such an approach may yield noisy results, a small manual assessment as well as our results comparing different models with different evaluator models indicate that this approach yields reliable results for comparing different models, and a reasonable estimate of the overall precision of the model. Note that the evaluator performs only a simple binary classification task that has a very high accuracy even in smaller models.

## 4.4     Settings""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk16> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk16> ex:chunkContent """## 4.4     Settings

We performed all our model inference related experiments on two NVIDIA V100 GPUs. We used the Huggingface library v4.26.1 [40] and publicly available model checkpoints.7 Classical pattern mining algorithms (GSP, SPADE) were implemented in Python through the use of spmf8 library v1.4. For event sequence generation, we use in-context learning under zero shot settings for all LLMs, with top-k sampling (k = 50, in conjunction with top-p, where p = 0.95). To identify influencing events from Summary Markov Models (SuMMs), we use implementations from [6] and split the dataset (generated from LLMs) into train/dev/test sets (70%/15%/15%) to generate results reported in Table 2. BSuMMs and OSuMMs were learned with hyperparameters of α = 0.1, γ = 0.5 and a look-back (κ) window of 4.

## 4.5     Results

Table 1 summarizes our results from these experiments. While prior work as proven the concept of LLM-as-a-judge [47] when much larger LLMs like GPT-4 are used, here we use less resource-intensive LLMs not for evaluation of the

7 huggingface.co/docs/transformers/model_doc/flan-t5
8 pypi.org/project/spmf/
---
10        Wadhwa et al.

| Precision Evaluator Model (P) | R | F-1 |
|--------------------------------|---|-----|
| Flan-T5-Large | Flan-T5-XL | Flan-T5-XXL | | |
|-----------------|-------------|-------------|---|-----|
| Flan-T5-Large (783M) [11] | 0.73 | 0.72 | 0.72 | 0.47 | 0.57 |
| Flan-T5-XL (3B) | 0.75 | 0.75 | 0.78 | 0.49 | 0.60 |
| Open-Research LLaMA (3B) [35] | 0.70 | 0.69 | 0.72 | 0.45 | 0.55 |
| Flan-T5-XXL (11B) | 0.79 | 0.79 | 0.81 | 0.54 | 0.65 |

Table 1. Evaluation of LLM-generated sequences. For the purpose of evaluating recall
(R), we count event pairs (e1, e2) if such a pair exists in Wikidata. For evaluating
precision (P), we treat correctness of all generated event pairs (e1, e2) as a standard
classification task. Best-performing model scores are in bold.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk17> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk17> ex:chunkContent """absolute quality of the generated sequences, but for comparison of the relative
performance of models
    Since we use our models for dual purposes – for generating and evaluating
event sequences, owing to this inherent conflict we find it prudent to indepen-
dently assess these evaluator models. To ensure robustness of our results, we
use multiple evaluator models and find no significant difference between evalu-
ated precision across different models used as evaluators with the largest model
performing marginally better as a precision evaluator.

## 4.6 Discussion

We observed in our analysis that some event triggers did not produce any se-
quences. Out of the 202 unique input event triggers in our data, 8 event triggers
yielded sequences of length 1, i.e., no sequences. This means that the underlying
model could not select a relevant event concept from the provided vocabulary
that might follow the given trigger.

## 5 Knowledge Distillation Through Event Sequence Analysis

Given a high-quality collection of event sequences generated by utilizing LLMs,
we use pattern mining to discover high-support sequence patterns not directly
derivable from the knowledge graph, and learn probabilistic models to discover
complex event sequence rules.

### 5.1 Mining New Patterns

In order to derive new, unseen relationships (has_cause or has_effect) between
the extracted event classes, we use the generated event sequence collection fol-
lowed by classical frequent itemset mining algorithms like GSP [34] and SPADE
[43] to derive new high support patterns. This aspect of the overall workflow is
---
Distilling Event Sequence Knowledge From Large Language Models          11

illustrated in the middle section of Figure 1, where high-support patterns are
mined from event sequences.

The classical pattern mining algorithms we use to mine for new event patterns
are both Apriori-based approaches. Under both methods, given two sequences
of the same event concept class""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk18> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk18> ex:chunkContent """The classical pattern mining algorithms we use to mine for new event patterns
are both Apriori-based approaches. Under both methods, given two sequences
of the same event concept class

$$α =< a_1, a_2, .., a_n > \\text{ and } β =< b_1, b_2, .., b_n >$$              (2)

α is a subsequence of β denoted as α ⊆ β iff there exists a set of values 1 ≤
j1 < j2 < ... < jn ≤ m such that a1 ⊆ bj1, a2 ⊆ bj2, ..., an ⊆ bjn,; then β is
a supersequence of α. Given multiple such sequences and a support threshold,
the task here is to find a set of frequent event subsequences. GSP is depicted in
Algorithm 1). SPADE uses a "vertical database format", which stores sequences
as lists of itemsets associated with their IDs (referred to as "id-lists"), which
allows faster support counting. The algorithm organizes the search space into
equivalence classes, and avoids multiple scans of the sequences.

| Algorithm 1 Generalized Sequential Pattern (GSP) Mining Algorithm |
|-------------------------------------------------------------------|
| 1: Input: Database of sequences D, minimum support threshold min_sup |
| 2: Output: Set of all sequential patterns SP |
| 3: SP ← ∅ |
| 4: C1 ← set of all individual items in D that meet min_sup |
| 5: L1 ← filter candidates in C1 by min_sup |
| 6: k ← 2 |
| 7: while Lk−1 ≠ ∅ do |
| 8:      Ck ← generate_candidates(Lk−1) |
| 9:      for each sequence s ∈ D do |
| 10:         for each candidate c ∈ Ck do |
| 11:             if c is contained in s then |
| 12:                 increment support count of c |
| 13:             end if |
| 14:         end for |
| 15:     end for |
| 16:     Lk ← filter candidates in Ck by min_sup |
| 17:     SP ← SP ∪ Lk |
| 18:     k ← k + 1 |
| 19: end while |
| 20: return SP |

Examples of Mined Patterns Following are example outputs from applica-
tion of GSP on LLM outputs.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk19> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk19> ex:chunkContent """Examples of Mined Patterns Following are example outputs from applica-
tion of GSP on LLM outputs.

Mined Pattern Example 1
Pattern: Civil Disorder (Q686984) → Democratization (Q1064441)
→ Energy Crises (Q8413663) Support: 5
---
12        Wadhwa et al.

The pattern above occurs with a support value of 5 i.e. at least supported by 5 super-sequences. Empirically, we can find support for such a pattern in history.9 In 1994, the country of South Africa democratized post civil disorder. This led to an increased energy demand over the following decade, eventually culminating in a full blown energy crises starting 2003.

Mined Pattern Example 2

Pattern: Famine (Q168247) → Refugee Crises (Q20898283) → Post Traumatic Stress Disorder(Q202387)
Support: 5

The pattern above again occurs with a support value of 5 i.e. at least supported by 5 super-sequences. Similar to the previous example, evidence supporting such an event tranisiton can be found in real life.10 During the Great Irish Famine people were forced to relocate and flee Ireland causing a refugee crises. A great number of these individuals suffered from mental health crises (e.g. PTSD) due to the events directly associated with the famine.

## 5.2     Identifying Influencing Sets through Summary Markov Models""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk20> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk2> ex:chunkContent """**Keywords:** Knowledge Graphs, Large Language Models, Knowledge Distillation

## 1 Introduction

Building probabilistic models from event sequence data has numerous practical applications across different domains when plentiful high-quality event data is available. For example, in Finance, event models can be used to predict stock market trends and make informed investment decisions. In Healthcare, event models can help identify patterns and correlations in patient data to improve diagnoses and treatment plans. In the field of Cybersecurity, these models can be used to detect and prevent potential cyber attacks by analyzing the sequence of events leading up to a breach. A common characteristic of the data in these domains is that sequences are clearly associated with an entity (e.g., a company, a person, or a device). There are however other domains where such a clean association between events and entities may not be possible. One such application is news event analysis [8,13,18,30]. While various news sources record
---
2         Wadhwa et al.

and describe newsworthy events, it is often not possible to automatically put
together coherent sequences of events, because each event may involve multiple
topics and actors, and many correlated and unrelated events may be occurring
simultaneously or in close proximity

Prior work has addressed this challenge by devising automated mechanisms
for extracting narratives [29,33], topic detection and tracking [2], and timeline
summarization [16]. While these different categories of solutions have been suc-
cessful in a range of applications, the outcome is inherently noisy and not in
the form of structured event sequences useful for the construction of event mod-
els. Prior work has shown little success in automatically turning narratives into
structured sequences. In this paper, we explore a novel mechanism for creating
structured event sequences in such domains, using generative language models.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk3> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk20> ex:chunkContent """## 5.2     Identifying Influencing Sets through Summary Markov Models

Learning about potential influencing events that lead to a given event type in a large set of event sequences is an important aspect of analyzing multivariate event sequences, i.e. events without time stamps, like the ones generated by our models. Classic kth order Markov chains capture these dynamics by modeling the probability of observing a particular event type given the preceding k events in-sequence. The recent family of summary Markov models (SuMMs) [6] generalize other well known Markov models for event sequences by leveraging a function that summarizes historical event occurrences, and identifying the subset of event types that affect the probability of occurrence of event types of interest; this forms the influencing set.

We use the LLM-generated sequences to learn two types of SuMMs: binary SuMMs (BSuMMs) and ordinal SuMMs (OSuMMs). In BSuMMs, it is only the presence or absence of an event in the relevant history that has an effect on the occurrence of other events, while in OSuMMs the order of the events is also taken into account. We refer the reader to Sections 3.3 and 3.4 in [6] for complete formal definitions and methods for learning SuMMs over event sequence collections. Briefly, given a subset of event labels of interest X ∈ L and a set of parameters ΘX = {θx|h}, where θx|h is the probability of a given event label x ∈ X occurring at any position in the sequence given the historical event occurrences h, influencing and non-influencing event sets can be formally defined as label sets U = L\\ U are influencing sets for event labels X such that they minimally determine the probability of observing any particular label of interest xi for a given position i in the sequence.

9 wikipedia.org/wiki/South_African_energy_crisis
10 wikipedia.org/wiki/Great_Famine_(Ireland)
---
Distilling Event Sequence Knowledge From Large Language Models    13""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk21> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk21> ex:chunkContent """9 wikipedia.org/wiki/South_African_energy_crisis
10 wikipedia.org/wiki/Great_Famine_(Ireland)
---
Distilling Event Sequence Knowledge From Large Language Models    13

Evaluation: SuMMs vs LSTMs Following the evaluation strategy implemented in [6], we focus on individual labels of interest and conduct an evaluation around probabilistic prediction. Consequently, we select negative log loss as the evaluation metric. Table 2 summarizes our results on the test set for both BSuMMs and OSuMMs, along with a simple LSTM baseline. We observe that models trained on event sequences generated from larger models (e.g. Flan-T5-XXL) fare better than the ones generated from their smaller counterparts (e.g. Flan-T5-Large). We treat this observation as a proxy for generated event sequence quality. That is, better quality sequences lead to better predictive models.

| (↓) Data Generator Model | BSuMMs | OSuMMs | LSTM |
|--------------------------|--------|---------|-------|
| Flan-T5-Large (783M) [11] | -63.49 | -84.29 | -109.28 |
| Flan-T5-XL (3B) | -63.20 | -92.65 | -121.23 |
| Open-Research LLaMA (3B) [35] | -110.57 | -101.29 | -190.68 |
| Flan-T5-XXL (11B) | -57.99 | -78.64 | -108.89 |

Table 2. Negative log likelihood (lower magnitude is better) averaged over interest labels from LLM-generated (Flan-T5-XXL) event sequences. Lookback window (k) for LSTM was fixed to 5. Best-performing data generator model scores are in bold.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk22> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk22> ex:chunkContent """Qualitative Assessment Figure 1 shows examples of learned influencing sets using BSuMMs for refugee crisis and mass migration events. Two events mass migration and famine are identified as influencing events for refugee crisis. The model indicates that the occurrence of both mass migration and famine together has a 0.91 probability of resulting in refugee crisis as a part of a sequence of events. On the other hand the occurrence of mass migration in the absence of famine has a 0.31 probability of resulting in refugee crisis. BSuMMs and OSuMMs deploy a greedy score-based forward and backward search strategy to efficiently discover the minimal influencing sets. Overall, the discovery of influencing sets from LLM-generated data provides a mechanism of distilling complex symbolic knowledge from the output of LLMs.

We provide two more examples below of influencing sets derived from the application of SuMMs. For each example, we show evidence supporting the accuracy of the extracted knowledge. Overall, these results show the high quality and usefulness of the distilled event sequence knowledge.

Influencing Set Example 1 The following example identifies "Hate Crimes" as a predecessor to the occurrence of "Civil Disorder" related events.

X: Civil Disorder (Q686984)
Parent: Hate Crime (Q459409)
 – P(Civil Disorder|NO Hate Crime) = 0.12
---
14       Wadhwa et al.

- P(Civil Disorder|Hate Crime) = 0.55

In the example above, we see that the likelihood of a civil disorder is greatly
influenced by the occurrence of a hate crime.

Influencing Set Example 2 The following example identifies "Disease Outbreaks"
and "Lockdowns" as precursors to the institution of "Travel Restrictions".
X:Travel Restriction (Q87745167)
Parents: Disease Outbreak (Q3241045), Lockdown (Q6665312)

- P(TR|NO Outbreak, NO Lockdown) = 0.0.0001
- P(TR|Lockdown, NO Outbreak) = 0.0.29
- P(TR|NO Lockdown, Outbreak) = 0.26""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk23> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk23> ex:chunkContent """- P(TR|NO Outbreak, NO Lockdown) = 0.0.0001
- P(TR|Lockdown, NO Outbreak) = 0.0.29
- P(TR|NO Lockdown, Outbreak) = 0.26

Quite intuitively, we see here the likelihood of Travel Restrictions directly
correlate with the institution of Disease Outbreaks and Lockdowns (since the
latter have been lately associated with the former).

Error Analysis We observed that in some cases, the identified influencing
setsfor some event concepts were not correct or, at times, illogical. Qualita-
tively, we observe that this occurs with rare event types that do not appear
often either in the underlying knowledge graph, or in the LLM-generated set of
event sequences. Consequently, SuMMs fail to identify logical influencing sets.
For example, for the event type United States Presidential Impeachment,
BSuMM model identified unequal treaty as the influencing event.

## 6 Conclusions and Future Work""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk24> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk24> ex:chunkContent """## 6 Conclusions and Future Work

In this paper, we presented methods of distilling event sequence knowledge from
large language models. We first presented an approach of generating high-quality
event sequences through knowledge-guided generation using LLMs. We then con-
ducted a three-pronged quantitative and qualitative evaluation of our results.
First, we evaluated LLM-generated event sequences in absolute terms through
manual evaluation and automated evaluation using standard accuracy metrics.
Second, we looked at some qualitative examples of newly identified high sup-
port sequences not already present in our base knowledge graph i.e. Wikidata.
Finally, we gauge how probabilistic models like SuMMs fare when trained on
LLM-generated data. We found that through our carefully-designed elicitation
procedure, LLMs are capable of producing high-quality event sequences that
can be used for the downstream tasks of pattern mining and the construction of
probabilistic event models such as SuMMs.
    This work demonstrates the utility of large language models for extracting
event-related information and helps researchers better navigate the complex in-
teraction between event-related entities. While we have demonstrated some use-
cases for the resulting dataset of event sequences – mining logical rules/patterns
---
Distilling Event Sequence Knowledge From Large Language Models    15

and extracting influencing event types – the resulting datasets in this work and
those from other domains could themselves be a useful resource for researchers,
as one may leverage them suitably to inform or justify decision-making. We make
our code and datasets publicly available for future research. Here, we outline a
number of directions for future work:""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk25> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk25> ex:chunkContent """- In this paper, we used Wikidata as our base source of knowledge. The event
concepts found on Wikidata often appear in the pre-training datasets of
large language models. We did not consider more complex, domain-specific
datasets of non-timestamped events (e.g. Healthcare, Finance). Applying
LLMs to generate domain-specific events may require considerable amounts
of data to fine-tune these models, and collecting such supervision to train
these models at scale may be prohibitively expensive.

- A key element of our evaluation strategy (precision) involves the use of the
same language models that were used to generate the sequences being eval-
uated in the first place. Through qualitative examples and past research on
using LLMs as annotators, we observe that such a strategy yields a noisy but
useful signal for evaluating model performance. However, an ideal and accu-
rate evaluation of model outputs should involve the use of human annotators
and a validation of the use of larger LLMs as a part of LLMs-as-a-judge [47]
evaluation strategy. Furthermore, our use of smaller models was justified in
part due to the simplicity of the precision evaluation task, which can be
posed as a binary classification task. It will be interesting to evaluate the
correctness of post-hoc explanations generated by models for their corre-
sponding output during precision evaluation, as such explanations are key
to the usefulness of the distilled knowledge, particularly in critical decision
support applications.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk26> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk26> ex:chunkContent """- In the context of our domain of interest in this paper (news event analy-
sis), an exciting avenue for future research is to study the effect of utiliz-
ing structured knowledge distilled from LLMs in a pipeline for future event
prediction, and a comparison with the performance of human forecasters.
Recent work [17] claims that LLMs, without symbolic knowledge elicita-
tion, come close and in some cases surpass the ability of a "crowd aggregate
of competitive forecasters" in making accurate forecasts in the form of an-
swering forecast questions on online platforms such as the Good Judgment
Open (https://www.gjopen.com). Two research questions arise: 1) Could
a neurosymbolic approach utilizing distilled symbolic knowledge complement
or outperform a purely LLM-based solution? 2) Would distilled high-quality
structured knowledge provide better explainability and therefore be a more
reliable tool as a part of a human-AI collaboration mechanism for event
forecasting?

Supplemental Material Statement: Our supplementary material (included as a
zip file in our submission) includes code and the prompts used for event sequence
generation and benchmarking, as well as the base KG, and sample output files.
README.md has all the details. Appendix.pdf contains our prompts and human
evaluation details.
---
16         Wadhwa et al.

## References

1. Agrawal, S., Zhou, C., Lewis, M., Zettlemoyer, L., Ghazvininejad, M.: In-context examples selection for machine translation. In: Findings of the Association for Computational Linguistics: ACL 2023. pp. 8857–8873. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.findings-acl.564, https://aclanthology.org/2023.findings-acl.564

2. Allan, J.: Topic Detection and Tracking: Event-Based Information Organization. Springer Publishing Company, Incorporated (2012)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk27> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk27> ex:chunkContent """2. Allan, J.: Topic Detection and Tracking: Event-Based Information Organization. Springer Publishing Company, Incorporated (2012)

3. Allan, J., Carbonell, J.G., Doddington, G., Yamron, J., Yang, Y.: Topic Detection and Tracking Pilot Study Final Report. In: Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, February, 1998. (1 1998). https://doi.org/10.1184/R1/6626252.v1

4. An, S., Lin, Z., Fu, Q., Chen, B., Zheng, N., Lou, J.G., Zhang, D.: How do in-context examples affect compositional generalization? In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 11027–11052. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.618, https://aclanthology.org/2023.acl-long.618

5. Begleiter, R., El-Yaniv, R., Yona, G.: On prediction using variable order markov models. J. Artif. Intell. Res. 22, 385–421 (2004). https://doi.org/10.1613/jair.1491, https://doi.org/10.1613/jair.1491

6. Bhattacharjya, D., Sihag, S., Hassanzadeh, O., Bialik, L.: Summary markov models for event sequences. In: Raedt, L.D. (ed.) Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. pp. 4836–4842. International Joint Conferences on Artificial Intelligence Organization (7 2022). https://doi.org/10.24963/ijcai.2022/670, https://doi.org/10.24963/ijcai.2022/670, main Track""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk28> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk28> ex:chunkContent """7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 1877–1901. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

8. Cekinel, R.F., Karagoz, P.: Event prediction from news text using subgraph embedding and graph sequence mining. World Wide Web 25(6), 2403–2428 (2022), https://doi.org/10.1007/s11280-021-01002-1

9. Chen, M., Zhang, H., Ning, Q., Li, M., Ji, H., McKeown, K., Roth, D.: Event-centric natural language processing. In: ACL (2021)

10. Chiang, C.H., Lee, H.y.: Can large language models be an alternative to human evaluations? In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 15607–15631. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.870, https://aclanthology.org/2023.acl-long.870

11. Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang,
---
Distilling Event Sequence Knowledge From Large Language Models    17

S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E.H.,
Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.V., Wei, J.: Scaling instruction-
finetuned language models (2022)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk29> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk29> ex:chunkContent """12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
    bidirectional transformers for language understanding. In: Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).
    pp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota
    (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.
    org/N19-1423

13. Du, X., Zhang, Z., Li, S., Yu, P., Wang, H., Lai, T., Lin, X., Wang, Z., Liu, I., Zhou,
    B., Wen, H., Li, M., Hannan, D., Lei, J., Kim, H., Dror, R., Wang, H., Regan, M.,
    Zeng, Q., Lyu, Q., Yu, C., Edwards, C., Jin, X., Jiao, Y., Kazeminejad, G., Wang,
    Z., Callison-Burch, C., Bansal, M., Vondrick, C., Han, J., Roth, D., Chang, S.F.,
    Palmer, M., Ji, H.: RESIN-11: Schema-guided event prediction for 11 newsworthy
    scenarios. In: Proceedings of the 2022 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies:
    System Demonstrations. pp. 54–63. Association for Computational Linguistics, Hy-
    brid: Seattle, Washington + Online (Jul 2022). https://doi.org/10.18653/v1/
    2022.naacl-demo.7, https://aclanthology.org/2022.naacl-demo.7

14. Filatova, E., Hatzivassiloglou, V., McKeown, K.: Automatic creation of domain
    templates. In: Proceedings of the COLING/ACL 2006 Main Conference Poster
    Sessions. pp. 207–214. Association for Computational Linguistics, Sydney, Aus-
    tralia (Jul 2006), https://aclanthology.org/P06-2027

15. Fournier-Viger, P., Lin, J.C.W., Kiran, R.U., Koh, Y.S.: A survey of sequential
    pattern mining. Data Science and Pattern Recognition 1(1), 54–77 (2017)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk30> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk3> ex:chunkContent """Large Language Models [7,31,38] have recently become the dominant
paradigm in a range of natural language processing (NLP) tasks [11] and often
beat traditional approaches on a number of challenging tasks, including complex
arithmetic reasoning [22] and open-domain question answering [23]. In this pa-
per, our goal is to examine the capability of LLMs to generate structured event
sequences useful for event analysis. Our aim is to use LLMs directly for genera-
tion of event sequences, as opposed to improving prior extraction methods that
would rely on high-quality and comprehensive sources describing detailed event
sequences for a wide variety of events. Our hypothesis is that LLMs trained on
large corpora have already gathered the required knowledge of plausible event se-
quences and therefore can be suitably guided to produce diverse and high-quality
event sequences. To effectively distill this knowledge, we use event-related con-
cepts in Wikidata [36], a comprehensive general-domain knowledge graph, to
guide the sequence generation. This can be viewed as a novel mechanism for
knowledge-guided text generation [42] and symbolic knowledge distillation [39].
We then use these generated sequences for pattern mining and learning proba-
bilistic event models, as a way of further structuring the underlying knowledge.
Figure 1 shows our overall framework along with examples from our experiments
of patterns mined from an LLM-generated event sequence collection, as a well
as a simple model learned from the collection.

In summary, we make the following contributions:""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk4> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk30> ex:chunkContent """15. Fournier-Viger, P., Lin, J.C.W., Kiran, R.U., Koh, Y.S.: A survey of sequential
    pattern mining. Data Science and Pattern Recognition 1(1), 54–77 (2017)

16. Gholipour Ghalandari, D., Ifrim, G.: Examining the state-of-the-art in news time-
    line summarization. In: Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics. pp. 1322–1334. Association for Computational Lin-
    guistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.122,
    https://aclanthology.org/2020.acl-main.122

17. Halawi, D., Zhang, F., Yueh-Han, C., Steinhardt, J.: Approaching human-level
    forecasting with language models (2024)

18. Hassanzadeh, O., Awasthy, P., Barker, K., Bhardwaj, O., Bhattacharjya, D.,
    Feblowitz, M., Martie, L., Ni, J., Srinivas, K., Yip, L.: Knowledge-based news
    event analysis and forecasting toolkit. In: Proceedings of the Thirty-First Interna-
    tional Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-
    29 July 2022. pp. 5904–5907. ijcai.org (2022), https://doi.org/10.24963/ijcai.
    2022/850

19. He, X., Lin, Z., Gong, Y., Jin, A.L., Zhang, H., Lin, C., Jiao, J., Yiu, S.M., Duan,
    N., Chen, W.: Annollm: Making large language models to be better crowdsourced
    annotators (2023)

20. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
    1735–1780 (nov 1997). https://doi.org/10.1162/neco.1997.9.8.1735, https:
    //doi.org/10.1162/neco.1997.9.8.1735

21. Hwang, J.D., Bhagavatula, C., Le Bras, R., Da, J., Sakaguchi, K., Bosselut, A.,
    Choi, Y.: Comet-atomic 2020: On symbolic and neural commonsense knowledge
    graphs. In: AAAI (2021)

22. Imani, S., Du, L., Shrivastava, H.: MathPrompter: Mathematical reasoning us-
    ing large language models. In: Proceedings of the 61st Annual Meeting of the
---
18        Wadhwa et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk31> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk31> ex:chunkContent """22. Imani, S., Du, L., Shrivastava, H.: MathPrompter: Mathematical reasoning us-
    ing large language models. In: Proceedings of the 61st Annual Meeting of the
---
18        Wadhwa et al.

Association for Computational Linguistics (Volume 5: Industry Track), pp. 37–
42. Association for Computational Linguistics, Toronto, Canada (Jul 2023).
https://doi.org/10.18653/v1/2023.acl-industry.4, https://aclanthology.
org/2023.acl-industry.4

23. Kamalloo, E., Dziri, N., Clarke, C., Rafiei, D.: Evaluating open-domain ques-
    tion answering in the era of large language models. In: Proceedings of the 61st
    Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers). pp. 5591–5606. Association for Computational Linguistics, Toronto,
    Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.307, https:
    //aclanthology.org/2023.acl-long.307

24. Li, M., Zeng, Q., Lin, Y., Cho, K., Ji, H., May, J., Chambers, N., Voss, C.: Con-
    necting the dots: Event graph schema induction with path language modeling.
    In: Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-
    guage Processing (EMNLP). pp. 684–695. Association for Computational Linguis-
    tics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.50,
    https://aclanthology.org/2020.emnlp-main.50

25. Li, S., Zhao, R., Li, M., Ji, H., Callison-Burch, C., Han, J.: Open-domain hier-
    archical event schema induction by incremental prompting and verification. In:
    Proceedings of the 61st Annual Meeting of the Association for Computational Lin-
    guistics (Volume 1: Long Papers). pp. 5677–5697. Association for Computational
    Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.
    acl-long.312, https://aclanthology.org/2023.acl-long.312""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk32> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk32> ex:chunkContent """26. Mabroukeh, N.R., Ezeife, C.I.: A taxonomy of sequential pattern mining al-
    gorithms. ACM Comput. Surv. 43(1) (dec 2010). https://doi.org/10.1145/
    1824795.1824798, https://doi.org/10.1145/1824795.1824798

27. Mannila, H., Toivonen, H., Verkamo, A.I.: Discovery of frequent episodes in event
    sequences. Data Mining and Knowledge Discovery 1, 259–289 (1997)

28. Mooney, C.H., Roddick, J.F.: Sequential pattern mining – approaches and al-
    gorithms. ACM Comput. Surv. 45(2) (mar 2013), https://doi.org/10.1145/
    2431211.2431218

29. Norambuena, B.K., Mitra, T., North, C.: A survey on event-based news narrative
    extraction. ACM Comput. Surv. 55(14s) (jul 2023). https://doi.org/10.1145/
    3584741

30. Radinsky, K., Davidovich, S., Markovitch, S.: Learning causality for news events
    prediction. In: Proceedings of the 21st World Wide Web Conference 2012, WWW
    2012, Lyon, France, April 16-20, 2012. pp. 909–918. ACM (2012), https://doi.
    org/10.1145/2187836.2187958

31. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,
    W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
    transformer. J. Mach. Learn. Res. 21(1) (jan 2020)

32. Raftery, A.: A model for high-order Markov chains. Journal of the Royal Statistical
    Society, Series B 47(3), 528–539 (1985)

33. Santana, B.S., Campos, R., Amorim, E., Jorge, A., Silvano, P., Nunes, S.: A survey
    on narrative extraction from textual data. Artif. Intell. Rev. 56(8), 8393–8435
    (2023). https://doi.org/10.1007/s10462-022-10338-7

34. Srikant, R., Agrawal, R.: Mining sequential patterns: Generalizations and perfor-
    mance improvements. In: Apers, P., Bouzeghoub, M., Gardarin, G. (eds.) Advances
    in Database Technology — EDBT '96. pp. 1–17. Springer Berlin Heidelberg, Berlin,
    Heidelberg (1996)
---
Distilling Event Sequence Knowledge From Large Language Models    19""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk33> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk33> ex:chunkContent """35. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
    Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
    foundation language models. arXiv preprint arXiv:2302.13971 (2023)
36. Vrandečić, D., Krötzsch, M.: Wikidata: A free collaborative knowledgebase. Com-
    mun. ACM 57(10), 78–85 (sep 2014). https://doi.org/10.1145/2629489, https:
    //doi.org/10.1145/2629489
37. Wadhwa, S., Amir, S., Wallace, B.: Revisiting relation extraction in the era of large
    language models. In: Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers). pp. 15566–15589. Association
    for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.
    18653/v1/2023.acl-long.868, https://aclanthology.org/2023.acl-long.868
38. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M.,
    Le, Q.V.: Finetuned language models are zero-shot learners. In: International Con-
    ference on Learning Representations (2022), https://openreview.net/forum?id=
    gEZrGCozdqR
39. West, P., Bhagavatula, C., Hessel, J., Hwang, J., Jiang, L., Le Bras, R., Lu,
    X., Welleck, S., Choi, Y.: Symbolic knowledge distillation: from general language
    models to commonsense models. In: Proceedings of the 2022 Conference of the
    North American Chapter of the Association for Computational Linguistics: Hu-
    man Language Technologies. pp. 4602–4625. Association for Computational Lin-
    guistics, Seattle, United States (Jul 2022). https://doi.org/10.18653/v1/2022.
    naacl-main.341, https://aclanthology.org/2022.naacl-main.341
40. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
    Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma,
    C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q.,""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk34> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk34> ex:chunkContent """Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma,
    C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q.,
    Rush, A.: Transformers: State-of-the-art natural language processing. In: Proceed-
    ings of the 2020 Conference on Empirical Methods in Natural Language Process-
    ing: System Demonstrations. pp. 38–45. Association for Computational Linguistics,
    Online (Oct 2020). https://doi.org/10.18653/v1/2020.emnlp-demos.6, https:
    //aclanthology.org/2020.emnlp-demos.6
41. Xiang, W., Wang, B.: A survey of event extraction from text. IEEE Access 7,
    173111–173137 (2019). https://doi.org/10.1109/ACCESS.2019.2956831
42. Yu, W., Zhu, C., Li, Z., Hu, Z., Wang, Q., Ji, H., Jiang, M.: A survey of knowledge-
    enhanced text generation. ACM Comput. Surv. 54(11s) (nov 2022). https://doi.
    org/10.1145/3512467, https://doi.org/10.1145/3512467
43. Zaki, M.J.: Spade: An efficient algorithm for mining frequent sequences. Ma-
    chine Learning 42, 31–60 (2004), https://api.semanticscholar.org/CorpusID:
    5387869
44. Zhang, Y., Feng, S., Tan, C.: Active example selection for in-context learning.
    In: Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-
    guage Processing. pp. 9134–9148. Association for Computational Linguistics, Abu
    Dhabi, United Arab Emirates (Dec 2022). https://doi.org/10.18653/v1/2022.
    emnlp-main.622, https://aclanthology.org/2022.emnlp-main.622
45. Zhao, L.: Event prediction in the big data era: A systematic survey. ACM Comput.
    Surv. 54(5) (may 2021). https://doi.org/10.1145/3450287, https://doi.org/
    10.1145/3450287
46. Zhao, M., Mi, F., Wang, Y., Li, M., Jiang, X., Liu, Q., Schuetze, H.: LMTurk: Few-
    shot learners as crowdsourcing workers in a language-model-as-a-service frame-
    work. In: Findings of the Association for Computational Linguistics: NAACL 2022.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk35> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk35> ex:chunkContent """shot learners as crowdsourcing workers in a language-model-as-a-service frame-
    work. In: Findings of the Association for Computational Linguistics: NAACL 2022.
    pp. 675–692. Association for Computational Linguistics, Seattle, United States
---
20         Wadhwa et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk36> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk36> ex:chunkContent """(Jul 2022). https://doi.org/10.18653/v1/2022.findings-naacl.51, https://
aclanthology.org/2022.findings-naacl.51

47. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li,
    Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge
    with mt-bench and chatbot arena (2023)
---
# Appendix

This section constitutes technical appendix (i.e. supplementary material) for the submission titled "Distilling Event Sequences from Large Language Models".

## 7 Event Sequence Prompts

We now describe the specific prompt-types used (including pseudo-code for their use) to generate event sequences given individual Wikidata event concepts. We start a Wikidata event concept label and feed it into the following prompt with 3 ICL exemplars. The prompt also includes a label space of other Wikidata concepts from which we instruct the model to choose a concept. In case of an out-of-domain generated output (≈ 15% of total outputs), we discard those outputs.

```python
def build_prompt1(vocab, target_label):
    prompt = "Use the following vocabulary to respond to the questions: " + \\
             f"{''.join(label_space)}\\n" + \\
             f"Question: what usually happens after earthquake?\\n" +\\
             f"Answer: tsunami\\n" + \\

             f"Question: what usually happens after economic crises?\\n" +\\
             f"Answer: unemployment\\n" + \\

             f"Question: what usually happens after bomb attack?\\n" +\\
             f"Answer: injury\\n" + \\

             f"Question: what usually happens after {target_label}?\\n" +\\
             f"Answer: "

    return prompt
```

Listing 1.1. Initial Trigger Prompt""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk37> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk37> ex:chunkContent """f"Question: what usually happens after {target_label}?\\n" +\\
             f"Answer: "

    return prompt
```

Listing 1.1. Initial Trigger Prompt

Following the output from the above prompt trigger, we feed the in-domain outputs to the following ICL iterative prompt with conjunctive event exemplars (i.e. X and Y). We then successively use this prompt by feeding model generated outputs as new event concepts. Restricting the generated vocabulary to existing Wikidata concepts allows for a reasonable recall evaluation even from a sparse reference set. We illustrate this approach in Figure 2 in the main paper.

```python
def build_prompt2(vocab, target_labels):
```
---
22        Wadhwa et al.

```python
prompt = "Use the following vocabulary to respond to the questions: " + \\
         f"{' '.join(label_space)}\\n" + \\
         f"Question: what usually happens after earthquake?\\n" +\\
         f"Answer: tsunami\\n" + \\

         f"Question: what usually happens after earthquake and tsunami?\\n" +\\
         f"Answer: nuclear disaster\\n" + \\

         f"Question: what usually happens after economic crises and wage decline and unemployment?\\n" +\\
         f"Answer: legislation\\n" + \\

         f"Question: what usually happens after military conflict?\\n" +\\
         f"Answer: war\\n" + \\

         f"Question: what usually happens after military conflict and war?\\n" +\\
         f"Answer: peace treaty\\n" + \\

         f"Question: what usually happens after {' and '.join(target_labels)}?\\n" +\\
         f"Answer:"

return prompt
```

Listing 1.2. Iterative ICL Prompt with Conjunctive Examples

## 8 Evaluator Models (Example Outputs and Prompts)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk38> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk38> ex:chunkContent """return prompt
```

Listing 1.2. Iterative ICL Prompt with Conjunctive Examples

## 8 Evaluator Models (Example Outputs and Prompts)

For a model generated event sequence α = a1, a2, .., an, we consider all possible pairs of events (ai, ai+1) as (trigger, consequence) pairs where a0 is the initial trigger sequence and i ∈ 1, .., n are all events generated by the model that follow a0. Then we use the following prompt to evaluate correctness of all such possible pairs and use this as proxy for a true precision evaluation of the generated sequences.

```python
def build_precision_eval(trigger, consequence):
    prompt = "Respond to the questions below with a (YES/NO) with a historical example:" + \\
             f"Question: Can economic crises cause a landslide?\\n" +\\
             f"Answer: NO. There is no historical example of an economic crisis causing a landslide, which is natural disaster.\\n" + \\
```
---
Distilling Event Sequence Knowledge From Large Language Models    23

| Trigger |
|---------|
| PersonX bakes some bread |

| Response 1 | Response 2 |
|------------|------------|
| PersonX eats the bread | PersonX throws some bread |

Questions:
1. Are both responses functionally the same?
   - [ ] Yes
   - [x] No

2. Which response is more realistic ("relevant")?
   - [x] Response 1
   - [ ] Response 2
   - [ ] They're functionally the same/equally relevant

3. Is either of the two responses completely irrelevant? If so, which one.
   - [ ] Response 1
   - [ ] Response 2
   - [x] Neither is irrelevant
   - [ ] Both are irrelevant

[Submit]

Fig. 3. Annotation interface for classifying model generated response vs reference (anonymized) for the ATOMIC event descriptions.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk39> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk39> ex:chunkContent """[Submit]

Fig. 3. Annotation interface for classifying model generated response vs reference (anonymized) for the ATOMIC event descriptions.

```python
f"Question: Can earthquake cause a tsunami?\\n" +\\
f"Answer: YES. In 2011, Japan experienced an earthquake in tohoku that caused a tsunami.\\n" + \\
f"Question: Can mass shooting cause a condensation cloud?\\n" +\\
f"Answer: NO. A condensation cloud is a weather phenomenon, not a mass shooting.\\n" + \\
f"Question: Can accident cause a stock market crash?\\n" +\\
f"Answer: NO. The stock market crash of 1929 was caused by a series of events, not an accident.\\n" + \\
f"Question: Can disease outbreak cause a inventory shrinkage?\\n" +\\
f"Answer: YES. The bubonic plague outbreak in Europe in 1348 caused a massive inventory shrinkage.\\n" + \\
f"Question: Can fraud cause a travel ban?\\n" +\\
f"Answer: YES. Travel bans are a form of punishment for immigration fraud.\\n" + \\
f"Question: Can {trigger} cause a {consequence}?\\n" + "Answer: "
return prompt
```

Listing 1.3. Prompt used to evaluate correctness of a given event pair

As mentioned in the limitations section, a key shortcoming of our approach here is that we do not evaluate the correctness of the post-hoc explanations
---
24       Wadhwa et al.

generated by the model for it's classification label. We leave that analysis for future work.

## 9    ATOMIC Human Evaluations

Amazon Mechanical Turk (AMT) is a platform for non-expert works to perform microtasks, in our case – human annotations. Three authors with graduate degrees in computer science carried out these human evaluations. Figure 3 shows the interface provided to these human annotators. As described in the main paper, human evaluators were asked to answer the following questions–

- Are both responses functionally similar?
- Which response do you prefer?
- Which response, if any, is completely irrelevant?""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk40> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk4> ex:chunkContent """In summary, we make the following contributions:

1. We devise a new iterative in-context prompting strategy for generating high-
   quality event sequences using LLMs. To the best of our knowledge, we are
   the first to use LLMs to generate structured event sequences for the purpose
   of analyzing various event models.
2. We compile high-quality event sequences using our generation mechanism,
   based on a curated set of high-level event concepts (classes) from Wikidata
   that represent newsworthy events.
3. We develop an evaluation framework and conduct experiments to show the
   value of LLM-powered event sequence generation on replicating and aug-
   menting knowledge in structured representations such as knowledge graphs.
4. We further demonstrate the practical usefulness of our approach by leverag-
   ing downstream pattern mining and probabilistic event models.
---
# Distilling Event Sequence Knowledge From Large Language Models



## Fig. 1. An overview of our framework for distilling event sequence knowledge from LLMs, along with examples portraying potential use cases. We show that by (1) starting with a sparse knowledge graph such as Wikidata, we can generate targeted event sequences. Owing to the inherent sparsity in the underlying KG, we can (2) use LLMs to carry out a portion of the evaluation (i.e. precision) to select an optimal model. On this new generated sequence dataset, we then (3) apply classical pattern-mining algorithms (e.g., GSP) to identify potentially interesting has_cause and has_effect event sequence chains, and (4) learn summary Markov models (SuMMs) to identify potential influencing events for particular event types of interest; these are both illustrations of extracting complex structured knowledge from the generated sequences.

Our code and generated sequence data are included in the supplementary material, and will be made publicly available for future research.

## 2 Related Work""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk5> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk40> ex:chunkContent """- Are both responses functionally similar?
- Which response do you prefer?
- Which response, if any, is completely irrelevant?

### Functionally Similar Responses 
For responses to be functionally similar, they must convey the same meaning even if there is major lexical mismatch between the actual tokens.

### Human Preferences 
The preferences elicited here are based on a humans degree of reasonableness given an event trigger. We observed a high Fleiss κ = 0.81 indiciating a high degree of agreement between the annotators.

### Irrelevant Responses 
Here, again the annotators were asked to exercise judgment in what they may find a completely unreasonable response to a given event trigger.

We release the results from these human annotations for a comprehensive analysis and any additional findings that readers may infer.""" .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk5> ex:chunkContent """Our code and generated sequence data are included in the supplementary material, and will be made publicly available for future research.

## 2 Related Work

News Event Analysis The primary applications of our work are around news event analysis and forecasting. Liang [45] presents a taxonomy of different flavors of event prediction in the literature. Our target event prediction applications fall under the "Semantic Prediction" category, with time and location details not being of interest, and the primary goal being the prediction of "event profiles" such as event types. Seminal work in this area is the work of Radinsky et al. [30] where causal relations between past events are extracted from text and then a
---
4         Wadhwa et al.

knowledge graph is utilized to generalize the extracted relations in order to make predictions. More recent work has explored the use of graph sequence mining over a graph structure representation of events extracted from text [8], with graph mining used as a mechanism of extracting useful relations from large and noisy outputs of extraction. The application of building event sequence models over such outputs has not been explored. Our paper explores an alternative approach of event sequence generation that is capable of generating longer and potentially higher-quality sequences.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk6> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk6> ex:chunkContent "Event Sequence Extraction from Text There is a wealth of literature on different methods of extracting sequences from textual corpora. Norambuena et al. [29] present a comprehensive survey of automated methods of narrative extraction. Narratives contain several elements including events, participants (actors/protagonists), time, and space. The task of event detection itself is a highly challenging task and the topic of extensive research [9,41,24], which has its root in the Topic Detection and Tracking (TDT) task [33]. This line of work started out as a DARPA-sponsored initiative with the same name [3]. Another closely related task is news timeline summarization [16]. While pattern mining algorithms have been applied to the output of such extractions, e.g. for creation of \"domain templates\" [14], we are not aware of any attempts to use the extractions to construct event models for analysis or prediction. This is mainly due to the fact that the output of such methods often result in very short and noisy sequences, not suitable for the majority of event sequence models." ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk7> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk7> ex:chunkContent """Sequential Pattern Mining and Event Sequence Models Sequential pattern mining and related approaches have been the subject of extensive research [27,26,28,15]. These algorithms take in a set of sequences (or "sequential records") with a set of unique events (or "items") and often a "minimum support" threshold, and return as output a ranked set of all frequent sequences in a given sequence collection (or database) meeting the minimum support threshold. There is also a long line of work on statistical and probabilistic models for analysis of different kinds of event sequences. Our focus in this paper is on multivariate event sequences, i.e., sequences of various event types without timestamps. Markov models for sequences [32,5] and long short-term memory (LSTM) [20] models are examples of prediction models. We leverage a more recent family of models – Summary Markov models [6] – for some of the experiments in this paper that involve analyzing generated event sequences.

## 3     Knowledge-Guided Event Sequence Generation

Through utilizing LLMs, we model event prediction as a conditional generation task under zero and iterative few-shot settings. Our targets are linearized sequences of event concepts. We begin by prompting a large language model (with in-context exemplars) with an event trigger y1 to generate the next concept from a defined set of labels, and repeat this process until we get a sequence of desired
---
Distilling Event Sequence Knowledge From Large Language Models               5

length. Formally, given an event trigger T, we model the probability of gener-
ating linearized string y of length T containing N unique event concepts that
follow T in sequence:

$$pLM(y|T) = \\prod_{t=2}^T p(y_t|T, y_{<t-1})$$                                           (1)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk8> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk8> ex:chunkContent """$$pLM(y|T) = \\prod_{t=2}^T p(y_t|T, y_{<t-1})$$                                           (1)

This is the standard conditional language modeling objective. We try multiple
prompting techniques and qualitatively observe optimal results with an iterative
in-context few-shot prompting strategy (Figure 2). Specifically, we start with
a set of six randomly selected examples of the form – "What usually follows
event X?". This approach follows the incremental prompting procedure from
[25]. Based on the model output (Y) from a pre-specified vocabulary, we append
this same example to the original prompt in conjunction (i.e. "What usually
follows event X and Y?") with ICL examples of the same form. As shown in
Figure 2, our iterative technique serves dual purposes: (i) it leverages in-context
learning; and (ii) eliminates the need for implementing complex resolvers to
post-process model outputs. We repeat this process until a sequence of a desired
minimum length m is achieved (in our experiments, m = 3) or we've exhausted a
maximum number of tries (k = 10 in our experiments) to generate an in-domain
event type.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk9> .

<https://dblp.org/rec/journals/corr/abs-2401-07237/chunk9> ex:chunkContent """Identifying Event Concepts With incremental prompting we curate a new dataset
of high-level event concepts (classes) from Wikidata that represent newswor-
thy events. To do so, we query Wikidata for event concepts that have links
to Wikinews articles and are instances of classes that are a subclass of the
occurrence class, i.e., indicating they are newsworthy event classes. We gath-
ered 50 top-level classes for these event concepts, each having multiple labels (e.g.
conflict → conflict (psychological), dispute, disagreement, etc.).
This yielded a total of 202 unique event labels for the 50 top-level classes. Most of
these event concepts have some causal relations (i.e. has_cause or has_effect).
We use these relation pairs as in-context exemplars to create our prompts. We
then generate event sequences through iterative in-context prompting (Figure 2).
Full length prompts used in all our experiments are provided in the Appendix.

To generate new event sequences in a zero-shot setting, we start with an
event trigger (e.g. a concept like workplace accident), create a prompt with
instructions describing desired relationships and a few in-context exemplars (ICL
prompt), and constrained the model output to the original 50 event concept la-
bels to generate the next event in the sequence (full text of the prompt is provided
in the supplementary material). We append the model output to another ICL
prompt with conjunctive event examples (i.e. questions of the form "What hap-
pens after X, Y, and Z"). We repeat this process until we reach a pre-defined
maximum sequence length (in this case, 10) or until the model fails to generate
an in-vocabulary response in k maximum attempts (in this case, k = 3). In this
process, we test the following two ablations –
---
6         Wadhwa et al.

### Label of Interest (LoI): Rise in Unemployment""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk10> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk10> ex:chunkContent """```mermaid
graph TD
    A[Starting Entities Selection SES] --> B[Highway Layer]
    B --> C[Initialization]
    
    subgraph A
        D[Pre-Embedded GNN] --> E[Pre-Embeddings]
        F[e.g. n=6, m=3] --> G[Selection]
    end
    
    subgraph B
        H[Conditional Embeddings] --> I[Efficient Propagation]
        J[Types] --> K[Added]
    end
    
    subgraph C
        L[Multi-Condition GNN]
    end
    
    M[InputQuery] --> N[query ?]
    N --> O[head] & P[tail]
    
    Q[Decoder] --> R[MLP x Embed entity scores]
```

Fig. 2. Framework overview of MStar

transmits messages among all entities at all times. Progressive propagation con-
tinuously incorporates the neighbor entities of the entity set in the previous step.
Based on progressive propagation, we use starting entities to indicate the entities
involved in the first layer of the GNN. Given the starting entities S, the entities
involved in the ℓth layer of the GNN can be formulated as

$$
V^ℓ = \\begin{cases}
S & ℓ = 0 \\\\
V^{ℓ-1} \\cup \\{x | \\exists(e, r, x) \\in N(e) \\wedge e \\in V^{ℓ-1}\\} & ℓ > 0
\\end{cases}
$$

where N(e) denotes the neighbor edges of the entity e. In particular, NBFNet
puts all the entities into S, i.e., S = E. RED-GNN only puts the head entity into
S, i.e., S = {u} with given query (u, q, ?). Too few starting entities limit the scope
of conditional message passing. On the contrary, too many start entities disperse
the attention of GNNs on local information which is critical for reasoning. Our
model MStar strikes a balance by including the head entity and some selected
query-dependent starting entities that are helpful for reasoning.

## 4 Methodology

### 4.1 Model Architecture Overview""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk11> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk11> ex:chunkContent """## 4 Methodology

### 4.1 Model Architecture Overview

The overview of MStar is presented in Fig. 2. Specifically, we first employ the pre-
embedded GNN to pre-encode all entities. Then, SES selects n query-dependent
starting entities according to the pre-embeddings. The highway layer classifies
starting entities into m types, considering the correlation between the head entity
and other starting entities. To improve message-passing efficiency, the highway
layer maps each entity type into a new relation and constructs shortcut edges
between the head entity and other starting entities. Based on the message pass-
ing on the shortcut edges, we use the highway layer to obtain conditional entity
---
Inductive Knowledge Graph Reasoning with MStar              7

embeddings as the initialization for multi-condition GNN. Finally, the multi-
condition GNN propagates relational information progressively conditioned on
these starting entities and generates pairwise embeddings of each entity. Ac-
cording to the final entity embeddings, the decoder operates as a multilayer
perceptron (MLP) and generates scores for each candidate entity.

## 4.2 Starting Entities Selection

As shown in Fig. 1, progressive propagation starts from the only entity (head
entity) and cannot reach the distant entities. However, the excessive utilization
of starting entities introduces noisy relational paths into the reasoning. Despite
the expansion of the propagation, some starting entities still miss the target
entities and visit other distant entities unrelated to the query. Thus, we propose
to select multiple query-dependent starting entities adaptively to cover a farther
area but not introduce irrelevant noise in reasoning.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk12> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk12> ex:chunkContent """### Pre-Embedded GNN 
To find the starting entities related to the query, we
first introduce a pre-embedded GNN to learn the simple semantics of the enti-
ties. The pre-embedded GNN transmits messages among all entities in the KG
following the full propagation paradigm. To explore query-related knowledge,
the pre-embedded GNN encodes the relation conditioned on query relation q.
Specifically, the computation for message passing is given by

$$h^ℓ_{pre|u,q}(e) = \\frac{1}{|N(e)|} \\sum_{(e,r,x)∈N(e)} (h^{ℓ-1}_{pre|u,q}(x) + \\hat{r}_q),$$

$$\\hat{r}_q = W_r q + b_r,$$

where $h^ℓ_{pre|u,q}(e)$ denotes the embedding of the entity e in propagation step ℓ,
q is a learnable embeddings for relation q, $W_r ∈ R^{d×d}$ is an r-specific learnable
weight matrix, and $b_r ∈ R^d$ is an r-specific learnable bias. d is the dimension
of both entity and relation embeddings. $\\hat{r}_q$ denotes the embedding of relation r
conditioned on q. The pre-embedded GNN initializes $h^0_{pre|u,q}$ as zero vectors and
produces the entity embeddings $h^{L_1}_{pre|u,q}$ after $L_1$ layers of message passing.

### Selection 
Provided with the embeddings of entities conditioned on u and q,
we design a score function to select query-dependent starting entities. The score
function measures the importance of entities relative to the head entity and
query relation. Given an entity e, the importance score $α_{e|u,q}$ is defined as

$$α_{e|u,q} = W_1 (ReLU(W_2 (h^{L_1}_{pre|u,q}(e) ⊕ h^{L_1}_{pre|u,q}(u) ⊕ q))),$$

where $W_1 ∈ R^{1×d}$ and $W_2 ∈ R^{d×3d}$ are learnable weight matrices. ⊕ denotes
the concatenation of two vectors. We keep the top-n entities as starting entity
set $S_{u,q}$. $S_{u,q}$ can propagate along the relational paths conditioned on the query.
---

## 4.3 Highway Layer""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk13> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk13> ex:chunkContent """## 4.3 Highway Layer

Given multiple starting entities, progressive propagation can traverse more entities, particularly those located at distant positions. The distant entities, however, receive nothing about the conditional information, due to the limited scope of conditional message passing. Inspired by the skip connection [8], which allows skip-layer feature propagation, we introduce a highway layer to tackle this issue.

Aiming to propagate conditional information to the starting entities, we consider constructing shortcut edges between the query head entity and the other starting ones. Due to the different semantics of the starting entities, we classify entities into m types based on the embeddings. Each type indicates that this group of entities has a specific semantic relationship with the head entity. Then, we map each entity type to a new semantic relation type and construct new edges. Given conditions u, q and entity e, the entity type is defined as follows:

$$\\beta_{e|u,q} = \\arg\\max_t \\mathbf{W}_t h_{pre|u,q}^{L_1}(e), \\quad t \\in [1, m],$$

where t is a type of starting entities, and $\\mathbf{W}_t \\in \\mathbb{R}^{1\\times d}$ is a t-specific learnable weight matrix.

Given starting entity types, the highway layer constructs shortcut edges as

$$\\mathcal{H}_{u,q} = \\{(u, r'_{\\beta_{e|u,q}}, e) | e \\in \\mathcal{S}_{u,q} - \\{u\\}\\},$$

where $r'_{\\beta_{e|u,q}}$ denotes the new relation that we introduce, corresponding to the starting entity type. These edges act as a skip connection to support skipping propagation from the head to the starting entities.

Finally, the highway layer performs message passing on $\\mathcal{H}_{u,q}$ to obtain the embeddings of the selected starting entities:

$$g_{u,q}(e) = \\sum_{(e,r,x)\\in \\mathcal{N}_{highway}(e)} g_{u,q}(x) \\odot \\hat{r}_q,$$""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk14> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk14> ex:chunkContent """$$g_{u,q}(e) = \\sum_{(e,r,x)\\in \\mathcal{N}_{highway}(e)} g_{u,q}(x) \\odot \\hat{r}_q,$$

where $g_{u,q}(e)$ denotes the embedding of entity e, $\\mathcal{N}_{highway}(e)$ denotes the neighbor edges of the entity e in set $\\mathcal{H}_{u,q}$, and $\\odot$ denotes the point-wise product between two vectors. To satisfy target entity distinguishability [10], we set a learnable embedding for the head entity u.

## 4.4 Multi-Condition GNN

In MStar, we introduce a multi-condition GNN to produce the final entity embeddings. The multi-condition GNN is a C-MPNN conditioned on the head entity and query relation. Specifically, the multi-condition GNN initializes entity embeddings $h_{u,q}^0$ as $g_{u,q}$ and propagates from the starting entities progressively. Given the query triplet (u, q, ?), we incorporate the query information into propagation in two ways.

First, we model the embedding of relation r in an edge as $\\hat{r}_q$ conditioned on the query relation q same as Eq. (2). Second, considering that the semantics of
---
Inductive Knowledge Graph Reasoning with MStar                                              9

edges are query-dependent, we use the attention mechanism [37] and assign a
weight for every edge (e, r, x) in step ℓ:

$$\\gamma_{(e,r,x)|u,q}^{\\ell} = \\sigma(W_{attn}^{\\ell}\\text{ReLU}(W_{attnu}^{\\ell}h_{u,q}^{\\ell-1}(e) + W_{attnr}^{\\ell}\\hat{r} + W_{attnq}^{\\ell}q)),$$    (7)

where $W_{attn}^{\\ell} \\in \\mathbb{R}^{1\\times d_\\gamma}$, $W_{attnu}^{\\ell}$, $W_{attnr}^{\\ell}$ and $W_{attnq}^{\\ell} \\in \\mathbb{R}^{d_\\gamma \\times d}$ are learnable weight
matrices, $d_\\gamma$ is the dimension of attention, $h_{u,q}^{\\ell}(e)$ denotes the embedding of the
entity e in multi-condition GNN at step ℓ, and σ denotes a sigmoid function.
Based on the two ways above, the entity embeddings are given by""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk15> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk15> ex:chunkContent """$$h_{u,q}^{\\ell}(e) = \\text{ReLU}\\left(W_o^{\\ell} \\sum_{(e,r,x)\\in N(e)\\wedge\\{e,x\\}\\subset V_{u,q}^{\\ell}} \\gamma_{(e,r,x)|u,q}^{\\ell}(h_{u,q}^{\\ell-1}(x) \\odot \\hat{r})\\right),$$    (8)

where $W_o^{\\ell} \\in \\mathbb{R}^{d\\times d}$ is a learnable weight matrix, $V_{u,q}^{\\ell}$ is the entity set in progres-
sive propagation step ℓ, and $V_{u,q}^0 = S_{u,q}$.

### 4.5 Training Strategy: LinkVerify

To reason the likelihood of a triplet (u, q, e), the decoder produces a score func-
tion s(·). Given the final output $h_{u,q}^{L_2}$ after $L_2$ layers of multi-condition GNN,
the score function is given by

$$s(u, q, e) = W_3(\\text{ReLU}(W_4(h_{u,q}^{L_2}(u) \\oplus h_{u,q}^{L_2}(e)))),$$    (9)

where $W_3 \\in \\mathbb{R}^{1\\times d}$ and $W_4 \\in \\mathbb{R}^{d\\times 2d}$ are learnable weight matrices. However,
multi-condition GNN propagates progressively and probably misses several dis-
tant target tail entities during the training. In this situation, the prediction
knows nothing about the target tail entity and brings a noisy score for training.
To alleviate the problem above, we propose a mechanism LinkVerify to filter
noisy training samples. The noisy sample represents the triplet whose target tail
entity is not involved in $V_{u,q}^{L_2}$. Taking the inductive KG reasoning task as a multi-
label classification problem, we use the multi-class log-loss [14, 47] to optimize
the model. Associated with LinkVerify, the final loss is given by

$$\\mathcal{L} = \\sum_{(u,q,v)\\in \\mathcal{F}} \\left(-s(u,q,v) + \\log\\left(\\sum_{e\\in\\mathcal{E}} \\exp(s(u,q,e))\\right)\\right) \\times \\mathbb{1}(v \\in V_{u,q}^{L_2}).$$    (10)

## 5 Experiments

In this section, we perform extensive experiments to answer the questions below:

- Q1: Does MStar perform well on inductive KG reasoning?
- Q2: How does each designed module influence the performance?
- Q3: Whether MStar can improve reasoning ability about distant entities or
not?
---

10        Z. Shao et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk16> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk16> ex:chunkContent """10        Z. Shao et al.

Table 2. Statistics of the inductive datasets. G and G' denote the KGs in the training and test sets, respectively.

| Datasets | FB15k-237 | NELL-995 | WN18RR |
|----------|-----------|----------|---------|
| Versions KGs | \\|R\\| | \\|V\\| | \\|F\\| | \\|R\\| | \\|V\\| | \\|F\\| | \\|R\\| | \\|V\\| | \\|F\\| |
| v1 | G' | 183 | 2,000 | 5,226 | 14 | 10,915 | 5,540 | 9 | 2,746 | 6,678 |
|    | G  | 146 | 1,500 | 2,404 | 14 | 225 | 1,034 | 9 | 922 | 1,991 |
| v2 | G' | 203 | 3,000 | 12,085 | 88 | 2,564 | 10,109 | 10 | 6,954 | 18,968 |
|    | G  | 176 | 2,000 | 5,092 | 79 | 4,937 | 5,521 | 10 | 2,923 | 4,863 |
| v3 | G' | 218 | 4,000 | 22,394 | 142 | 4,647 | 20,117 | 11 | 12,078 | 32,150 |
|    | G  | 187 | 3,000 | 9,137 | 122 | 4,921 | 9,668 | 11 | 5,084 | 7,470 |
| v4 | G' | 222 | 5,000 | 33,916 | 77 | 2,092 | 9,289 | 9 | 3,861 | 9,842 |
|    | G  | 204 | 3,500 | 14,554 | 61 | 3,294 | 8,520 | 9 | 7,208 | 15,157 |

## 5.1 Experiments Settings

### Datasets
We perform inductive KG reasoning experiments on the benchmark datasets proposed in GraIL [32], which are derived from WN18RR [6], FB15k-237 [34], and NELL-995 [42]. Each benchmark dataset is divided into four versions (v1, v2, v3, v4), and the size typically increases following the version number. Each version consists of training and test graphs without overlapping entities. The training graphs contain triplets for training and validation, following a split ratio of 10:1. The statistics of the datasets are presented in Table 2.

### Baselines
We compare MStar with 10 inductive baselines organized into three groups, including (i) three rule-based models: RuleN [20], Neural LP [45], and DRUM [27]; (ii) two subgraph-based models: GraIL [32] and CoMPILE [18]; (iii) five C-MPNN-based models: NBFNet [50], A*Net [49], RED-GNN [47], AdaProp [48], and RUN-GNN [41].""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk17> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk17> ex:chunkContent """### Evaluation and Tie Policy
Following [47-49], we evaluate all the models using the filtered mean reciprocal rank (MRR) and Hits@10 metrics. The best models are chosen according to MRR on the validation dataset. Subgraph-based models typically rank each test triplet among 50 randomly sampled negative triplets, whereas C-MPNNs evaluate each triplet against all possible candidates. In this paper, we follow the latter and take the results of rule-based and subgraph-based models from [48]. Missing results are reproduced by their official code.

There are different tie policies [30] to compute MRR when several candidate entities receive equal scores. In progressive propagation, all unvisited entities are assigned identical scores. Following [41,47], we measure the average rank among the entities in the tie, as suggested in [26]. To keep the tie policy consistent, we re-evaluate AdaProp using the official code.
---
Inductive Knowledge Graph Reasoning with MStar                                                                          11

Table 3. Inductive KG reasoning results (measured with MRR). The best scores are
in bold and the second-best scores are underlined. "-" denotes the result unavailable,
and values with suffix "⋆" are reproduced using the released code.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk18> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk18> ex:chunkContent """| Models    | FB15k-237 |     |     |     | NELL-995 |     |     |     | WN18RR |     |     |     |
|-----------|-----------|-----|-----|-----|----------|-----|-----|-----|--------|-----|-----|-----|
|           | v1        | v2  | v3  | v4  | v1       | v2  | v3  | v4  | v1     | v2  | v3  | v4  |
| RuleN     | .363      | .433| .439| .429| .615     | .385| .381| .333| .668   | .645| .368| .624|
| Neural LP | .325      | .389| .400| .396| .610     | .361| .367| .261| .649   | .635| .361| .628|
| DRUM      | .333      | .395| .402| .410| .628     | .365| .375| .273| .666   | .646| .380| .627|
| GraIL     | .279      | .276| .251| .227| .481     | .297| .322| .262| .627   | .625| .323| .553|
| CoMPILE   | .287      | .276| .262| .213| .330     | .248| .319| .229| .577   | .578| .308| .548|
| NBFNet    | .270      | .321| .335| .288| .584     | .410| .425| .287| .686   | .662| .410| .601|
| A*Net     | -         | -   | -   | -   | -        | -   | -   | -   | -      | -   | -   | -   |
| RED-GNN   | .341      | .411| .411| .421| .591⋆    | .373⋆| .391⋆| .195⋆| .693   | .687| .422| .642|
| AdaProp   | .279⋆     | .467⋆| .470⋆| .440⋆| .725⋆    | .416⋆| .413⋆| .338⋆| .706⋆  | .703⋆| .433⋆| .651⋆|
| RUN-GNN   | .397      | .473| .468| .463| .617⋆    | .413⋆| .479⋆| .282⋆| .699   | .697| .445| .654|
| MStar     | **.458**  | **.526**| **.506**| **.487**| **.787**     | **.540**| **.496**| **.384**| **.733**   | **.702**| **.442**| **.645**|

Table 4. Inductive KG reasoning results (measured with Hits@10)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk19> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk19> ex:chunkContent """Table 4. Inductive KG reasoning results (measured with Hits@10)

| Models    | FB15k-237 |     |     |     | NELL-995 |     |     |     | WN18RR |     |     |     |
|-----------|-----------|-----|-----|-----|----------|-----|-----|-----|--------|-----|-----|-----|
|           | v1        | v2  | v3  | v4  | v1       | v2  | v3  | v4  | v1     | v2  | v3  | v4  |
| RuleN     | .446      | .599| .600| .605| .760     | .514| .531| .484| .730   | .694| .407| .681|
| Neural LP | .468      | .586| .571| .593| .871     | .564| .576| .539| .772   | .749| .476| .706|
| DRUM      | .474      | .595| .571| .593| .873     | .540| .577| .531| .777   | .747| .477| .702|
| GraIL     | .429      | .424| .424| .389| .565     | .496| .518| .506| .760   | .776| .409| .687|
| CoMPILE   | .439      | .457| .449| .358| .575     | .446| .515| .421| .747   | .743| .406| .670|
| NBFNet    | .530      | .644| .623| .642| .795     | .635| .606| .591| .827   | .799| .568| .702|
| A*Net     | .535      | .638| .610| .630| -        | -   | -   | -   | .810   | .803| .544| .743|
| RED-GNN   | .483      | .629| .603| .621| .866⋆    | .601⋆| .594⋆| .556⋆| .799   | .780| .524| .721|
| AdaProp   | .461⋆     | .665⋆| .636⋆| .632⋆| .776⋆    | .618⋆| .580⋆| .589⋆| .796⋆  | .792⋆| .532⋆| .730⋆|
| RUN-GNN   | .496      | .639| .631| .665| .833⋆    | .575⋆| .659⋆| .436⋆| .807   | .798| .550| .735|
| MStar     | **.583**  | **.702**| **.675**| **.665**| **.900**     | **.735**| **.666**| **.617**| **.817**   | **.803**| **.547**| **.726**|""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk20> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk2> ex:chunkContent """**Keywords:** Knowledge graphs · Inductive reasoning · Conditional message passing.

## 1 Introduction

Knowledge graphs (KGs) have become a valuable asset for many downstream AI applications, including semantic search, question answering, and logic reasoning [4, 11, 33]. Real-world KGs, such as Freebase [1], NELL [21], and DBpedia [15], often suffer from the incompleteness issue that lacks massive certain triplets [5, 12]. The KG reasoning task aims to alleviate incompleteness by discovering missing triplets based on the knowledge learned from known facts. Early studies [38] assume that KGs are static, ignoring the potential unseen entities and emerging triplets in the continuously updated real-world KGs. This motivates the task of inductive KG reasoning [32, 46], which allows for incorporating emerging entities and facts during inference.
---
2          Z. Shao et al.

Table 1. Hits@10 results of RED-GNN [47] in our empirical study. We divide all
triplets in the FB15k-237 (v1) dataset [32] into four groups according to the shortest
distance between head and tail entities. "∞" denotes that the head entity cannot reach
the corresponding tail entity in the KG. The maximum shortest distance is 10 in the
FB15k-237 (v1) when ignoring triplets belonging to ∞.

| Distance | Proportions | Layers = 3 | Layers = 6 | Layers = 9 |
|----------|-------------|------------|------------|------------|
| [1, 4)   | 70.25%      | .611       | .594       | .587       |
| [4, 7)   | 22.44%      | .000       | .102       | .154       |
| [7, 10]  | 3.90%       | .000       | .000       | .088       |
| ∞        | 3.41%       | .000       | .000       | .000       |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk3> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk20> ex:chunkContent """Implementation Details We implement our model using the PyTorch frame-
work [24] and employ the Adam optimizer [13] for training. Due to the relatively
small size of the inductive dataset and its susceptibility to overfitting, we apply
early stopping to mitigate this issue. We tune the hyper-parameters using grid
search and select the number of starting entities n in {1, 2, 4, 8, 16, 32, 64}, the
number of starting entity types m in {2, 3, 5, 7, 9}. The best hyperparameters are
selected according to the MRR metric on the validation sets. All experiments
are conducted on a single NVIDIA RTX A6000 GPU with 48GB memory.
---
12        Z. Shao et al.

## 5.2     Main Results (Q1)

Tables 3 and 4 depict the performance of different models on inductive KG reasoning. MStar demonstrates the best performance across all metrics on FB15k-237 and NELL-995, and compares favorably with the top models on WN18RR. We observe that (i) subgraph-based models typically perform poorly. This is because subgraphs are often sparse or empty and provide less information, particularly for distant entities. (ii) Rule-based models are generally more competitive but are still weaker compared to C-MPNN-based models. However, DRUM outperforms existing models except MStar in Hits@10 on NELL-995 (v1). NELL-995 (v1) is a special dataset and the distance between the head and tail entities for all triplets in the test graph is no longer than 3, which is very short. Thus, we conjecture that the length of the learned rules limits the reasoning capabilities of rule-based models. Differently, MStar holds an edge over these two groups of models on all datasets. This suggests that multiple starting entities in MStar alleviate the distance limit issues as much as possible when reasoning.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk21> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk21> ex:chunkContent """Compared with the best C-MPNN-based results, MStar achieves an average relative gain of 9.9% in MRR, 5.2% in Hits@10 on FB15k-237, and 13.9% in MRR, 6.1% in Hits@10 on NELL-995. Existing C-MPNN-based models typically use all entities in the KG or only the head entity as starting entities, without providing conditional information to distant entities, which can introduce excessive noise or lack sufficient information. Instead, our MStar selects multiple query-dependent starting entities adaptively and propagates conditions farther through the highway for accurate reasoning. Moreover, LinkVerify in MStar additionally reduces noisy samples in training. We also observe that the improvement of the model on WN18RR is not as pronounced as on the other datasets. To provide insights into this phenomenon, we conduct further analysis in Section 5.4.

## 5.3     Ablation Study

Variants of MStar (Q2) In this section, we design several variants of MStar to study the contributions of three components: (i) selection, (ii) highway, and (iii) LinkVerify in training. The results are summarized in Tables 5 and 6, which indicate that all components contribute significantly to MStar on the three datasets.

First, the variant of w/o selection propagates only from the head entity which is the same as RED-GNN. According to the results, removing selection significantly decreases performance, highlighting the effectiveness of using multiple starting entities to explore reasoning patterns across a broader neighborhood.

Second, it can be observed that the performance of variant w/o highway is worse than MStar. This observation suggests that transmitting query-dependent information to the starting entities is a promising approach to expedite propagation for conditions and enhance reasoning accuracy.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk22> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk22> ex:chunkContent """Third, the variant of w/o LinkVerify is inferior to MStar all the time, as triplets with unvisited target entities in training KG introduce noise. Removing LinkVerify results in poorer performance, especially on smaller datasets. For
---
## Inductive Knowledge Graph Reasoning with MStar

### Table 5. Ablation study of the proposed framework (measure with MRR)

| Models         | FB15k-237                | NELL-995                | WN18RR                  |
|----------------|--------------------------|-------------------------|--------------------------|
|                | v1    v2    v3    v4     | v1    v2    v3    v4    | v1    v2    v3    v4     |
| MStar          | .458  .526  .506  .487   | .787  .540  .496  .384  | .733  .702  .442  .645   |
| w/o Selection  | .432  .491  .483  .457   | .719  .479  .457  .280  | .721  .674  .432  .643   |
| w/o Highway    | .411  .488  .460  .474   | .774  .473  .494  .297  | .726  .700  .438  .629   |
| w/o LinkVerify | .426  .517  .498  .481   | .661  .502  .482  .375  | .729  .698  .420  .641   |

### Table 6. Ablation study of the proposed framework (measured with Hits@10)

| Models         | FB15k-237                | NELL-995                | WN18RR                  |
|----------------|--------------------------|-------------------------|--------------------------|
|                | v1    v2    v3    v4     | v1    v2    v3    v4    | v1    v2    v3    v4     |
| MStar          | .583  .702  .675  .665   | .900  .735  .666  .617  | .817  .803  .547  .726   |
| w/o Selection  | .534  .686  .644  .629   | .775  .693  .619  .425  | .811  .778  .528  .717   |
| w/o Highway    | .532  .657  .609  .644   | .855  .682  .648  .532  | .814  .788  .543  .698   |
| w/o LinkVerify | .568  .699  .657  .658   | .785  .695  .645  .608  | .811  .797  .508  .724   |

### Table 7. Per-distance evaluation on FB15k-237 (v1) (measured with Hits@10). "∞" indicates that the head entity fails to reach the tail entity.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk23> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk23> ex:chunkContent """### Table 7. Per-distance evaluation on FB15k-237 (v1) (measured with Hits@10). "∞" indicates that the head entity fails to reach the tail entity.

| Distance | Proportions | RED-GNN | AdaProp | RUN-GNN | NBFNet | MStar |
|----------|-------------|---------|---------|---------|--------|-------|
| 1        | 32.68%      | .813    | .933    | .851    | .545   | .948  |
| 2        | 12.20%      | .640    | .520    | .740    | .760   | .780  |
| 3        | 25.37%      | .433    | .269    | .414    | .490   | .471  |
| 4        | 7.32%       | .000    | .000    | .267    | .333   | .300  |
| 5        | 11.22%      | .000    | .000    | .217    | .261   | .174  |
| 6        | 3.90%       | .000    | .000    | .000    | .438   | .188  |
| 7        | 1.46%       | .000    | .000    | .000    | .333   | .000  |
| 8        | 1.46%       | .000    | .000    | .000    | .333   | .167  |
| 9        | 0.00%       | .000    | .000    | .000    | .000   | .000  |
| 10       | 0.98%       | .000    | .000    | .000    | .250   | .000  |
| ∞        | 3.41%       | .000    | .000    | .000    | .357   | .214  |

instance, w/o LinkVerify decreases 7.0% for FB15k-237 (v1) and 1.3% for FB15k-237 (v4) relatively. This is because the noisy triplets negatively influence training when data is lacking. Thus, LinkVerify demonstrates to be more effective when applied to KGs with fewer triplets.

### Per-distance Performance (Q3)
To check the reasoning ability on distant tail entities, we compare MStar with several expressive models on FB15k-237 (v1). To make the comparison more precise, we split FB15k-237 (v1) into 11 subsets according to the shortest distance between the head and tail entity for each triplet. The comparisons are conducted on each subset based on official
---
14       Z. Shao et al.

Table 8. Proportions of long-distance triplets in the KGs. The shortest distance be-
tween head and tail entities in a long-distance triplet is longer than 3.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk24> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk24> ex:chunkContent """Table 8. Proportions of long-distance triplets in the KGs. The shortest distance be-
tween head and tail entities in a long-distance triplet is longer than 3.

| Datasets | FB15k-237 | NELL-995 | WN18RR |
|----------|-----------|----------|---------|
| Versions | G | G' | G | G' | G | G' |
| v1 | 15.78% | 29.76% | 39.64% | 0.00% | 34.31% | 17.55% |
| v2 | 8.69% | 15.48% | 10.62% | 2.52% | 20.86% | 16.33% |
| v3 | 3.41% | 4.51% | 11.16% | 3.96% | 22.32% | 26.94% |
| v4 | 2.39% | 2.74% | 9.30% | 6.98% | 22.39% | 20.50% |

code and parameters. RED-GNN, AdaProp and MStar use 3 layers of GNN.
RUN-GNN and NBFNet use 5 and 6 layers of GNN, respectively. The results
are shown in Table 7.

Compared to the models with a single starting entity (RED-GNN, AdaProp,
and RUN-GNN), MStar performs better significantly on distant entities. For
instance, RED-GNN fails to predict entities beyond 3 hops. Moreover, MStar can
even reason about unreachable target entities. This is because MStar can select
query-related starting entities that are disconnected from the head entity but in
the neighborhood of the unreachable entities. These observations demonstrate
that multiple starting entities can expand the reasoning area effectively, and the
highway layer provides additional evidence for reasoning about distant entities.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk25> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk25> ex:chunkContent """Differently, the reasoning performance of NBFNet on close entities is signifi-
cantly decreased despite the ability to reason about distant entities. For instance,
NBFNet is inferior to the other models on Hits@10 for 1-distance triplets with
a great gap of at least 0.268. This is because NBFNet propagates from query-
independent starting entities and reasons along many noisy relational paths,
which disrupts the inference about close entities. Instead, MStar improves the
reasoning performance for distant entities and keeps the reasoning abilities for
close entities simultaneously. This is achieved due to MStar propagating condi-
tions along query-related relational paths and removing noisy links by LinkVerify.

### 5.4 Further Analysis

Perspective of Datasets As shown in Tables 5 and 6, the improvement of
MStar on WN18RR is not as great as the one on other datasets. As can be
seen from Table 2, WN18RR (v1, v2, v3, v4) and NELL-995 (v1) have fewer
relations. Due to the entity-independent nature of inductive KG reasoning, entity
embeddings usually rely on the representation of relations. With fewer relations,
entities carry more monotonous information. Therefore, it becomes challenging
to select query-dependent entities and propagate messages to the target ones. To
study the situation further, we count the proportion of triplets whose shortest
distance between the head and tail entities exceeds 3. We regard these triplets
as long-distance triplets. The result is shown in Table 8. We can see that NELL-
995 (v1) owns zero long-distance triplets in the test graphs. Thus, NELL-995 (v1)
---
Inductive Knowledge Graph Reasoning with MStar             15

Table 9. Comparison of different starting entities selection methods""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk26> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk26> ex:chunkContent """Table 9. Comparison of different starting entities selection methods

| Models | FB15k-237 (v1) | NELL-995 (v1) | WN18RR (v1) |
|---------|----------------|---------------|--------------|
|         | MRR Hits@10    | MRR Hits@10   | MRR Hits@10  |
| MStar   | .462 .598      | .801 .921     | .736 .816    |
| w/ random | .427 .587    | .787 .901     | .698 .803    |
| w/ degree | .403 .553    | .362 .595     | .709 .810    |

can resolve the above issues by propagating conditional information to any target
entity in 3 hops, even without multiple starting entities.

Perspective of Starting Entities Selection MStar leverages an importance
score function to select starting entities. The score function is conditioned on the
query head and relation, aiming to explore query-dependent entities. Here, we
consider two other score function variants, i.e., variant w/ random and variant
w/ degree. Variant w/ random scores the entities with random values. Similar
to EL-GNN [25], variant w/ degree assigns higher scores to entities with higher
degrees. All variants keep top-n entities as starting ones.

Table 9 shows the comparison results. We can observe that random scores
lead to a degraded performance. This is because random starting entities prop-
agate along many noisy relational paths. Noisy paths hinder MStar's ability
to capture query-related rules and to reach distant target tail entities. Variant
w/ degree is also inferior to our MStar, even worse than random scores. For
instance, the performance of variant w/ degree on FB15k-237 (v1) decreases by
54.8% and 54.0% relative to MStar and variant w/ random, respectively. This is
mainly due to the fact that the global feature degree fixes the starting entities
and cannot support query-dependent propagation.

## 6 Conclusion and Future Work""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk27> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk27> ex:chunkContent """## 6 Conclusion and Future Work

In this paper, we explore the issue of inefficient message propagation for KG rea-
soning and propose a new inductive KG reasoning model called MStar. Specif-
ically, we propose using multiple starting entities to expand the propagation
scope. Moreover, we construct a highway between the head entity and the other
starting entities to accelerate conditional message passing. Additionally, we in-
troduce a training strategy LinkVerify to filter inappropriate samples. Experi-
mental results demonstrate the effectiveness of MStar. In particular, ablation
results validate the superiority of MStar for reasoning about distant entities. In
future work, we plan to explore alternative modules for selecting and classify-
ing starting entities. We also intend to investigate methods to effectively utilize
noisy triplets during training instead of dropping them.

Acknowledgments We thank the anonymous reviewers for their valuable com-
ments. This work was supported by the National Natural Science Foundation of
---
16        Z. Shao et al.

China under Grant 62272219 and the Collaborative Innovation Center of Novel
Software Technology & Industrialization.

Supplemental Material Statement The source code and hyperparameters
are available at our GitHub repository: https://github.com/nju-websoft/
MStar.

## References""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk28> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk28> ex:chunkContent """1. Bollacker, K.D., Evans, C., Paritosh, P.K., Sturge, T., Taylor, J.: Freebase: A
   collaboratively created graph database for structuring human knowledge. In: Proc.
   of SIGMOD. pp. 1247–1250 (2008)
2. Bordes, A., Usunier, N., García-Durán, A., Weston, J., Yakhnenko, O.: Translating
   embeddings for modeling multi-relational data. In: Proc. of NeurIPS. vol. 26, pp.
   2787–2795 (2013)
3. Chen, J., He, H., Wu, F., Wang, J.: Topology-aware correlations between relations
   for inductive link prediction in knowledge graphs. In: Proc. of AAAI. vol. 35, pp.
   6271–6278 (2021)
4. Chen, X., Jia, S., Xiang, Y.: A review: Knowledge reasoning over knowledge graph.
   Expert Syst. Appl. 141, 112948 (2020)
5. Chen, Z., Wang, X., Wang, C., Li, Z.: PosKHG: A position-aware knowledge hy-
   pergraph model for link prediction. Data Sci. Eng. 8, 135–145 (2023)
6. Dettmers, T., Minervini, P., Stenetorp, P., Riedel, S.: Convolutional 2D knowledge
   graph embeddings. In: Proc. of AAAI. vol. 32, pp. 1811–1818 (2018)
7. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message
   passing for quantum chemistry. In: Proc. of ICML. vol. 70, pp. 1263–1272 (2017)
8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
   In: Proc. of CVPR. pp. 770–778 (2016)
9. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected
   convolutional networks. In: Proc. of CVPR. pp. 2261–2269 (2017)
10. Huang, X., Romero, M., Ceylan, İ.İ., Barceló, P.: A theory of link prediction via
    relational Weisfeiler-Leman on knowledge graphs. In: Proc. of NeurIPS. vol. 36,
    pp. 19714–19748 (2023)
11. Ji, S., Pan, S., Cambria, E., Marttinen, P., Yu, P.S.: A survey on knowledge graphs:
    Representation, acquisition, and applications. IEEE Trans. Neural Networks Learn.
    Syst. 33, 494–514 (2022)
12. Jia, T., Yang, Y., Lu, X., Zhu, Q., Yang, K., Zhou, X.: Link prediction based on""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk29> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk29> ex:chunkContent """Representation, acquisition, and applications. IEEE Trans. Neural Networks Learn.
    Syst. 33, 494–514 (2022)
12. Jia, T., Yang, Y., Lu, X., Zhu, Q., Yang, K., Zhou, X.: Link prediction based on
    tensor decomposition for the knowledge graph of COVID-19 antiviral drug. Data
    Intell. 4, 134–148 (2022)
13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Proc. of
    ICLR. pp. 1–13 (2015)
14. Lacroix, T., Usunier, N., Obozinski, G.: Canonical tensor decomposition for knowl-
    edge base completion. In: Proc. of ICML. vol. 80, pp. 2869–2878 (2018)
15. Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes, P.N.,
    Hellmann, S., Morsey, M., van Kleef, P., Auer, S., Bizer, C.: DBpedia - A large-
    scale, multilingual knowledge base extracted from wikipedia. Semantic Web 6,
    167–195 (2015)
---
Inductive Knowledge Graph Reasoning with MStar              17""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk30> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk3> ex:chunkContent """Due to their excellent efficiency and performance, conditional message pass-
ing neural networks (C-MPNNs), such as NBFNet [50] and RED-GNN [47], have
emerged as one of the premier models in the field of inductive KG reasoning. To
transmit conditions, existing C-MPNNs only incorporate conditional informa-
tion into the head entity and propagate along the relational paths progressively.
However, this single-starting-entity strategy results in a limited conditional mes-
sage passing scope, leading to the failure of message passing from the head entity
to distant target entities. This inspires us to extend the scope of conditional mes-
sage passing to support reasoning on target entities in a farther area.

We conduct an empirical study to analyze the drawbacks of the limited
message passing scope. Specifically, we report the results of a C-MPNN, RED-
GNN [47], on predicting target entities at different distances in Table 1. It can
be observed that RED-GNN performs poorly for queries with distant target
entities, even stacking more message-passing layers. This indicates that exist-
ing C-MPNNs cannot effectively propagate conditional messages toward distant
target entities, hindering performance on these queries. Although stacking more
GNN layers can alleviate this issue to some extent, it causes high computation
and performance declines on the queries with target entities nearby.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk4> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk30> ex:chunkContent """16. Lin, Q., Liu, J., Xu, F., Pan, Y., Zhu, Y., Zhang, L., Zhao, T.: Incorporating
    context graph with logical reasoning for inductive relation prediction. In: Proc. of
    SIGIR. pp. 893–903 (2022)
17. Lin, Y., Liu, Z., Sun, M., Liu, Y., Zhu, X.: Learning entity and relation embeddings
    for knowledge graph completion. In: Proc. of AAAI. vol. 29, pp. 2181–2187 (2015)
18. Mai, S., Zheng, S., Yang, Y., Hu, H.: Communicative message passing for inductive
    relation reasoning. In: Proc. of AAAI. vol. 35, pp. 4294–4302 (2021)
19. Meilicke, C., Chekol, M.W., Ruffinelli, D., Stuckenschmidt, H.: Anytime bottom-up
    rule learning for knowledge graph completion. In: Proc. of IJCAI. pp. 3137–3143
    (2019)
20. Meilicke, C., Fink, M., Wang, Y., Ruffinelli, D., Gemulla, R., Stuckenschmidt, H.:
    Fine-grained evaluation of rule- and embedding-based systems for knowledge graph
    completion. In: Proc. of ISWC. vol. 11136, pp. 3–20 (2018)
21. Mitchell, T.M., Cohen, W.W., Jr., E.R.H., Talukdar, P.P., Yang, B., Betteridge,
    J., Carlson, A., Mishra, B.D., Gardner, M., Kisiel, B., Krishnamurthy, J., Lao, N.,
    Mazaitis, K., Mohamed, T., Nakashole, N., Platanios, E.A., Ritter, A., Samadi, M.,
    Settles, B., Wang, R.C., Wijaya, D., Gupta, A., Chen, X., Saparov, A., Greaves,
    M., Welling, J.: Never-ending learning. Commun. ACM 61, 103–115 (2018)
22. Nickel, M., Tresp, V., Kriegel, H.: A three-way model for collective learning on
    multi-relational data. In: Proc. of ICML. pp. 809–816 (2011)
23. Pan, Y., Liu, J., Zhang, L., Zhao, T., Lin, Q., Hu, X., Wang, Q.: Inductive relation
    prediction with logical reasoning using contrastive representations. In: Proc. of
    EMNLP. pp. 4261–4274 (2022)
24. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
    Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E.Z., DeVito,
    Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk31> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk31> ex:chunkContent """Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E.Z., DeVito,
    Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,
    S.: PyTorch: An imperative style, high-performance deep learning library. In: Proc.
    of NeurIPS. vol. 32, pp. 8024–8035 (2019)
25. Qiu, H., Zhang, Y., Li, Y., Yao, Q.: Logical expressiveness of graph neural network
    for knowledge graph reasoning. In: Proc. of ICLR. pp. 1–21 (2020)
26. Rossi, A., Barbosa, D., Firmani, D., Matinata, A., Merialdo, P.: Knowledge graph
    embedding for link prediction: A comparative analysis. ACM Trans. Knowl. Discov.
    Data 15, 14:1–14:49 (2021)
27. Sadeghian, A., Armandpour, M., Ding, P., Wang, D.Z.: DRUM: End-to-end dif-
    ferentiable rule mining on knowledge graphs. In: Proc. of NeurIPS. vol. 32, pp.
    15321–15331 (2019)
28. Schlichtkrull, M.S., Kipf, T.N., Bloem, P., van den Berg, R., Titov, I., Welling, M.:
    Modeling relational data with graph convolutional networks. In: Proc. of ESWC.
    vol. 10843, pp. 593–607 (2018)
29. Sun, Z., Deng, Z., Nie, J., Tang, J.: RotatE: Knowledge graph embedding by rela-
    tional rotation in complex space. In: Proc. of ICLR. pp. 1–18 (2019)
30. Sun, Z., Vashishth, S., Sanyal, S., Talukdar, P.P., Yang, Y.: A re-evaluation of
    knowledge graph completion methods. In: Proc. of ACL. pp. 5516–5522 (2020)
31. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.E., Anguelov, D., Erhan,
    D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proc. of
    CVPR. pp. 1–9 (2015)
32. Teru, K.K., Denis, E.G., Hamilton, W.L.: Inductive relation prediction by subgraph
    reasoning. In: Proc. of ICML. vol. 119, pp. 9448–9457 (2020)
33. Tian, L., Zhou, X., Wu, Y.P., Zhou, W.T., Zhang, J.H., Zhang, T.S.: Knowledge
    graph and knowledge reasoning: A systematic review. J. Electron. Sci. Technol.
    20, 100159 (2022)
---
18        Z. Shao et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk32> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk32> ex:chunkContent """34. Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon, M.: Representing text for joint embedding of text and knowledge bases. In: Proc. of EMNLP. pp. 1499–1509 (2015)
35. Trouillon, T., Welbl, J., Riedel, S., Gaussier, É., Bouchard, G.: Complex embeddings for simple link prediction. In: Proc. of ICML. vol. 48, pp. 2071–2080 (2016)
36. Vashishth, S., Sanyal, S., Nitin, V., Talukdar, P.P.: Composition-based multi-relational graph convolutional networks. In: Proc. of ICLR. pp. 1–16 (2020)
37. Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., Bengio, Y.: Graph attention networks. In: Proc. of ICLR. pp. 1–12 (2018)
38. Wang, Q., Mao, Z., Wang, B., Guo, L.: Knowledge graph embedding: A survey of approaches and applications. IEEE Trans. Knowl. Data Eng. 29, 2724–2743 (2017)
39. Wang, Z., Zhang, J., Feng, J., Chen, Z.: Knowledge graph embedding by translating on hyperplanes. In: Proc. of AAAI. vol. 28, pp. 1112–1119 (2014)
40. Wu, F., Jing, X., Wei, P., Lan, C., Ji, Y., Jiang, G., Huang, Q.: Semi-supervised multi-view graph convolutional networks with application to webpage classification. Inf. Sci. 591, 142–154 (2022)
41. Wu, S., Wan, H., Chen, W., Wu, Y., Shen, J., Lin, Y.: Towards enhancing relational rules for knowledge graph link prediction. In: Proc. of EMNLP Findings. pp. 10082–10097 (2023)
42. Xiong, W., Hoang, T., Wang, W.Y.: DeepPath: A reinforcement learning method for knowledge graph reasoning. In: Proc. of EMNLP. pp. 564–573 (2017)
43. Xu, X., Zhang, P., He, Y., Chao, C., Yan, C.: Subgraph neighboring relations infomax for inductive link prediction on knowledge graphs. In: Proc. of IJCAI. pp. 2341–2347 (2022)
44. Yang, B., Yih, W., He, X., Gao, J., Deng, L.: Embedding entities and relations for learning and inference in knowledge bases. In: Proc. of ICLR. pp. 1–12 (2015)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk33> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk33> ex:chunkContent """44. Yang, B., Yih, W., He, X., Gao, J., Deng, L.: Embedding entities and relations for learning and inference in knowledge bases. In: Proc. of ICLR. pp. 1–12 (2015)
45. Yang, F., Yang, Z., Cohen, W.W.: Differentiable learning of logical rules for knowledge base reasoning. In: Proc. of NeurIPS. vol. 30, pp. 2319–2328 (2017)
46. Zhang, W., Yao, Z., Chen, M., Huang, Z., Chen, H.: NeuralKG-ind: A Python library for inductive knowledge graph representation learning. In: Proc. of SIGIR. pp. 3140–3144 (2023)
47. Zhang, Y., Yao, Q.: Knowledge graph reasoning with relational digraph. In: Proc. of WWW. pp. 912–924 (2022)
48. Zhang, Y., Zhou, Z., Yao, Q., Chu, X., Han, B.: AdaProp: Learning adaptive propagation for graph neural network based knowledge graph reasoning. In: Proc. of KDD. pp. 3446–3457 (2023)
49. Zhu, Z., Yuan, X., Galkin, M., Xhonneux, L.P., Zhang, M., Gazeau, M., Tang, J.: A*Net: A scalable path-based reasoning approach for knowledge graphs. In: Proc. of NeurIPS. vol. 36, pp. 59323–59336 (2023)
50. Zhu, Z., Zhang, Z., Xhonneux, L.A.C., Tang, J.: Neural Bellman-Ford networks: A general graph neural network framework for link prediction. In: Proc. of NeurIPS. vol. 34, pp. 29476–29490 (2021)""" .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk4> ex:chunkContent """In this paper, we propose a novel inductive KG reasoning model MStar based
on Multi-Starting progressive propagation, which expands the scope of efficient
conditional message passing. Our key insight is to utilize more conditional start-
ing entities and create shortcuts between the head entity and them. Specifically,
we design a starting entities selection (SES) module and a highway layer to select
multiple starting entities and create shortcuts for conditional message passing,
respectively. First, the SES module encodes entities using a pre-embedded GNN
and then selects multiple query-dependent starting entities, which may include
entities distant from the head entity. These entities broaden the scope of subse-
quent progressive propagation and allow MStar to propagate along query-related
relational paths to enhance reasoning concerning distant entities. Second, we cre-
ate shortcuts from the head entity to the selected multiple starting entities in
the highway layer. The design of the highway layer is inspired by skip connec-
tion from ResNet [8]. The conditional message can be passed to distant entities
through the highway layer. For example, in Fig. 1, a 3-layer RED-GNN would
---
Inductive Knowledge Graph Reasoning with MStar                            3

```mermaid
graph TD
    A[U:Univ. of California, Berkeley] -->|also_known_as| B[C:State Univ.]
    B -->|also_known_as| C[U:The Ohio State Univ.]
    A -->|plays_for| D[T:MSU Spartans]
    D -->|plays_for| E[U:Michigan State Univ.]
    E -->|also_known_as| B
    C -->|supported_by| F[S:State of Ohio]
    C -->|plays_for| G[T:Ohio State Buckeyes]
    A -->|supported_by| H[S:State of California]
    H -->|also_known_as| I[T:California Golden Bears]
    I -->|also_known_as| J[C:Univ. Team]
    J -->|also_known_as| G
    E -->|supported_by| K[S:State of Michigan]
    D -->|also_known_as| J
```""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk5> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk5> ex:chunkContent """Fig. 1. A motivating example of distant target tail entities for predicting (Univ. of California, Berkeley → also_known_as → State Univ.). Prefix "U", "S", and "T" represent university, state, and basketball teams, respectively. Prefix "C" represents category-type entities. Different colors and prefixes symbolize distinct entity types.

fail to predict the target answer, because the length of the shortest path between head entity Univ. of California, Berkeley and the target entity State Univ. is larger than 3. In contrast, our MStar can select multiple starting entities, e.g., Michigan State Univ. and The Ohio State Univ., and transmit conditional messages to them through the highway layer. Thus, MStar can achieve a better reasoning performance than other C-MPNNs on this query. After the highway layer, we follow it with a multi-condition GNN to perform message passing based on the embeddings of multiple starting entities. We also propose a training sample filtering strategy called LinkVerify to reduce the impact of the unvisited target entities. Overall, MStar visits more query-related distant entities in limited steps and provides more conditional information to these entities compared with existing models.

Our main contributions in this paper are summarized as follows:""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk6> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk6> ex:chunkContent """Our main contributions in this paper are summarized as follows:

- We propose a novel inductive KG reasoning framework based on C-MPNN, named MStar. It extends the scope of conditional message passing to improve the predictions of distant target entities.
- We design two modules, SES and highway layer. The SES module performs starting entities selection for visiting distant entities. The highway layer provides shortcuts for efficient conditional message passing, alleviating computation waste during additional propagation.
- We conduct extensive experiments on inductive datasets to demonstrate the effectiveness of our framework and each module. The results show that MStar outperforms the existing state-of-the-art reasoning models and improves the performance on queries with distant target entities.

The rest of this paper is organized as follows. We first discuss related works in Section 2. Then, we describe the reasoning task and propagation mechanisms in
---
4         Z. Shao et al.

Section 3. The details of MStar are presented in Section 4, and the experimental results are reported in Section 5. Finally, in Section 6, we discuss the superiority of MStar and possible extensions in future work.

## 2     Related Work

### 2.1     Knowledge Graph Reasoning""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk7> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk7> ex:chunkContent """## 2     Related Work

### 2.1     Knowledge Graph Reasoning

KG reasoning has been an active research area due to the incompleteness of KGs. Typical KG reasoning models process each triplet independently and extract the latent semantics of entities and relations. To model the semantics of the triplets, TransE [2], TransH [39], TransR [17], and RotatE [29] compute translational distance variously. RESCAL [22], DistMult [44], and ComplEx [35] follow another reasoning paradigm based on semantic matching. Instead of exploring the information implied in a single triplet, R-GCN [28] and CompGCN [36] capture global structure evidence based on graph neural networks (GNNs). These models, however, learn unary fixed embedding from training, which cannot be generalized to emerging entities in the inductive KGs. Instead, our model embodies relational information to encode emerging entities.

### 2.2     Inductive Knowledge Graph Reasoning

One research line of inductive KG reasoning is rule mining, independent of entity identities. RuleN [20] and AnyBURL [19] try to prune the process of rule searching. Neural LP [45] and DRUM [27] propose to learn logical rules in an end-to-end differentiable manner, learning weights for each relation type and path. However, the rules are usually short due to the expensive computation for mining and may not be generalized to distant entities.

Another research line is subgraph extraction. GraIL [32] extracts subgraphs around each candidate triplet and labels the entities with the distance to the head and tail entities. CoMPILE [18], TACT [3], SNRI [43], LogCo [23], and ConGLR [16] follow a similar subgraph-labeling paradigm. However, the subgraphs that these models extract convey insufficient information due to sparsity. These models constitute our baselines for inductive KG reasoning.

### 2.3     Conditional Message Passing Neural Networks""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk8> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk8> ex:chunkContent """### 2.3     Conditional Message Passing Neural Networks

Recently, a variant of GNNs called conditional message passing neural networks (C-MPNNs) [10] propagates messages along the relational paths and encodes pairwise entity embeddings. Given a query head u and a query relation q as conditions, C-MPNNs compute embeddings of (v | u, q) for all entity v. To incorporate conditions into embeddings, NBFNet [50] and A*Net [49] initialize the head entity with the embedding of query relation and propagate in the full KG for each GNN layer. However, conditional information passing is still restricted in the neighborhood of the head entity. Differently, RED-GNN [47], AdaProp [48],
---
Inductive Knowledge Graph Reasoning with MStar          5

and RUN-GNN [41] propagate the message progressively starting from the head
entity without special initialization. During progressive propagation, the involved
entity set is augmented step by step with the neighbor entities of the current
set instead of being a full entity set. Thus, progressive propagation cannot even
visit distant entities in limited steps. MStar alleviates the above problem by
selecting multiple starting entities adaptively for progressive propagation and
transmitting conditional information through shortcuts.

EL-GNN [25] is another work related to C-MPNNs. This study proposes that
C-MPNNs learn the rules of treating the head entity as constant when the head
entity is initialized with conditional information. Thus, EL-GNN learns more
rules by assigning unique embeddings for entities whose out-degree in the KG
reaches a specific threshold. However, the degree and entity-specific embeddings
are fixed, which violates the nature of inductive KG reasoning. Our MStar se-
lects starting entities according to the query and generates conditional entity
embeddings, which can be applied to unseen entities.

## 2.4 Skip Connection""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk9> .

<https://dblp.org/rec/journals/corr/abs-2407-10430/chunk9> ex:chunkContent """## 2.4 Skip Connection

Skip connection [8] is a popular technique in deep learning that skips one or
more layers. Skipping layers contributes to addressing vanishing or exploding
gradients [31] by providing a highway for the gradients. ResNet [8] constructs
the highway by adding input x and output F(x). DenseNet [9] provides multiple
highways by concatenating the input of each layer. These models transmit the
input in shallow layers directly to the target deeper layer in an efficient way.
Inspired by skip connection, MStar constructs a highway with several new edges
to transmit messages faster and propagate to farther entities.

# 3 Preliminaries

Knowledge Graph A KG $G = (E, R, F)$ is composed of finite sets of entities
$E$, relations $R$, and triplets $F$. Each triplet $f \\in F$ describes a fact from head
entity to tail entity with a specific relation, i.e., $f = (u, q, v) \\in E \\times R \\times E$, where
$u$, $q$, and $v$ denote the head entity, relation, and tail entity, respectively.

(Inductive) Knowledge Graph Reasoning To complete the missing triplet
in real-world KGs, KG reasoning is proposed to predict the target tail entity
or head entity with a given query $(u, q, ?)$ or $(?, q, v)$. Given a source KG $G =
(E, R, F)$, inductive KG reasoning aims to predict the triplets involved in the
target KG $G' = (E', R', F')$, where $R' \\subseteq R$, $E' \\not\\subset E$, and $F' \\not\\subset F$.

Starting Entities in Progressive Propagation GNNs transmit messages
based on the message propagation framework [7, 40]. This framework prepares
an entity set to transmit messages for each propagation step. Full propagation
---
6          Z. Shao et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk10> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk10> ex:chunkContent """### 3.3     Instruction Construction

For a query q = (h, r, ?), we construct the prompt P by integrating four pieces of
information: Query Q, Description D, Neighbor facts N and Candidate entities
C, which can be represented as:

$$P(q) = [Q; D; N ; C],                        (1)$$

where ";" is the concatenation operation between texts. We give an example of
querying (Titanic, film language, ?), as illustrated in Fig. 1.

Query refers to a natural language sentence containing the incomplete fact
(h, r, ?). Instead of designing a complex natural language question to prompt
off-the-shelf LLMs, we simply concatenate the entity and relation names in the
form of a triplet and indicate which entity is missing. During the finetuning
process, the LLM M will be trained to fit our prompt format.

Description is the descriptive text of h, which contains abundant information
about the entity. This additional information helps the LLM M get a better
understand of the entity h. For instance, we depict Titanic in Fig. 1 as a 1997
American epic romantic disaster film directed by James Cameron.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk11> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk11> ex:chunkContent """Neighbor facts are obtained by sampling facts related to the entity h. As there
may be numerous facts associated with h, we devise a straightforward yet effective
sampling mechanism, namely relation co-occurrence (RC) sampling. It is rooted
in relation co-occurrence, and streamlines the number of facts while ensuring the
inclusion of relevant information. The intuition behind RC sampling lies in the
observation that the relations frequently co-occurring with r are considered to be
crucial to complete (h, r, ?). For example, the relations film language and film
country in Fig. 1 often co-occur, because the language of a film is closely related
to the country where it is released. Therefore, we can infer that the language
of Titanic is highly likely to be English from the fact that it is an American
film. Drawing on the above observation, we sort the neighboring relations of h
based on their frequency of co-occurrences with r and subsequently select facts
containing these relations until a preset threshold γ is reached.

Candidate entities are the names of top-m entities ranked by the KG embedding
model ME. We retain the order of candidate entities since the order reflects the
confidence of each entity from ME. We instruct the LLM M to select an entity
from the given candidates, thereby avoiding the grounding errors.

### 3.4     Truncated Sampling

We design a sampling method to select representative samples to reduce instruction data. The main idea is to opt for high-confidence samples indicated by ME,
thereby empowering M to acquire intrinsic semantic knowledge of ME efficiently.
---
Finetuning Generative LLMs with Discrimination Instructions 7

By finetuning M on these selected instruction samples, we effectively mitigate
the computational burden associated with training.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk12> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk12> ex:chunkContent """By finetuning M on these selected instruction samples, we effectively mitigate
the computational burden associated with training.

We take the sample fact (h, r, t) with query (h, r, ?) and answer entity t as an
example. We denote the sample fact as s. Specifically, we assess the confidence
of s from both global and local perspectives. The global confidence Confglobal(s)
is computed as 1/R(h,r,t), where R(h, r, t) is the ranking of t for the query (h, r, ?).
We name it as the global confidence because it measures the ranking of t among
all candidates in the KG.

Considering that the global confidence ignores the differences between two
queries whose answer entities are in the same rank, inspired by [32], we present
the local confidence to measure the score of a fact itself. The local confidence
Conflocal(s) is computed as f(h, r, t), i.e., the score of s obtained from ME. It
is worth noting that Conflocal(s) is assigned as 0 if t is not ranked within the
top-m. Finally, the confidence of s is determined by the weighted sum of global
and local confidence, expressed as follows:

Conf(s) = Confglobal(s) + α × Conflocal(s), (2)

where α serves as a hyperparameter to balance the global and local confidence.
Subsequently, we introduce a threshold β and keep the samples with confidence
greater than β as the final instruction data.

### 3.5 Instruction Tuning with Knowledge Adaption

Given the prompt P(q), we finetune the LLM M to generate the entity name of
t. The loss of instruction tuning is a re-construction loss:

$$LM = -\\sum_{i=1}^N \\log p(y_i | y_{<i}, P(q)),$$  (3)

where N denotes the number of tokens in the entity name of t, yi (i = 1, 2, . . . , N)
represents the i-th token, and p(yi | y<i, P(q)) signifies the probability of generating yi with M given the prompt P(q) and tokens that have been generated.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk13> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk13> ex:chunkContent """The facts provided in P(q) are presented in the text format, losing the global
structure information of KGs. Therefore, we propose to inject the embeddings
learned from KG structure into M to further improve its graph reasoning ability.
We align the embeddings from ME with the semantic space of M, to get the
knowledge representations:

$$\\hat{e} = W_2(SwiGLU(W_1 \\cdot e + b_1)) + b_2,$$ (4)

where ê denotes the knowledge representation obtained based on the embeddings
e. W1 ∈ ℝd0×d1, b1 ∈ ℝd1, W2 ∈ ℝd2×d1, and b2 ∈ ℝd2 are trainable weights.
d0 is the embedding dimension of ME, d2 is the hidden size of M, and d1 is a
hyperparameter. SwiGLU is a common activation function used in LLaMA [27].

Considering that ME scores a fact based on the embeddings of the query q
and the candidate entity t, we inject the knowledge representations of q and all
---
8         Y. Liu et al.

Table 1. The statistics of datasets.

| Datasets   | #Entities | #Relations | #Training | #Validation | #Testing |
|------------|-----------|------------|-----------|-------------|----------|
| FB15K-237  | 14,541    | 237        | 272,115   | 17,535      | 20,466   |
| WN18RR     | 40,943    | 11         | 86,835    | 3,034       | 3,134    |

candidate entities in C into M. We add two special placeholders "[QUERY]" and
"[ENTITY]" to indicate that there will be a knowledge representation from ME,
as shown in Fig. 1. Specifically, we place a "[QUERY]" after the missing entity in
Q and an "[ENTITY]" after each entity name in C.

## 4 Experiments

### 4.1 Experiment Setup""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk14> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk14> ex:chunkContent """## 4 Experiments

### 4.1 Experiment Setup

Datasets. In the experiments, we use two benchmark datasets, FB15K-237 [26]
and WN18RR [8], to evaluate our proposed framework. FB15K-237 consists of
real-world named entities and their relations, constructed based on Freebase
[2]. On the other hand, WN18RR contains English phrases and the semantic
relations between them, constructed based on WordNet [18]. Notably, these two
datasets are updated from their previous versions (i.e., FB15K and WN18 [3])
respectively, they both removed some inverse edges to prevent data leakage. For
a detailed overview, the statistics of these two datasets are shown in Table 1.

Evaluation protocol. For each test fact, we conduct both head entity prediction
and tail entity prediction by masking the corresponding entities, respectively. The
conventional metrics are ranking evaluation metrics, i.e., Hits@k (k = 1, 3, 10) and
mean reciprocal rank (MRR). Hits@k is the percentage of queries whose correct
entities are ranked within the top-k, and MRR measures the average reciprocal
ranks of correct entities. In our framework, the finetuned LLM selects an entity
as the answer from the ranking list of candidates. To assess its performance and
make the results comparable to existing work, we move the selected entity to the
top of the ranking list, and other candidates remain unchanged. We then use
Hits@k and MRR to assess the reranked candidate list. We report the averaged
results of head and tail entity prediction under the filtered ranking setting [3].""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk15> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk15> ex:chunkContent """Implementation details. We run our experiments on two Intel Xeon Gold
CPUs, an NVIDIA RTX A6000 GPU, and Ubuntu 18.04 LTS. Text attributes
are taken from KG-BERT [36]. We select three representative embedding-based
models to experiment with DIFT, namely, TransE, SimKGC, and CoLE. Each
embedding-based model is pre-trained on the training set. We obtain the top
20 predicted entities for each query in the validation set and test set. We also
obtain the embeddings of all queries and entities for knowledge adaption.

As for the instruction tuning, we select LLaMA-2-7B³ as the LLM. We employ
LoRA [14] for parameter-efficient finetuning. The hyperparameters of LoRA are

³ https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
---
# Finetuning Generative LLMs with Discrimination Instructions

set to r = 64, alpha = 16, and dropout = 0.1. We introduce LoRA for all query and value projection matrices in the self-attention module of Transformer. To further speed up the finetuning process, we quantize the LLM by QLoRA [9], which quantizes the LLM parameters to 4 bits by introducing Double Quantization with 4-bit NormalFloat data type. Inspired by KICGPT [31], we partition the validation set into two parts according to 9:1. The first part is used to finetune the LLM to follow the instructions, and the second part is used for hyperparameter selection. Note that we do not use the training data of each benchmark to construct instructions. Since the embedding-based model has learned the training data, it would rank the correct entity at the first in the candidate list for most training facts. If we use these candidate lists to construct instructions, the LLM would learn a tricky solution to pick the first candidate as an answer, which is not the goal of our finetuning.

## 4.2 Baselines""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk16> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk16> ex:chunkContent """## 4.2 Baselines

### Embedding-based models. 
We choose eight structure-based models as baselines. Three triplet-based models are selected, including TransE [3], RotatE [25], and TuckER [1]. We also choose two path-based models. Neural-LP [34] is the first model that learns logic rules from relation paths and NCLR [6] is the state-of-the-art path-based model. The remaining models are all graph-based. CompGCN [28] employs GCNs to encode the multi-relational graph structure of the KG, while HittER [5] leverages the Transformer architecture. NBFNet [39] currently performs best among the structure-based models. We also select five PLM-based models as the competitors, namely KG-BERT [36], StAR [29], MEM-KGC [7], SimKGC [30], and CoLE [17]. Note that, SimKGC stands the state-of-the-art link prediction model on WN18RR, which benefits from efficient contrastive learning. CoLE promotes PLMs and structure-based models mutually to achieve the best performance on FB15K-237 among PLM-based models. To ensure a fair comparison, we present results derived solely from N-BERT, the PLM-based KG embedding module within CoLE, rather than the entire CoLE framework.

### Generation-based models. 
We select three generation-based KG completion models, all of which are either based on BART or T5, namely, GenKGC [33], KGT5 [23], and KG-S2S [4]. Further, we select two recent models based on LLMs as baselines. ChatGPTone-shot is a baseline proposed by AutoKG [38], and KICGPT evaluates it on the whole test sets of FB15K-237 and WN18RR for comparison. KICGPT is the most competitive KG completion model, which employs RotatE to provide the top-m predicted entities for each query and re-ranks these candidates with ChatGPT through multi-round interactions. We also report the performance of DIFT without finetuning, denoted by LLaMA+TransE, LLaMA+SimKGC, and LLaMA+CoLE, respectively.

## 4.3 Main Results""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk17> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk17> ex:chunkContent """## 4.3 Main Results

We report the link prediction results on FB15K-237 and WN18RR in Table 2. Generally speaking, our proposed framework DIFT achieves the best performance
---
10         Y. Liu et al.

## Table 2. Link prediction Results. We mark the best scores in terms of each metric in bold and the second-best scores are underlined. We reproduce the results of TransE, SimKGC and CoLE using their source code and hyperparameters. The results of Neural-LP are obtained from [21]. The results of GenKGC, KGT5 and KG-S2S are obtained from [4]. The results of other baselines are taken from their respective original papers.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk18> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk18> ex:chunkContent """| Models | FB15K-237 | WN18RR |
|--------|-----------|--------|
| | MRR | Hits@1 | Hits@3 | Hits@10 | MRR | Hits@1 | Hits@3 | Hits@10 |
| **Embedding-based** |
| TransE | 0.312 | 0.212 | 0.354 | 0.510 | 0.225 | 0.016 | 0.403 | 0.521 |
| RotatE | 0.338 | 0.241 | 0.375 | 0.533 | 0.476 | 0.428 | 0.492 | 0.571 |
| TuckER | 0.358 | 0.266 | 0.394 | 0.544 | 0.470 | 0.443 | 0.482 | 0.526 |
| Neural-LP | 0.237 | 0.173 | 0.259 | 0.361 | 0.381 | 0.368 | 0.386 | 0.408 |
| NCRL | 0.300 | 0.209 | - | 0.473 | 0.670 | 0.563 | - | **0.850** |
| CompGCN | 0.355 | 0.264 | 0.390 | 0.535 | 0.479 | 0.443 | 0.494 | 0.546 |
| HittER | 0.373 | 0.279 | 0.409 | 0.558 | 0.503 | 0.462 | 0.516 | 0.584 |
| NBFNet | 0.415 | 0.321 | 0.454 | **0.599** | 0.551 | 0.497 | 0.573 | 0.666 |
| KG-BERT | - | - | - | 0.420 | 0.216 | 0.041 | 0.302 | 0.524 |
| StAR | 0.365 | 0.266 | 0.404 | 0.562 | 0.551 | 0.459 | 0.594 | 0.732 |
| MEM-KGC | 0.346 | 0.253 | 0.381 | 0.531 | 0.557 | 0.475 | 0.604 | 0.704 |
| SimKGC | 0.338 | 0.252 | 0.364 | 0.511 | 0.671 | 0.595 | 0.719 | 0.802 |
| CoLE | 0.389 | 0.294 | 0.429 | 0.572 | 0.593 | 0.538 | 0.616 | 0.701 |
| **Generation-based** |
| GenKGC | - | 0.192 | 0.355 | 0.439 | - | 0.287 | 0.403 | 0.535 |
| KGT5 | 0.276 | 0.210 | - | 0.414 | 0.508 | 0.487 | - | 0.544 |
| KG-S2S | 0.336 | 0.257 | 0.373 | 0.498 | 0.574 | 0.531 | 0.595 | 0.661 |
| ChatGPT<sub>one-shot</sub> | - | 0.267 | - | - | - | 0.212 | - | - |
| KICGPT | 0.412 | 0.327 | 0.448 | 0.581 | 0.564 | 0.478 | 0.612 | 0.677 |
| LLaMA + TransE | 0.232 | 0.080 | 0.321 | 0.502 | 0.202 | 0.037 | 0.360 | 0.516 |
| LLaMA + SimKGC | 0.236 | 0.074 | 0.335 | 0.503 | 0.391 | 0.065 | 0.695 | 0.798 |
| LLaMA + CoLE | 0.238 | 0.033 | 0.387 | 0.561 | 0.374 | 0.117 | 0.602 | 0.697 |
| DIFT + TransE | 0.389 | 0.322 | 0.408 | 0.525 | 0.491 | 0.462 | 0.496 | 0.560 |
| DIFT + SimKGC | 0.402 | 0.338 | 0.418 | 0.528 | **0.686** | **0.616** | **0.730** | 0.806 |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk19> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk19> ex:chunkContent """| DIFT + TransE | 0.389 | 0.322 | 0.408 | 0.525 | 0.491 | 0.462 | 0.496 | 0.560 |
| DIFT + SimKGC | 0.402 | 0.338 | 0.418 | 0.528 | **0.686** | **0.616** | **0.730** | 0.806 |
| DIFT + CoLE | **0.439** | **0.364** | **0.468** | 0.586 | 0.617 | 0.569 | 0.638 | 0.708 |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk20> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk2> ex:chunkContent """**Keywords:** Knowledge graph completion · Large language model · Instruction tuning.

## 1 Introduction

Knowledge graphs (KGs) store real-world facts in multi-relational structures, where nodes represent entities and edges are labeled with relations to describe facts in the form of triplets like (head entity, relation, tail entity). KGs often face the incompleteness problem [12], which adversely affects the performance of downstream knowledge-intensive applications such as question answering [11,24] and recommender systems [13]. KG completion models are designed to resolve the incompleteness issue by inferring the missing facts based on the facts already in KGs. Conventional KG completion models are based on KG embeddings. Given an incomplete fact where either the head or tail entity is missing and requires prediction, embedding-based models first compute the plausibility for candidate entities using an embedding function of entities and relations and then rank them to obtain predictions. Entity and relation embeddings can be learned based on either graph structures [1,3,35,39] or text attributes [17,20,29,30,36].
---
2         Y. Liu et al.

In recent years, motivated by the impressive performance of generative pre-trained language models (PLMs) such as T5 [22] and BART [16], some models convert KG completion to a sequence-to-sequence generation task [4,23,33]. Given an incomplete fact, generation-based models first construct a natural language query with text attributes of the given entity and relation, and then ask a generative PLM to generate an answer directly. Finally, they ground the answer to the entities in the KG, which, however, inevitably brings errors.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk3> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk20> ex:chunkContent """## Table 3. Results of ablation study

| Models | FB15K-237 | WN18RR |
|--------|-----------|--------|
| | MRR | Hits@1 | Hits@3 | Hits@10 | MRR | Hits@1 | Hits@3 | Hits@10 |
| DIFT | **0.439** | **0.364** | 0.468 | 0.586 | **0.617** | **0.569** | 0.638 | 0.708 |
| w/o truncated sampling | 0.423 | 0.338 | 0.459 | 0.587 | 0.600 | 0.537 | **0.638** | **0.712** |
| w/o RC sampling | 0.434 | 0.354 | 0.468 | **0.588** | 0.614 | 0.564 | 0.636 | 0.708 |
| w/o description | 0.436 | 0.358 | 0.467 | 0.586 | 0.603 | 0.548 | 0.630 | 0.705 |
| w/o neighbors | 0.438 | 0.360 | **0.469** | **0.588** | 0.610 | 0.558 | 0.637 | 0.708 |
| w/o knowledge adaption | 0.437 | 0.358 | 0.468 | 0.587 | 0.612 | 0.560 | 0.637 | 0.708 |
---
Finetuning Generative LLMs with Discrimination Instructions        11

in most metrics on two datasets. Compared with the selected embedding-based
models TransE, SimKGC, and CoLE, DIFT improves the performance of these
models on both datasets, significantly in terms of Hits@1. Without finetuning, the
performance of DIFT drops dramatically, which demonstrates that it is necessary
to finetune the LLM for KG completion task.

Compared with the LLM-based model ChatGPTone-shot, DIFT consistently
outperforms it in terms of Hits@1, regardless of the integration with any of
the embedding-based models. This indicates that prompting ChatGPT with
in-context learning is less effective than finetuning a smaller LLM with the help
of existing embedding-based models for link prediction. Compared with the most
competitive baseline model KICGPT which also provides the LLM with candidate
entities, the relative improvement brought by DIFT is less. However, KICGPT
needs multi-round interactions with ChatGPT, which has 175B parameters. In
contrast, DIFT finetunes a small LLaMA with only 7B parameters.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk21> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk21> ex:chunkContent """Comparing different metrics, we find that the performance improvement is
more significant on Hits@1 while less significant on Hits@10. In DIFT, we ask
the LLM to select the plausible entity from the given candidate list. Given that
the correct entity is more likely to be ranked in the top 10 entities rather than
outside the top 10, the LLM is more likely to select an entity in the top 10 as the
answer. Thus, the improvement is more obvious on Hits@1 rather than Hits@10.

We also find that the performance improvement on FB15K-237 is more
significant than that on WN18RR. This discrepancy can be attributed to the
stark disparity in density between the two datasets: FB15K-237 is considerably
denser than WN18RR, implying a richer reservoir of knowledge. More knowledge
leads to better improvement since the knowledge is provided for the LLM to
comprehend in the form of prompts and embeddings.

## 4.4 Ablation Study

For the ablation study, we select CoLE as the embedding-based model to provide
candidate entities since DIFT with CoLE performs best overall on both datasets.
We evaluate the effectiveness of two kinds of sampling mechanisms, i.e., truncated
sampling and RC sampling, as well as three kinds of support information, i.e.,
description, neighbors, and embeddings used in knowledge adaption.

From the results presented in Table 3, it is evident that all components
contribute a lot to DIFT. Among all these components, truncated sampling has
the most substantial impact on performance. The Hits@1 score experiences a
degradation of at least 5.6% in the absence of truncated sampling. This shows
that this mechanism can effectively select useful instruction data for the LLM to
learn intrinsic semantic knowledge of the embedding-based model.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk22> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk22> ex:chunkContent """We can also observe that the impact of description, neighbors, and RC sam-
pling differs significantly between the two datasets. Without description, the
Hits@1 will drop more on WN18RR. This is attributed to WN18RR being a
sparse KG with less structural information compared with FB15K-237. There-
fore, it needs additional description to enrich entity information, aiding in the
differentiation between similar entities. In addition, neighbor information is also
---
12                                     Y. Liu et al.

| Number of candidates | Hits@1 Score | Training Time (s) | Number of candidates | Hits@1 Score | Training Time (s) |
|----------------------|--------------|-------------------|----------------------|--------------|-------------------|
| 10                   | 0.34         | 45000             | 10                   | 0.55         | 12000             |
| 20                   | 0.35         | 55000             | 20                   | 0.56         | 14000             |
| 30                   | 0.37         | 65000             | 30                   | 0.57         | 18000             |
| 40                   | 0.37         | 72000             | 40                   | 0.57         | 20000             |
|                      | (a) FB15K-237                    |                      | (b) WN18RR                     |

Fig. 2. Hits@1 results and training time of DIFT on FB15K-237 and WN18RR along
with the numbers of candidate entities.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk23> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk23> ex:chunkContent """Fig. 2. Hits@1 results and training time of DIFT on FB15K-237 and WN18RR along
with the numbers of candidate entities.

more important for WN18RR. This is because many correct entities will directly
appear in the neighbor facts of WN18RR, facilitating the LLM in making ac-
curate predictions. Instead, the improvement of Hits@1 is more significant on
FB15K-237 than WN18RR for RC sampling. We posit that this is attributed to
FB15K-237 being highly dense, with each entity having numerous neighbor facts.
Many of these facts are irrelevant to the query, leading to interference. Hence,
RC sampling can minimize irrelevant facts and enhance effectiveness.

As for knowledge adaption, we observe consistent performance improvements
across the two datasets, indicating good generality and robustness.

### 4.5 Further Analyses""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk24> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk24> ex:chunkContent """As for knowledge adaption, we observe consistent performance improvements
across the two datasets, indicating good generality and robustness.

### 4.5 Further Analyses

Effect of the number of candidates. In Section 4.3, we set the number of
candidate entities m provided by the embedding-based model to 20. Here we
investigate the effect of m on the performance and the training time of DIFT.
The results are shown in Fig. 2. First, for the training time, we find that it grows
linearly when we increase m. It is intuitive since increasing m leads to longer
prompts. Second, as for the performance of DIFT, we find that the performance
is best when m is set to 30 on FB15K-237, and there is a slight drop when m
is set to 40. The same observation can be found on WN18RR if we continue to
increase m after 20. This indicates that blindly increasing the number of candidate
entities cannot improve performance. Third, we find that the performance is
best when m is set to 30 on FB15K-237 and 20 on WN18RR. That is to say, to
achieve the best performance, DIFT needs more candidate entities on FB15K-237
than WN18RR. We think that this discrepancy arises from the generally inferior
performance of models on FB15K-237 compared to WN18RR. Consequently, to
ensure the presence of answer entities within the prompts, a larger m is advisable
on FB15K-237 than on WN18RR.

Effect of truncated sampling thresholds. In Section 3.4, we use a threshold β
to control the quantity of instruction data. To investigate the impact of β on the
performance and the training time of DIFT, we conduct an experiment by setting
---
## Finetuning Generative LLMs with Discrimination Instructions

### Fig. 3. Hits@1 results and training time of DIFT on FB15K-237 and WN18RR along with the threshold for truncated sampling.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk25> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk25> ex:chunkContent """### Fig. 3. Hits@1 results and training time of DIFT on FB15K-237 and WN18RR along with the threshold for truncated sampling.

| Threshold | Hits@1 Score (FB15K-237) | Training Time (FB15K-237) | Hits@1 Score (WN18RR) | Training Time (WN18RR) |
|-----------|--------------------------|---------------------------|----------------------|------------------------|
| 0         | 0.36                     | 70000                     | 0.55                 | 18000                  |
| 0.05      | 0.36                     | 60000                     | 0.57                 | 16000                  |
| 0.5       | 0.34                     | 30000                     | 0.56                 | 8000                   |
| 1         | 0.32                     | 10000                     | 0.54                 | 2000                   |

### Fig. 4. Correct predictions of DIFT and CoLE on FB15K-237 and WN18RR.

The light blue area represents the accurate triplets predicted by DIFT, excluding those that can also be predicted by CoLE. The dark green area illustrates the overlapping triplets predicted accurately by DIFT and CoLE. The light green area represents the accurate triplets predicted by CoLE, excluding those that can also be predicted by DIFT.

| Dataset    | DIFT - CoLE | DIFT ∩ CoLE | CoLE - DIFT |
|------------|-------------|-------------|-------------|
| FB15K-237  | 4695        | 10191       | 1848        |
| WN18RR     | 344         | 3221        | 154         |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk26> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk26> ex:chunkContent """Different values for β. In particular, we change β from 0.05 in the main experiments to 0, 0.5, and 1.0 respectively. The results are shown in Fig. 3. We have the following observations. First, with β increasing, the quantity of instruction data decreases, and therefore the training time also decreases accordingly. Second, the performance drops when we set β to 0 on both datasets, which indicates that increasing the quantity of instruction data does not necessarily improve the performance, and its quality also affects the performance. Third, if we strictly ensure that the quality of the instruction data is high enough, i.e., we set β to 0.5 or 1.0, the performance of DIFT also drops. We think there are mainly two reasons: (1) When β is set to 0.5 or 1.0, the limited instruction data is not enough to finetune the LLM sufficiently. (2) Instruction data with high confidence usually places the answer entity among the first few in the candidate list. Therefore, finetuning the LLM with this data will cause the LLM to always choose the top-ranked entities, regardless of whether they are correct.

### Comparison of DIFT and basic embedding models. 
We further investigate the predictions of DIFT in comparison with those of the selected embedding-based
---

## Table 4. Link prediction Results of different versions of LLaMA-2-7B.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk27> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk27> ex:chunkContent """## Table 4. Link prediction Results of different versions of LLaMA-2-7B.

| Models | FB15K-237 | WN18RR |
|--------|-----------|--------|
| | MRR | Hits@1 | Hits@3 | Hits@10 | MRR | Hits@1 | Hits@3 | Hits@10 |
|--------|-----------|--------|
| LLaMA-2-7B-Chat |
| DIFT + TransE | 0.389 | 0.322 | 0.408 | 0.525 | 0.491 | 0.462 | 0.496 | 0.560 |
| DIFT + SimKGC | 0.402 | 0.338 | 0.418 | 0.528 | 0.686 | 0.616 | 0.730 | 0.806 |
| DIFT + CoLE | 0.439 | 0.364 | 0.468 | 0.586 | 0.617 | 0.569 | 0.638 | 0.708 |
| LLaMA-2-7B-Foundation |
| DIFT + TransE | 0.393 | 0.328 | 0.409 | 0.525 | 0.481 | 0.450 | 0.486 | 0.552 |
| DIFT + SimKGC | 0.405 | 0.341 | 0.420 | 0.530 | 0.682 | 0.608 | 0.731 | 0.806 |
| DIFT + CoLE | 0.439 | 0.363 | 0.468 | 0.587 | 0.619 | 0.571 | 0.641 | 0.710 |

model. For this analysis, we continue to employ CoLE as the embedding-based
model to analyze the results. We draw Venn diagrams to highlight both their
shared and individual correct predictions, as illustrated in Fig. 4. It is obvious
that in addition to the shared correct predictions, DIFT can also get some correct
predictions by itself. Conversely, we observe instances where CoLE makes correct
inferences that DIFT fails to replicate. Based on the divergence between the
correct predictions of DIFT and CoLE, we can conclude that the LLM does not
repeat the predicted entities by CoLE blindly, instead, it can reason the missing
facts based on its knowledge obtained in the pre-training stage.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk28> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk28> ex:chunkContent """Comparison of different versions of the LLM. In the main experiment,
we employ LLaMA-2-7B-Chat as the LLM for DIFT. In order to investigate
the influence of different versions of the LLM on the performance of DIFT, we
experiment with the foundation version, denoted as LLaMA-2-7B-Foundation.
The results are shown in Table 4. DIFT with LLaMA-2-7B-Foundation performs
slightly better than that with LLaMA-2-7B-Chat on FB15K-237, but the obser-
vation is the opposite on WN18RR. Generally speaking, DIFT achieves a similar
performance no matter which version of the LLM are employed. It demonstrates
the robustness and generalization of DIFT for different LLM versions.

### 4.6 The Finetuning Learns What?

In this section, we investigate what the LLM learns during our finetuning process.
DIFT employs a lightweight embedding-based model to provide candidate entities
for both finetuning and inference. A natural question arises: Does the LLM learn
the preference of the embedding-based model predictions or the knowledge in
the KG? To answer this question, we design the following experiment to evaluate
the effect of the candidate order in both the finetuning and inference stages.

Effect of the order of candidates. DIFT takes the top-m predicted entities
from the embedding-based model as the candidates for the LLM. We retain the
order of candidates because we assume that the order reflects the knowledge
---
Finetuning Generative LLMs with Discrimination Instructions    15

Table 5. Influence of the order of candidates""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk29> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk29> ex:chunkContent """Table 5. Influence of the order of candidates

| Ordered finetuning | Ordered inference | FB15K-237 |     |     |     | WN18RR |     |     |     |
|---------------------|-------------------|-----------|-----|-----|-----|---------|-----|-----|-----|
|                     |                   | MRR       | Hits@1 | Hits@3 | Hits@10 | MRR | Hits@1 | Hits@3 | Hits@10 |
| ✓                   | ✓                 | 0.439     | 0.364 | 0.468 | 0.586 | 0.686 | 0.616 | 0.730 | 0.806 |
| ✓                   | ✗                 | 0.328     | 0.168 | 0.441 | 0.584 | 0.484 | 0.233 | 0.712 | 0.806 |
| ✗                   | ✓                 | 0.423     | 0.333 | 0.466 | 0.589 | 0.627 | 0.500 | 0.731 | 0.809 |
| ✗                   | ✗                 | 0.417     | 0.324 | 0.464 | 0.591 | 0.625 | 0.493 | 0.736 | 0.808 |

learned by the embedding-based model. Here, to investigate the influence of the
order of candidates, we shuffle the candidates in the finetuning or inference stages
to ask the LLM to select an entity from the shuffled candidate list. Remember
that the shuffled candidate list is only used for entity selection, we move the
selected entity to the top of the ranking list from the embedding-based model for
evaluation. Results are shown in Table 5, and we have the following observations.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk30> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk3> ex:chunkContent "More recently, some work attempts to conduct KG completion using large language models (LLMs), such as ChatGPT and LLaMA [27]. Given an incomplete fact, KICGPT [31] first constructs query prompts with demonstration facts and the top-m candidate entities predicted by a pre-trained KG completion model. Then, it engages in a multi-round online interaction with ChatGPT using these query prompts. Finally, it rearranges these candidates according to the response of ChatGPT. This method may not make full use of the reasoning ability of LLMs because the LLMs (e.g., ChatGPT) may not fit the KG well. Besides, the multi-round interaction costs too much. In contrast, KG-LLM [37] converts KG completion queries to natural language questions and finetunes LLMs (e.g., LLaMA-7B) to generate answers. It then uses a heuristic method to ground the output of LLMs to KG entities: if the output text contains an entity name, then this entity is selected as the answer. The errors in such a grounding process cause KG-LLM to lag behind the state-of-the-art KG completion models. Besides, generation-based models obtain multiple output texts and rank them by the generation probabilities, which is time-consuming and unsuitable for LLMs." ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk4> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk30> ex:chunkContent """On FB15K-237, we employ CoLE as the embedding-based model. We can find
that the performance drops dramatically if we finetune the LLM with ordered
candidates but shuffle the candidates during inference. We think the reason is
that ordered candidates instruct the LLM to select within the top few entities as
they are more plausible. Therefore, the LLM still focus on the top few candidates
during inference, even though the candidates are shuffled. When we finetune the
LLM with shuffled candidates, we find that the performance changes slightly
whether the candidates are shuffled or not during inference. The reason is that
the LLM has no idea about the preference that the top few candidates are more
plausible, so it can not benefit from the order of candidates.

On WN18RR, we use SimKGC as the embedding-based model and similar
observations can be found. However, we find that the performance of DIFT is
even worse than SimKGC when we finetune the LLM with shuffled candidates.
This demonstrates that the LLM can not outperform SimKGC solely based on
its inherent knowledge without prediction preferences.

Based on the above analyses, it appears that our DIFT not only captures
prediction preferences but also primarily acquires knowledge from the KG.

Case study. To explore how DIFT improves performance compared with the
selected embedding-based models, we conduct a case study on DIFT (integrating
CoLE), TransE, SimKGC, and CoLE. Table 6 presents the Hits@1 results of
the four models on three queries from FB15K-237, in which the entities marked
with horizontal lines at the bottom are the answers. In the first two cases, DIFT
consistently performs accurately while the other models all predict wrong entities.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk31> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk31> ex:chunkContent """- In Case 1, the contextual description of the head entity, "It tells the story
  of an aspiring actress named Betty Elms, newly arrived in Los Angeles
  ...", offers ample support to determine the answer "Los Angeles", and our
  DIFT generates the correct entity name, indicating that DIFT has improved
  contextual inference capability compared with the embedding-based models.
---
16        Y. Liu et al.

Table 6. Case study on three queries from FB15K-237. Correct answers are underlined.

| | Case 1 | Case 2 | Case 3 |
|------------|------------------|----------------|------------------|
| Head entity | Mulholland Drive | Shonda Rhimes | ? |
| Relation | featured film locations | gender | film language |
| Tail entity | ? | ? | English language |
| DIFT | Los Angeles | Female | The Last King of Scotland |
| TransE | Paris | Male | Pan's Labyrinth |
| SimKGC | Berkeley | Male | The Illusionist |
| CoLE | New York City | Male | The Illusionist |

- In Case 2, neither the description nor the neighbor information provides clues to Shonda Rhimes' gender. It is difficult for embedding-based models to infer the correct entity based on such incomplete knowledge. Instead, DIFT has open knowledge and powerful commonsense reasoning ability, allowing it to overcome this limitation and predict the correct answers. This case shows the complementarity of embedding-based models and LLMs in our framework.

- In Case 3, despite DIFT inferring an "incorrect" entity "The Last King of Scotland", it is crucial to highlight that the underlying issue is associated with the dataset, not DIFT itself. This is because the language of "The Last King of Scotland" is also English, but FB15K-237 lacks this specific knowledge. This case demonstrates that DIFT is capable of leveraging open knowledge in LLMs, surpassing the constraints of closed knowledge in KGs.

## 5 Conclusion and Future Work""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk32> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk32> ex:chunkContent """## 5 Conclusion and Future Work

In this paper, we propose a novel KG completion framework DIFT. It finetunes generative LLMs with discrimination instructions using LoRA, which does not involve grounding the output of LLMs to entities in KGs. To further reduce the computation cost and make DIFT more efficient, we propose a truncated sampling method to select facts with high confidence for finetuning. KG embeddings are also added into the LLMs to improve the finetuning effectiveness. Experiments show that DIFT achieves state-of-the-art results on KG completion. In future work, we plan to support other KG tasks such as KGQA and entity alignment.

Acknowledgments This work was funded by National Natural Science Foundation of China (No. 62272219), Postdoctoral Fellowship Program of CPSF (No. GZC20240685), and CCF-Tencent Rhino-Bird Open Research Fund.

Supplemental Material Statement: Source code and datasets are available on GitHub: https://github.com/nju-websoft/DIFT.
---
Finetuning Generative LLMs with Discrimination Instructions    17

## References""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk33> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk33> ex:chunkContent """1. Balazevic, I., Allen, C., Hospedales, T.M.: TuckER: Tensor factorization for knowledge graph completion. In: EMNLP-IJCNLP. pp. 5185–5194 (2019)
2. Bollacker, K.D., Evans, C., Paritosh, P.K., Sturge, T., Taylor, J.: Freebase: A collaboratively created graph database for structuring human knowledge. In: SIGMOD. pp. 1247–1250 (2008)
3. Bordes, A., Usunier, N., García-Durán, A., Weston, J., Yakhnenko, O.: Translating embeddings for modeling multi-relational data. In: NIPS. pp. 2787–2795 (2013)
4. Chen, C., Wang, Y., Li, B., Lam, K.: Knowledge is flat: A seq2seq generative framework for various knowledge graph completion. In: COLING. pp. 4005–4017 (2022)
5. Chen, S., Liu, X., Gao, J., Jiao, J., Zhang, R., Ji, Y.: HittER: Hierarchical transformers for knowledge graph embeddings. In: EMNLP. pp. 10395–10407 (2021)
6. Cheng, K., Ahmed, N.K., Sun, Y.: Neural compositional rule learning for knowledge graph reasoning. In: ICLR (2023)
7. Choi, B., Jang, D., Ko, Y.: MEM-KGC: Masked entity model for knowledge graph completion with pre-trained language model. IEEE Access 9, 132025–132032 (2021)
8. Dettmers, T., Minervini, P., Stenetorp, P., Riedel, S.: Convolutional 2D knowledge graph embeddings. In: AAAI. pp. 1811–1818 (2018)
9. Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: QLoRA: Efficient finetuning of quantized LLMs. arXiv 2305.14314 (2023)
10. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: NAACL. pp. 4171–4186 (2019)
11. Du, H., Le, Z., Wang, H., Chen, Y., Yu, J.: COKG-QA: Multi-hop question answering over COVID-19 knowledge graphs. Data Intell. 4, 471–492 (2022)
12. Galárraga, L., Razniewski, S., Amarilli, A., Suchanek, F.M.: Predicting completeness in knowledge bases. In: WSDM. pp. 375–383 (2017)
13. Guo, Q., Zhuang, F., Qin, C., Zhu, H., Xie, X., Xiong, H., He, Q.: A survey on knowledge graph-based recommender systems. IEEE Trans. Knowl. Data Eng. 34, 3549–3568 (2022)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk34> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk34> ex:chunkContent """13. Guo, Q., Zhuang, F., Qin, C., Zhu, H., Xie, X., Xiong, H., He, Q.: A survey on knowledge graph-based recommender systems. IEEE Trans. Knowl. Data Eng. 34, 3549–3568 (2022)
14. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. In: ICLR (2022)
15. Ji, S., Pan, S., Cambria, E., Marttinen, P., Yu, P.S.: A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Trans. Neural Netw. Learn. Syst. 33, 494–514 (2022)
16. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: ACL. pp. 7871–7880 (2020)
17. Liu, Y., Sun, Z., Li, G., Hu, W.: I know what you do not know: Knowledge graph embedding via co-distillation learning. In: CIKM. pp. 1329–1338 (2022)
18. Miller, G.A.: WordNet: A lexical database for English. Commun. ACM 38, 39–41 (1995)
19. Nayyeri, M., Vahdati, S., Khan, M.T., Alam, M.M., Wenige, L., Behrend, A., Lehmann, J.: Dihedron algebraic embeddings for spatio-temporal knowledge graph completion. In: ESWC (2022)
20. Omeliyanenko, J., Zehe, A., Hotho, A., Schlör, D.: CapsKG: Enabling continual knowledge integration in language models for automatic knowledge graph completion. In: ISWC (2023)
---
18         Y. Liu et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk35> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk35> ex:chunkContent """21. Qu, M., Chen, J., Xhonneux, L.A.C., Bengio, Y., Tang, J.: RNNLogic: Learning
    logic rules for reasoning on knowledge graphs. In: ICLR (2021)
22. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,
    W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
    transformer. J. Mach. Learn. Res. 21, 1–67 (2020)
23. Saxena, A., Kochsiek, A., Gemulla, R.: Sequence-to-sequence knowledge graph
    completion and question answering. In: ACL. pp. 2814–2828 (2022)
24. Saxena, A., Tripathi, A., Talukdar, P.P.: Improving multi-hop question answering
    over knowledge graphs using knowledge base embeddings. In: ACL. pp. 4498–4507
    (2020)
25. Sun, Z., Deng, Z., Nie, J., Tang, J.: RotatE: Knowledge graph embedding by
    relational rotation in complex space. In: ICLR (2019)
26. Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon, M.: Rep-
    resenting text for joint embedding of text and knowledge bases. In: EMNLP. pp.
    1499–1509 (2015)
27. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière,
    B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample,
    G.: LLaMA: Open and efficient foundation language models. arXiv 2302.13971
    (2023)
28. Vashishth, S., Sanyal, S., Nitin, V., Talukdar, P.P.: Composition-based multi-
    relational graph convolutional networks. In: ICLR (2020)
29. Wang, B., Shen, T., Long, G., Zhou, T., Wang, Y., Chang, Y.: Structure-augmented
    text representation learning for efficient knowledge graph completion. In: WWW.
    pp. 1737–1748 (2021)
30. Wang, L., Zhao, W., Wei, Z., Liu, J.: SimKGC: Simple contrastive knowledge graph
    completion with pre-trained language models. In: ACL. pp. 4281–4294 (2022)
31. Wei, Y., Huang, Q., Zhang, Y., Kwok, J.T.: KICGPT: Large language model with
    knowledge in context for knowledge graph completion. In: EMNLP-Findings. pp.
    8667–8683 (2023)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk36> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk36> ex:chunkContent """31. Wei, Y., Huang, Q., Zhang, Y., Kwok, J.T.: KICGPT: Large language model with
    knowledge in context for knowledge graph completion. In: EMNLP-Findings. pp.
    8667–8683 (2023)
32. Xie, R., Liu, Z., Lin, F., Lin, L.: Does William Shakespeare really write Hamlet?
    Knowledge representation learning with confidence. In: AAAI. pp. 4954–4961 (2018)
33. Xie, X., Zhang, N., Li, Z., Deng, S., Chen, H., Xiong, F., Chen, M., Chen, H.:
    From discrimination to generation: Knowledge graph completion with generative
    transformer. In: WWW. pp. 162–165 (2022)
34. Yang, F., Yang, Z., Cohen, W.W.: Differentiable learning of logical rules for knowl-
    edge base reasoning. In: NeurPS (2017)
35. Yang, Y., Ye, Z., Zhao, H., Meng, L.: A novel link prediction framework based on
    gravitational field. Data Sci. Eng. 8, 47–60 (2023)
36. Yao, L., Mao, C., Luo, Y.: KG-BERT: BERT for knowledge graph completion.
    arXiv 1909.03193 (2019)
37. Yao, L., Peng, J., Mao, C., Luo, Y.: Exploring large language models for knowledge
    graph completion. arXiv 2308.13916 (2023)
38. Zhu, Y., Wang, X., Chen, J., Qiao, S., Ou, Y., Yao, Y., Deng, S., Chen, H., Zhang,
    N.: LLMs for knowledge graph construction and reasoning: Recent capabilities and
    future opportunities. arXiv 2305.13168 (2023)
39. Zhu, Z., Zhang, Z., Xhonneux, L.A.C., Tang, J.: Neural Bellman-Ford networks:
    A general graph neural network framework for link prediction. In: NeurIPS. pp.
    29476–29490 (2021)""" .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk4> ex:chunkContent """To address the above issues and fully exploit the reasoning ability of LLMs, we propose DIFT that finetunes LLMs with discrimination instructions for KG completion. To avoid the grounding errors in generation-based models, DIFT constructs discrimination instructions that require LLMs to select an entity from the given candidates as the answer. Specifically, it first employs a lightweight embedding-based model to provide the top-m candidate entities for each incomplete fact, and adds the names of these entities to the prompts as candidate answers to the KG completion query. Then, it finetunes an LLM with parameter-efficient finetuning methods like LoRA [14] to select one entity name from the prompt as the output. In this way, the LLM gets enhanced by finetuning and can always generate entities in the KG instead of unconstrained generation.

However, training the LLM with parameter-efficient finetuning methods is still costly. To further reduce the computation cost of finetuning, we design a truncated sampling method that can select useful samples from the KG for instruction construction. Let us assume that we get an example for finetuning with the query q = (h, r, ?) and the answer entity t. We use the pre-trained embedding-based model to compute the score of the fact (h, r, t) and the rank of t. Then, the truncated sampling method decides whether to discard the example based on the score of the fact and the rank of the answer entity. To unleash the graph reasoning ability of the LLM on KGs, we inject the embedded knowledge of queries and candidate entities into the LLM to further enhance it.

In summary, our main contributions are threefold:
---
Finetuning Generative LLMs with Discrimination Instructions          3""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk5> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk5> ex:chunkContent """In summary, our main contributions are threefold:
---
Finetuning Generative LLMs with Discrimination Instructions          3

- We propose a new KG completion framework, namely DIFT, which leverages
  discrimination instructions to finetune generative LLMs. DIFT does not
  require grounding the output of LLMs to entities in KGs.
- We propose a truncated sampling method to select useful KG samples for
  instruction construction to improve finetuning efficiency. We also inject KG
  embeddings into LLMs to improve finetuning effectiveness.
- Experiments show that DIFT advances the state-of-the-art KG completion
  results, with 0.364 Hits@1 on FB15K-237 and 0.616 on WN18RR.

The remaining sections of this paper are structured as follows. In Section 2,
we delve into the existing research on knowledge graph completion. Section 3
provides a detailed exposition of our proposed framework. We then present our
experimental results and analyses in Section 4. Finally, in Section 5, we conclude
this paper and outline potential avenues for future research.

## 2 Related Work

Related studies can be divided into embedding- and generation-based models.

### 2.1 Embedding-based KG Completion

Embedding-based KG completion methods compute prediction probability with
entity and relation embeddings learned from either structural or textual features.
We divide existing embedding-based models into two categories: structure-based
models and PLM-based models.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk6> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk6> ex:chunkContent """Structure-based Models. These models learn embeddings using structural
features such as edges (i.e., triplets), paths or neighborhood subgraphs. Therefore,
they can be categorized into three groups. The first group comprises triplet-
based embedding models to preserve the local relational structures of KGs. They
interpret relations as geometric transformations [3,25] or utilize semantic matching
methods for scoring triplets [1,19]. The second group contains path-based models
[6,34], which predominantly learn probabilistic logical rules from relation paths
to facilitate reasoning and infer missing entities. The models in the third group
use various deep neural networks to encode the subgraph structures of KGs.
CompGCN [28] captures the semantics of multi-relational graphs of KGs based
on the graph convolutional networks (GCN) framework. Instead, HittER [5] uses
Transformer to aggregate relational neighbor information. Recently, NBFNet [39]
employs a flexible and general framework to learn the representation of entity
pairs, demonstrating strong performance among structure-based models.

PLM-based Models. PLM-based models employ PLMs (e.g., BERT [10])
to encode the text attributes of entities and relations in facts, and compute
prediction probabilities using the output embeddings. KG-BERT [36] is the first
PLM-based KG completion model, which verifies that PLMs are capable of
capturing the factual knowledge in KGs. It turns a fact into a natural language
---
4        Y. Liu et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk7> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk7> ex:chunkContent """sentence by concatenating entity and relation names, and then predicts whether
this sentence is correct or not. Following KG-BERT, some subsequent works
make improvements in different aspects. StAR [29] divides each fact into two
asymmetric parts and encodes them separately with a Siamese-style encoder.
SimKGC [30] introduces three types of negatives for efficient contrastive learning.
CoLE [17] promotes structure-based models and PLM-based models mutually
through a co-distillation learning framework. These works are all embedding-based
models. They obtain query embeddings and entity embeddings with encoder-only
PLMs like BERT.

## 2.2 Generation-based KG Completion

Different from embedding-based models that need to learn entity, relation or fact
embeddings, generation-based models convert KG completion as a text generation
task. These models first translate a KG completion query into a natural language
question and then ask a generative language model (e.g., T5 [22] and BART [16])
to give an answer. Finally, they ground answers to entities in KGs using some
matching methods. To compare with traditional KG completion models that rank
entities based on their scores, generation-based models generate multiple entities
with beam search or sampling and rank them by the generation probabilities.
GenKGC [33] converts KG completion to sequence-to-sequence generation task
to achieve fast inference speed. KGT5 [23] designs a unified framework for KG
completion and question answering, but discards the pre-trained weights and
trains T5 from scratch. KG-S2S [4] proposes to employ a generative language
model to solve different forms of KG completion tasks including static KG
completion, temporal KG completion, and few-shot KG completion [15]. Although
these works provide some insight into how to conduct KG completion with LLMs,
simply replacing PLMs with current LLMs is infeasible as finetuning LLMs on
KGs is time-consuming and takes many computational resources.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk8> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk8> ex:chunkContent """With the emergence of LLMs, several works attempt to adapt LLMs for KG
completion. KG-LLM [37] performs instruction tuning on KG completion tasks
with relatively smaller LLMs (e.g., LLaMA-7B, ChatGLM-6B) and surpasses
ChatGPT and GPT-4, but it still lags behind state-of-the-art KG completion
models. KICGPT [31] employs an embedding-based model as the retriever to
generate an ordered candidate entity list and designs an in-context learning strategy to prompt ChatGPT to re-rank the entities with a multi-round interaction.
KICGPT is the most similar work to our proposed method DIFT, because we
also employ an embedding-based model to obtain candidate entities and provide
them to LLMs. However, accessing closed-source LLMs like ChatGPT is costly
as the inference cost grows linearly with the number of missing facts. In contrast,
we propose an effective and efficient method to finetune open-source LLMs.

## 3 The DIFT Framework

In this section, we describe the proposed DIFT framework for KG completion.
---
# Finetuning Generative LLMs with Discrimination Instructions

Fig. 1. Illustration of the proposed DIFT framework.

| Data Selection | Prompt Construction | [Answer]: English |
|----------------|----------------------|-------------------|
| Confidence > β | Query<br>Here is a fact with tail entity t unknown:<br>(Titanic, film/language, t [QUERY]).<br>Description<br>Titanic is a 1997 American epic romantic disaster film directed by James Cameron ...<br>Neighbor facts<br>(Titanic, film country, America); (Titanic, film release region, America); ...<br>Candidate entities<br>[Spanish [ENTITY]; English [ENTITY]; Arabic [ENTITY]; Portuguese [ENTITY];...] | LoRA<br>LLaMA |

| Knowledge Graph | Embedding-based models | Knowledge Adaption |
|-----------------|-------------------------|---------------------|
| | TransE,<br>SimKGC,<br>CoLE ... | |
| | Query embeddings Entity embeddings | |

## 3.1 Notations""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk9> .

<https://dblp.org/rec/journals/corr/abs-2407-16127/chunk9> ex:chunkContent """## 3.1 Notations

We start by introducing the definitions and notations used in this paper.

Knowledge graph. A KG is denoted as G = (E, R, T). E is the set of entities, and R is the set of relations. T = {(h, r, t)|h, t ∈ E, r ∈ R} is the set of facts. We denote a fact as (h, r, t), in which h is the head entity, t is the tail entity, and r is the relation between h and t. Furthermore, the available text attributes of G encompass entity names, relation names, and entity descriptions.

Knowledge graph completion. KG completion (a.k.a. link prediction) aims to predict the missing entity given an incomplete fact. To be more specific, given an incomplete fact (h, r, ?) or (?, r, t), the purpose of KG completion is to find the missing entity t or h from the entity set E.

## 3.2 Framework Overview

Fig. 1 shows the overall framework of the proposed DIFT. In general, DIFT finetunes an LLM M on a given KG G with the help of an embedding-based model ME which has been trained on G in advance. To elaborate, take a tail prediction query q = (h, r, ?) as an example, we feed q into ME to get the top-m predicted entities C = [e1, e2, . . . , em] where m is a predefined hyperparameter. Subsequently, we construct a discrimination instruction P(q) based on the query q and the candidate entities C. Finally, P(q) is fed into M to select the most plausible entity. In this way, we ensure that M always predicts an entity in E as the answer, avoiding grounding the unconstrained output texts from M to entities. For efficient finetuning, we employ ME to score the instruction samples and only keep samples with high confidence. Additionally, to enhance the graph reasoning ability of M, we design a knowledge adaption module to inject the embeddings of q and candidate entities in C obtained from ME into M.
---
6         Y. Liu et al.

### 3.3     Instruction Construction""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk10> .

<https://dblp.org/rec/journals/corr/abs-2407-18752> a dblp:Informal,
        dblp:Publication ;
    rdfs:label "Yuni Susanti and Michael Färber: Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery. (2024)" ;
    ex:hasChunk <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk1>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk10>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk11>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk12>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk13>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk14>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk15>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk16>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk17>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk18>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk19>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk2>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk20>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk21>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk22>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk23>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk24>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk25>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk26>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk27>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk28>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk29>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk3>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk30>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk31>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk32>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk33>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk34>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk35>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk36>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk37>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk4>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk5>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk6>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk7>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk8>,
        <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk9> ;
    datacite:hasIdentifier [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:dblp-record ;
            litre:hasLiteralValue "journals/corr/abs-2407-18752" ],
        [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:doi ;
            litre:hasLiteralValue "10.48550/ARXIV.2407.18752" ] ;
    schema1:abstract "Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present \"KG Structure as Prompt\", a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub." ;
    schema1:keywords "causal relation",
        "knowledge graph",
        "language model" ;
    owl:sameAs <http://dx.doi.org/10.48550/ARXIV.2407.18752>,
        <https://doi.org/10.48550/ARXIV.2407.18752> ;
    dblp:authoredBy <https://dblp.org/pid/129/9499>,
        <https://dblp.org/pid/166/2417> ;
    dblp:bibtexType bibtex:Article ;
    dblp:createdBy <https://dblp.org/pid/129/9499>,
        <https://dblp.org/pid/166/2417> ;
    dblp:documentPage <https://doi.org/10.48550/ARXIV.2407.18752> ;
    dblp:doi <https://doi.org/10.48550/ARXIV.2407.18752> ;
    dblp:hasSignature [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/166/2417> ;
            dblp:signatureDblpName "Yuni Susanti" ;
            dblp:signatureOrdinal 1 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-18752> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/129/9499> ;
            dblp:signatureDblpName "Michael Färber 0001" ;
            dblp:signatureOrdinal 2 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-18752> ] ;
    dblp:listedOnTocPage <https://dblp.org/db/journals/corr/corr2407> ;
    dblp:numberOfCreators 2 ;
    dblp:primaryDocumentPage <https://doi.org/10.48550/ARXIV.2407.18752> ;
    dblp:publishedIn "CoRR" ;
    dblp:publishedInJournal "CoRR" ;
    dblp:publishedInJournalVolume "abs/2407.18752" ;
    dblp:publishedInStream <https://dblp.org/streams/journals/corr> ;
    dblp:title "Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery." ;
    dblp:yearOfPublication "2024"^^xsd:gYear .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk10> ex:chunkContent """For instance, we may infer a causal relationship by looking at multi-hop
relations between a node pair in a knowledge graph, as illustrated in Fig. 3.

```mermaid
graph LR
    FGF6 -->|interact| FGFR4
    FGFR4 -->|associate| prostate_cancer
    FGF6 -.->|causal?| prostate_cancer
```

Fig. 3. Illustration of inferring a causal relationship in KG.

In Fig. 3, the node FGF6 is indirectly connected to the node prostate cancer
within one hop via the node FGFR4. As verified by a human expert, there is indeed
a causal relation between the nodes FGF6 and prostate cancer. We argue that
such graph structural information, in this example a path, adds background
knowledge on top of internal knowledge of LMs, effectively assisting LMs in
inferring causal relation between the variable pair. Specifically, we aim to use a
natural language description of the structural information from the knowledge
graph to be used as a prompt for prompt-based learning. We refer to such a
description of knowledge graph structure as a graph context.

In this work, we specifically examine three kinds of vital structural information of a KG to be used as the graph context, namely (1) neighbor nodes,
(2) common neighbor nodes, (3) metapath, described in detail as follows.

(1) Neighbor Nodes (NN). The essence of GNNs lies in applying different
aggregate functions to the graph structure, i.e., passing node features to neighboring nodes, where each node aggregates the feature vectors of its neighbors
to further update its feature vector. Thus, it is evident that the neighbor nodes
are the most crucial feature within a graph. Inspired by that, we examine the
neighboring nodes of the target node pairs to infer their causal relationship.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk11> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk11> ex:chunkContent """Formally, a node x is a neighbor of a node y in a knowledge graph KG =
(V, E) if there is an edge {x, y} ∈ E. We provide an example of neighbor nodes
from Wikidata [40] in Fig. 4. According to the provided example, the node
prostate cancer has urology as one of its neighbor nodes, while FGF6 has
urinary bladder as one of its neigbors. Thus, it is likely that a connection
exists between the node pair (FGF6, prostate cancer) due to their respective
neighboring nodes: urinary bladder ↔ urology.
---
KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery                       7

| gene | FSHR |
|------|------|
| homo sapiens | urinary bladder | F6F10 |
| FGF6 | urology | prostate cancer |
| exocrine gland | human chromosom12 | cabazitaxel |
| | | nilutamide |

Fig. 4. Example of neighbor nodes for FGF6 (left) and prostate cancer (right).

For utilizing the neighbor nodes structure in the prompt, we describe it in
natural language to form a graph context C, which we formally denote as:

$$C(x, V, E) = \\{x\\} \\text{"is connected to"} \\{[x_2]_{x_2 \\in V_x^2}\\}$$           (1)

We also create a variation of C where we include the edge description/relation
labels E, formally denoted as follows:

$$C(x, V, E) = \\{x\\} \\text{"has"} \\{E_{x,x_2}\\} \\text{"relation with"} \\{[x_2]_{x_2 \\in V_x^2}\\}$$    (2)

where $V_x^k$ represents the list of node x's k-hop neighbor nodes and $E_{x,x_2}$
represents the relation or edge description between the node x and its neighbor
nodes. The additional template words such as "is connected to" and "has relation
with" are optional, and can be replaced by any other fitting words. Then, with
Eq. 1, the generated graph context C for the node prostate cancer in Fig. 4
is shown in the following Example 1.

Example 1. prostate cancer is connected to nilutamide, cabazitaxel, urology,
FSHR, F6F10

When including the relation labels (Eq. 2), the graph context would be:""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk12> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk12> ex:chunkContent """Example 1. prostate cancer is connected to nilutamide, cabazitaxel, urology,
FSHR, F6F10

When including the relation labels (Eq. 2), the graph context would be:

Example 2. prostate cancer has drug or therapy used for treatment relation
with nilutamide and cabazitaxel, has genetic association with FSHR and F6F10

(2) Common Neighbor Nodes (CNN). Unlike neighbor nodes, common
neighbor nodes capture the idea that the more neighbors a pair of nodes
(x, y) shares, the more likely it is for the pair to be connected, i.e., there exists
an edge e = x, y between them. We argue that common neighbors between
two nodes help infer their relationship, so we examine the common neighbors
information between the node pair as graph context, as well. Fig. 5 shows an
example of common neighbor nodes for the pair (breast cancer, ERBB2), taken
from Hetionet knowledge graph [17]. According to the provided example, the pair
has in total 95 common neighbors, confirming a close relationship between them.

Formally, common neighbors between the nodes x and y can be defined as:

$$CN\\{x, y\\} = N(x) \\cap N(y)$$                                (3)
---
8        Y. Susanti and M. Färber

```mermaid
graph TD
    A[ADH5] --> B[breast cancer]
    B --> C[TGFBR2]
    B --> D[mammary gland]
    E[exemestano] --> F[ERBB2]
    F --> B
    F --> G[DPYSL2]
```

Fig. 5. Example of common neighbor nodes for the pair (ERBB2, breast cancer).
Different colors of the nodes represent different node types.

where N(x) is the set of nodes adjacent to node x (the neighbors of x), and
N(y) is the set of nodes adjacent to node y (the neighbors of y). Subsequently,
the graph context C for describing the common neighbors between the pairs x
and y can be formed as follows:

C(x,CN, y) = "Common neighbor nodes of {x} and {y} are" : {[n]n∈CN}   (4)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk13> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk13> ex:chunkContent """C(x,CN, y) = "Common neighbor nodes of {x} and {y} are" : {[n]n∈CN}   (4)

where CN represents the list of common neighbor nodes of the pair x and y as
defined in Eq. 3. Again, the additional template words "Common neighbor nodes
of..." are optional and can be replaced by other words. Then, we can generate
the graph context C including the common neighbor nodes information for the
pair in Fig. 5, as follows:

Example 3. Common neighbor nodes of breast cancer and ERBB2 are: ADH5,
mammary gland, exemestane, TGFBR2, DPYSL2

(3) Metapath (MP). Metapaths, or meta-paths are sequences of node types
which define a walk from an origin node to a destination node [37]. The term
"metapath" in this work is borrowed from the biomedical domain, referring to
specific node type combinations thought to be informative [30]. Due to its im-
portance in biomedical network analysis [41,43], we investigate the metapaths
of two nodes for inferring their causal relationship. Moreover, causal relation-
ships are frequently observed in the biomedical domain. Fig. 6 shows examples
of metapaths of the pair prostate cancer and FGF6.

Formally, a metapath MP can be defined as a path Z1 --R1--> Z2 --R2--> ... --Rn-->
Zn+1 describing a relation R between node types Z and Zn+1. The following
examples illustrate metapaths of different path length n from Fig. 6.

MP = (FGF6, FGFR4, prostate cancer), composed of node types {gene,
gene, disease}, with n = 3

MP = (FGF6, tendon, SDRDL, FGFR2, prostate cancer), composed
of node types {gene, anatomy, gene, gene, disease}, with n = 5
---
KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery                      9

```mermaid
graph LR
    FGF6 -->|interact| FGFR4
    FGFR4 -->|associate| prostate_cancer
    FGF6 -->|express| tendon
    tendon -->|express| SDRDL
    SDRDL -->|regulate| FGFR2
    FGFR2 -->|associate| prostate_cancer
```""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk14> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk14> ex:chunkContent """Fig. 6. Example of metapaths for the pair (FGF6, prostate cancer). Different colors of the nodes represent different node types.

As explained by the example provided in Fig. 3, we argue that an indirect path between two nodes can be useful for inferring a causal relation between two pairs, even when the edge itself does not describe a causal relation. Thus, an indirect metapath, or combination of meaningful node types could be latent evidence of causality between a pair of variables. We describe the metapath structure from KG in natural language to form a graph context C, as follows:

$$C(x, y, MP\\{V, E\\}) = \\{x\\} \\text{"is connected to"} \\{y\\} \\text{"via the following path: "}\\{v\\} \\{E_{v,v2}\\} \\{v2\\}$$

(5)

where MP{V, E} is a metapath MP with length n containing a set of nodes V (v1, v2, ...v|n|) and a set of edges E(v1, v1+n). Additional tokens "is connected to..." are optional and can be replaced by other tokens. Then, the graph context C with metapath information for the example in Fig. 6 would be:

Example 4. FGF6 is connected to prostate cancer via the following paths: FGF6 expressed in tendon, tendon expresses SQRDL, FGFR2 regulates SQRDL, FGFR2 associates with prostate cancer

To prevent bias in prediction by the LMs, we avoid the direct path. For instance, for the pair (x, y), we avoid the path MP = (x, y) and MP = (y, x), when such path exists in the KG.

### 4.3 Prompt-based learning with graph context

As illustrated in the model architecture in Fig. 2, we feed the textual context into SLMs together with the graph context generated with KG Structure as Prompt. We further design a prompt-based learning approach utilizing both contexts elaborated in this section. To get a clear distinction between conventional fine-tuning and our proposed prompt-based learning approach, we first provide a short overview of the conventional fine-tuning approach, as follows.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk15> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk15> ex:chunkContent """Given a pre-trained LM L to fine-tune on a dataset D, the conventional fine-tuning method encodes the training sequence x = {w1, w2, ...w|n|} into the corresponding output hidden vectors of the LMs h = {h1, h2, ...h|n|}. For MLMs such as BERT [8], the special token "[CLS]" is inserted at the beginning
---
10          Y. Susanti and M. Färber

of the sequence, and this special token is used as the final sequence represen-
tation h′, since it is supposed to contain information from the whole sequence.
A fully-connected layer and a softmax layer are further applied on top of this
representation to calculate the probability distribution over the class set Y, as
follows.

$$p = softmax(W_f h' + b_f)$$                (6)

Prompt-based learning, on the other hand, adapts the pre-trained LMs
for the downstream task via priming on natural language prompts−pieces of
text that are combined with the input and fed to the LMs to produce an output
for downstream tasks [1]. Concretely, we first convert each input sequence x with
a template T to form a prompt x′: T : x 7→ x′. In addition, a mapping func-
tion M is used to map the downstream task class set Y to a set of label words
V constituting all vocabularies of the LM L, i.e., M : Y 7→ V. As in the pre-
training of LMs, we further insert the special token "[MASK]" into x′ for L to fill
with the label words V. We provide an example of the prompt formulation below.

Given x ="Smoking causes cancer in adult male.", we set a template T , e.g.,

T = [x] "It shows [MASK] relation."

Then, the prompt x′ would be:

x′ = "Smoking causes cancer in adult male. It shows [MASK] relation.\"""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk16> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk16> ex:chunkContent """T = [x] "It shows [MASK] relation."

Then, the prompt x′ would be:

x′ = "Smoking causes cancer in adult male. It shows [MASK] relation."

We further feed the prompt x′ into L to obtain the hidden vector h[MASK]
of [MASK]. Next, with the mapping function M connecting the class set Y and
the label words, we formalize the probability distribution over Y at the masked
position, i.e., p(y|x) = p([MASK] = M(y)|x′). Here, the mapping function can
also be set manually e.g., M(true) = "causal" and M(f alse) = "non-causal".
Note that depending on the task, dataset, and the prompt design, the class labels
themselves can be used directly without any mapping function M.

In this study, our prompt-based learning combines the input sequence x with
the graph context C into the prompt x′, as illustrated in Fig. 2. Specifically, we
formulate the prompt x′ to include the following elements:

(1) textual context: input sequence x containing the pair,
(2) graph context C: context generated from KG structures as described in §4.2,
(3) target pair: pair e1 and e2 as the target, e.g., (FGF6, prostate cancer),
(4) [MASK] token,
(5) (optional) template tokens.

Subsequently, our final prompt x′ as the input to the LM for the pair e1 and
e2 can be formally defined as:

x′ = [x] [C] The pair [e1] and [e1] shows a [MASK] relation.       (7)

In this study, we select three SLMs, one for each of the three architectures:
MLM, CLM, Seq2SeqLM. Since each type of SLMs is trained differently, we de-
sign the prompt x′ differently across each type of SLMs. For instance, the prompt
---
KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery              11

x′ in Eq. 7, which is a cloze-style task prompt, suits the MLM architecture, since
this model is trained to be able to see the preceding and succeeding words in
texts. As for CLM and Seq2SeqLM, we cast the task as a generation-type, with
prompt x′ such as:

x′ = [x] [C] The pair [e1] and [e2] shows a causal relation: [MASK].       (8)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk17> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk17> ex:chunkContent """x′ = [x] [C] The pair [e1] and [e2] shows a causal relation: [MASK].       (8)

As mentioned earlier, the design of the mapping function M to map the
output into the downstream task labels varies depending on the task, dataset,
and the prompt design. For instance, with the prompt x′ as in Eq. 7, we can
directly use the class labels set Y = {causal, non-causal} without any mapping
function. Meanwhile, for prompt x′ in Eq. 8, we manually define a mapping
function, e.g., M(causal) = "true" and M(non-causal) = "f alse". Note that
the template "The pair shows..." is optional and can be replaced with other text.

## 5 Evaluation

Experiment Settings. We evaluate the proposed approach under few-shot
settings, using k = 16 training samples across all experiments. Precision (P),
Recall (R), and F1-score (F1) metrics are employed to evaluate the performance.
Since fine-tuning on low resources often suffers from instability and results may
change given a different split of data [35], we apply 5-fold cross-validation and
the metric scores are averaged. We restrict the number of contents from the KG
structures to be included in the prompt since the length of the prompt for SLMs
is limited, and we restrict the number of hops when querying the KG, as well. We
experimented with different settings and reported the best performing models.
Additional technical details are provided online as supplementary materials.

Datasets. The evaluation datasets are summarized in Table 1. Causality is often
observed in the biomedical domain, thus we primarily evaluate our approach
within this field, supplemented by an open-domain dataset. Each instance in the
dataset comprises textual context where a variable pair co-occurs in a text (see
Example 5 & 6), and is annotated by human experts to determine if there is a
causal relation between the variables.

Example 5. FGF6 contributes to the growth of prostate cancer by activating...""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk18> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk18> ex:chunkContent """Example 5. FGF6 contributes to the growth of prostate cancer by activating...

Example 6. The deadly train crash was caused by a terrorist attack.

Choice of SLMs. In this work, we define SLMs as LMs with less than 1
billion of total parameters. We experimented with SLMs with three different
architectures, as follows.

(a) MLM: roberta [27] model adapted to the biomedical domain, with 125 mil-
    lion parameters (biomed-roberta-base-125m [12]),
(b) CLM: bloomz-560m [29] with 560 million parameters,
(c) Seq2SeqLM: T5-base-220m [32] model with 220 million parameters
---
12        Y. Susanti and M. Färber

Table 1. Dataset sizes and types.

| dataset                               | domain       | total instances | description               |
|---------------------------------------|--------------|-----------------|---------------------------|
| GENEC (ours)                          | biomedical   | 789             | gene-gene causality       |
| DDI [16]                              | biomedical   | 33,508          | drug-drug causality       |
| COMAGC [22]                           | biomedical   | 820             | gene-disease causality    |
| SEMEVAL-2010 Task 8 [15]              | open-domain  | 10,717          | general domain causality  |

Choice of KGs. We selected the following two KGs for the experiments:

(a) Wikidata [40], as a representation of the general-domain KG,
(b) Hetionet [17], a domain-specific KG assembled from 29 different databases,
    covering genes, compounds, and diseases.

We selected Wikidata for its broad coverage of numerous subjects and topics. As
a comparison, we selected biomedical-KG Hetionet since we primarily evaluate
our approach on the datasets from this particular domain.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk19> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk19> ex:chunkContent """Model Comparison. We compare the following models: Models (1) to (4) rep-
resent the models trained without the graph context, i.e., the baselines, while
models marked with "PBL" (model 5 to 7) are our proposed prompt-based learn-
ing method injected with graph context information from KGs.

(1) ICL: In-Context Learning refers to a prompting method where few demon-
    strations of the task are provided to the LLMs as part of the prompt [4]. For
    this method, we selected GPT-3.5-turbo-instruct model by OpenAI, and
    provided k = 16 samples as demonstrations to query the model.
(2) FTfull: Conventional Fine-Tuning models trained using the full datasets.
(3) FTfew-shot: Conventional Fine-Tuning models under few-shot k = 16 setting.
(4) PTfew-shot: Original Prompt Tuning [23] as a baseline, which is essentially
    prompt-based learning without any graph context.
(5) PBLNN-Wiki-few-shot: Our proposed Prompt-based Learning + KG Structure
    as Prompt using the neighbor nodes NN structure from Wikidata.
(6) PBLCNN-Het-few-shot: Our proposed Prompt-based Learning + KG Structure
    as Prompt using the common neighbor nodes CNN structure from Hetionet.
(7) PBLMP-Het-few-shot: Our proposed Prompt-based Learning + KG Structure
    as Prompt using the metapaths MP structure from Hetionet.

Some variants of the proposed approach, such as PBLMP-Wiki-few-shot, were
omitted due to the computational expense of multi-hop querying in Wikidata,
which often results in no usable metapaths for many pairs. In addition, to focus
the discussion on the more effective models, we have omitted the results of less
effective models, such as PBLNN-Het-few-shot. We provide the more complete re-
sults, including zero-shot prompting and classical ML experiment results, online
as supplementary materials.
---
KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery                                  13

## 6 Results and Discussion""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk20> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk2> ex:chunkContent """**Keywords:** causal relation · language model · knowledge graph

## 1 Introduction

One of the fundamental tasks in various scientific disciplines is to find underlying causal relationships and eventually utilize them [10]. Causal discovery is a branch of causality study which estimates causal structures from observational data and generates a causal graph as a result. A causal graph, as illustrated in Fig. 1, is a directed graph modeling the causal relationships between observed variables; a node represents a variable and an edge represents a causal relationship.

* This work was conducted during a research stay at KIT and ScaDS.AI/TU Dresden, Germany.
1 https://github.com/littleflow3r/kg-structure-as-prompt
---
2         Y. Susanti and M. Färber

Conventionally, causal discovery involves learning causal relations from observational data by measuring how changes in one variable are associated with changes in another variable, an approach referred to as covariance-based causal discovery [21]. Driven by the recent advancements in LLMs, recent work has explored the causal capabilities of LLMs using metadata (e.g., variable names) rather than their actual data values. In other words, the causal relation is queried in natural language directly to the LLMs. This paper focuses on the latter, and to differentiate with covariance-based causal discovery, we refer to this approach as knowledge-based causal discovery, following the definition of [21].

| smoking |
|----------|
| genetics |
|    ↓     |
|   lung   |
|  cancer  |
|    ↓     |
| coughing |

Fig. 1. Example of a causal graph.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk3> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk20> ex:chunkContent """## 6 Results and Discussion

Table 2 & 3 summarize the results. We report the averaged Precision (P), Recall (R), and F1 scores, including the standard deviation values of the F1 scores over the 5-folds cross-validation. We provide a summary of the primary findings (§6.1), followed by analysis and discussion of the results (§6.2).

Table 2. Evaluation results on biomedical-domain datasets. Values in parenthesis are the standard deviations of F1 scores over 5-cv test folds. NN, CNN, MP indicate the KG structures used as graph context: neighbors nodes, common neighbors nodes, and metapath, respectively. bold: highest F1 scores per LMs architecture and per dataset, underline: F1 scores of the highest-performed models per dataset.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk21> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk21> ex:chunkContent """| | COMAGC | | | GENEC | | | DDI | | |
|---|---|---|---|---|---|---|---|---|---|
| | P | R | F1 | P | R | F1 | P | R | F1 |
| (baseline) ICL(GPT-3.5-turbo) | 64.1 | 67.7 | 65.5(.18) | 55.4 | 77.2 | 62.6(.16) | 53.2 | 99.0 | 68.9(.08) |
| *MLM architecture (biomed-roberta-base-125m)* | | | | | | | | | |
| FTfull | 90.0 | 86.8 | 88.2(.02) | 61.0 | 61.0 | 61.5(.03) | 84.0 | 98.9 | 90.7(.04) |
| (baseline) FTfew-shot | 87.0 | 71.0 | 76.8(.02) | 52.0 | 52.0 | 52.0(.02) | 64.9 | 87.0 | 73.9(.07) |
| (baseline) PTfew-shot | 87.0 | 77.2 | 80.6(.03) | 58.0 | 58.0 | 58.0(.02) | 69.6 | 83.0 | 75.4(.03) |
| (ours) PBLNN-Wiki-few-shot | 82.0 | 85.0 | 83.5(.02) | 63.0 | 63.0 | 63.0(.03) | 70.2 | 85.0 | 76.6(.03) |
| (ours) PBLCNN-Het-few-shot | 77.4 | 89.5 | 82.7(.05) | 54.5 | 54.4 | 54.4(.02) | 65.1 | 89.9 | 74.6(.05) |
| (ours) PBLMP-Het-few-shot | 80.0 | 89.3 | 83.9(.03) | 60.5 | 60.5 | 60.5(.02) | 67.0 | 87.6 | 75.4(.02) |
| *CLM architecture (bloomz-560m)* | | | | | | | | | |
| FTfull | 58.0 | 91.8 | 71.3(.07) | 53.8 | 73.0 | 60.7(.07) | 83.0 | 91.4 | 86.7(.06) |
| (baseline) FTfew-shot | 73.4 | 85.0 | 77.6(.04) | 48.2 | 86.0 | 61.7(.01) | 72.0 | 61.5 | 66.1(.04) |
| (baseline) PTfew-shot | 64.1 | 95.0 | 76.3(.01) | 51.3 | 97.0 | 67.6(.01) | 60.2 | 92.0 | 72.7(.02) |
| (ours) PBLNN-Wiki-few-shot | 71.7 | 90.9 | 79.0(.02) | 54.1 | 90.0 | 67.4(.01) | 63.3 | 88.0 | 72.6(.02) |
| (ours) PBLCNN-Het-few-shot | 65.7 | 94.7 | 77.2(.05) | 52.8 | 100 | 69.1(.00) | 60.4 | 93.9 | 72.8(.05) |
| (ours) PBLMP-Het-few-shot | 76.9 | 89.0 | 81.8(.04) | 52.8 | 91.0 | 68.5(.03) | 65.5 | 90.8 | 74.8(.05) |
| *Seq2SeqLM architecture (T5-base-220m)* | | | | | | | | | |
| FTfull | 88.5 | 78.0 | 82.6(.07) | 60.6 | 43.0 | 50.0(.11) | 96.3 | 83.0 | 88.8(.04) |
| (baseline) FTfew-shot | 68.8 | 81.0 | 72.7(.06) | 48.3 | 67.0 | 55.5(.01) | 55.6 | 94.0 | 69.6(.04) |
| (baseline) PTfew-shot | 64.5 | 91.0 | 75.0(.05) | 63.0 | 83.0 | 69.3(.06) | 63.3 | 82.0 | 70.3(.04) |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk22> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk22> ex:chunkContent """| (baseline) PTfew-shot | 64.5 | 91.0 | 75.0(.05) | 63.0 | 83.0 | 69.3(.06) | 63.3 | 82.0 | 70.3(.04) |
| (ours) PBLNN-Wiki-few-shot | 65.3 | 90.0 | 74.7(.01) | 66.5 | 76.0 | 70.3(.02) | 57.5 | 95.0 | 70.9(.03) |
| (ours) PBLCNN-Het-few-shot | 65.4 | 86.3 | 73.4(.02) | 56.0 | 95.8 | 70.6(.01) | 56.2 | 96.0 | 70.6(.02) |
| (ours) PBLMP-Het-few-shot | 73.3 | 82.0 | 75.6(.03) | 56.5 | 91.0 | 69.7(.02) | 60.6 | 92.9 | 72.9(.03) |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk23> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk23> ex:chunkContent """### 6.1 Primary Findings

We listed the summary of the main results from Table 2 below.
---
14        Y. Susanti and M. Färber

Table 3. Evaluation results for open-domain dataset: SEMEVAL-2010 Task 8

| | P | R | F1 | P | R | F1 | P | R | F1 |
|---|---|---|---|---|---|---|---|---|---|
| (baseline) ICLGPT-3.5-turbo | 52.2 | 97.2 | 67.5(.07) | | | | | | |
| | MLM | | | CLM | | | Seq2SeqLM | | |
| FTfull | 94.8 | 88.7 | 91.6(.02) | 73.0 | 74.0 | 71.3(.07) | 82.8 | 77.0 | 79.4(.02) |
| (baseline) FTfew-shot | 57.2 | 83.0 | 67.2(.04) | 50.2 | 99.0 | 66.6(.01) | 52.1 | 90.9 | 61.1(.04) |
| (baseline) PTfew-shot | 51.9 | 94.0 | 66.9(.00) | 51.7 | 98.0 | 67.6(.02) | 52.5 | 93.0 | 66.7(.02) |
| (ours) PBLNN-Wiki-few-shot | 55.7 | 93.0 | 69.7(.01) | 53.4 | 91.2 | 67.4(.00) | 53.7 | 93.0 | 67.9(.01) |

(a) Our proposed approach outperforms the no-graph context baseline models,
    in most of the experiments across different dataset domains: up to 15.1 points
    of improvement of F1 scores on biomedical datasets (55.5 → 70.6, GENEC
    dataset) and 6.8 points of improvement on open-domain dataset (61.1 →
    67.9, SEMEVAL dataset).
(b) Under few-shot settings with k = 16 training samples, our proposed approach
    generally achieves the second-best performance compared to FTfull model,
    which is the conventional fine-tuning models trained with full datasets. Few
    even surpassed them, such as PBLNN-Wiki-few-shot model with MLM architecture (63.0 vs. 61.5) and PBLCNN-Het-few-shot model with CLM architecture,
    on GENEC dataset (69.1 vs. 60.7).
(c) Our models based on SLMs with less that 1 billion parameters surpassed the
    ICL prompting method on much larger model across all datasets in most
    experiments, underlining the importance of KGs to support the constrained
    internal knowledge of smaller LMs.

## 6.2 Analysis and Discussion""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk24> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk24> ex:chunkContent """## 6.2 Analysis and Discussion

### NN vs. CNN vs. MP
In our experiments, the KG structure metapath
MP contributed the most to the top-performing models, while the neighbors
nodes NN and common neighbors nodes CNN roughly exhibited comparable
performance across models and datasets. The effectiveness of MP likely depends on the hop count between entity pairs in the dataset, i.e., the hop count is
relatively high (2.8) for the COMACG dataset, where MP gave the best performance. Conversely, for the GENEC dataset, where the average MP hop is low
(1.8), CNN and NN outperformed MP. To train a robust model that is able
to generalize well given any KG structure, we opted to not optimize the content
selection of the KG structures in the current experiments. For instance, when
there are more than m metapaths for a pair, we randomly select m of them, m
being a hyperparameter of the number of metapaths to be included as prompt.
In spite of that, our proposed approach achieved a relatively satisfactory performance, suggesting that rather than the content of the structure, the type of
the structural information, i.e., NN vs. CNN vs. MP, is arguably more
important based on the experiments.
---
## KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery            15""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk25> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk25> ex:chunkContent """### MLM vs. CLM vs. Seq2SeqLM. 
For classification tasks, language models trained with MLM architecture are often preferred. This preference comes from the fact that MLMs are trained to consider both preceding and succeeding words, a crucial aspect for accurately predicting the correct class in a classification model. In line with this, the top-performing models trained on both full and few-shot datasets are based on the MLM architecture. The second best-performing models using full dataset are based on the Seq2SeqLM architecture, followed by those based on the CLM architecture. This is most likely because, similar to MLMs, Seq2SeqLMs also include encoder blocks and are trained to recognize the surrounding words [32]. However, this trend slightly differs in experiments under few-shot settings, as the models based on CLM architecture outperformed those based on Seq2SeqLM architecture. Thus, selecting an appropriate architecture, specifically how the LMs are trained, is crucial when adapting the LMs for downstream tasks. As demonstrated by the outcomes of our experiments, LMs trained with the MLM architecture are generally more suitable for classification tasks than those with Seq2SeqLM and CLM architectures.

### Wikidata vs. Hetionet. 
In the biomedical domain, the proposed approach injected with structural information from Hetionet demonstrates better performance in most experiments. This is expected considering the domain-specific nature of the dataset. Nevertheless, both Wikidata and Hetionet performed relatively well; the top-performing models for COMAGC and GENEC datasets are attained with Hetionet, while for DDI dataset are achieved with Wikidata. We also achieved 6.8 points of F1 score improvement on SEMEVAL dataset with Wikidata. This suggests that the proposed approach is rather flexible regarding the choice of KGs.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk26> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk26> ex:chunkContent """### SLMs vs. LLMs. 
We selected OpenAI's GPT-3.5-turbo-instruct [31] as a representative of larger parameter-LLMs. However, OpenAI does not provide technical details such as the numbers of parameters; except the context windows which is 4,096 tokens in size for this model [31]. This is much larger than our choice of SLMs with a maximum token length ranging from 128 to 512. To summary, the results demonstrate that the SLMs outperformed this model across all datasets in most experiment. This further shows the potential of SLMs: combined with prompt-based learning and access to KGs, the proposed approach outperforms LLMs with considerably larger size and parameters, with minimal training effort (few-shot). Note that we also provided k = 16 training samples as task demonstration to query the GPT model for a more fair comparison with the experiments under few-shot settings.

Typically, SLMs are trained on significantly less data compared to LLMs, which leads to reduced capacity and inferior performance in downstream tasks. Therefore, the graph context derived from the structural information of KG by our proposed KG Structure as Prompt approach effectively serves as an additional evidence of causality; in other words, it assists the SLMs to rely
---
16        Y. Susanti and M. Färber

not only on their constrained internal knowledge, but also by enhancing their
capacity through denser information sourced from the KGs.

## 7     Conclusion""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk27> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk27> ex:chunkContent """not only on their constrained internal knowledge, but also by enhancing their
capacity through denser information sourced from the KGs.

## 7     Conclusion

In this paper, we presented "KG Structure as Prompt", a novel approach for
integrating structural information from KGs into prompt-based learning, to further enhance the capability of Small Language Models (SLMs). We evaluated
our approach on knowledge-based causal discovery tasks. Extensive experiments
under few-shot settings on biomedical and open-domain datasets highlight the
effectiveness of the proposed approach, as it outperformed most of the no-KG
baselines, including the conventional fine-tuning method with a full dataset. We
also demonstrated the robust capabilities of SLMs: in combination with prompt-
based learning and KGs, SLMs are able to surpass a language model with larger
parameters. Our proposed approach has proven to be effective with different
types of LMs architectures and KGs, as well, showing its flexibility and adaptability across various LMs and KGs.

Our work has been centered on discovering causal relationships between pairs
of variables. In future work, we aim to tackle more complex scenarios by developing methods to analyze causal graphs with multiple interconnected variables,
which will offer a deeper understanding of causalities.

Supplemental Material Statement: Datasets, source code, and other details are
available online at https://github.com/littleflow3r/kg-structure-as-prompt

## References

1. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., Sontag, D.: Large language models are few-shot clinical information extractors. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. pp. 1998–2022. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates (Dec
2022), https://aclanthology.org/2022.emnlp-main.130""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk28> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk28> ex:chunkContent """2. Baek, J., Aji, A.F., Saffari, A.: Knowledge-augmented language model prompting
for zero-shot knowledge graph question answering. In: Dalvi Mishra, B., Durrett,
G., Jansen, P., Neves Ribeiro, D., Wei, J. (eds.) Proceedings of the 1st Workshop on
Natural Language Reasoning and Structured Explanations (NLRSE). pp. 78–106.
Association for Computational Linguistics, Toronto, Canada (Jun 2023), https:
//aclanthology.org/2023.nlrse-1.7

3. Blanco, E., Castell, N., Moldovan, D.I.: Causal relation extraction. In: Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May - 1 June 2008, Marrakech, Morocco. European
Language Resources Association (2008), http://www.lrec-conf.org/proceedings/
lrec2008/summaries/87.html

4. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,
---
KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery                   17

J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language
models are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
M., Lin, H. (eds.) Advances in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual (2020), https://proceedings.neurips.cc/paper/2020/
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html

5. Bui, Q.C., Nualláin, B.Ó., Boucher, C.A., Sloot, P.M.: Extracting causal relations
   on hiv drug resistance from literature. BMC Bioinformatics 11(1), 101 (Feb 2010),
   https://doi.org/10.1186/1471-2105-11-101""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk29> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk29> ex:chunkContent """6. Chang, D.S., Choi, K.S.: Incremental cue phrase learning and bootstrapping
   method for causality extraction using cue phrase and word pair probabilities.
   Information Processing & Management 42(3), 662–678 (2006), https://www.
   sciencedirect.com/science/article/pii/S0306457305000580

7. Chen, X., Zhang, N., Xie, X., Deng, S., Yao, Y., Tan, C., Huang, F., Si, L., Chen,
   H.: Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization
   for relation extraction. In: Proceedings of the ACM Web Conference 2022. p.
   2778–2788. WWW '22, Association for Computing Machinery, New York, NY,
   USA (2022), https://doi.org/10.1145/3485447.3511998

8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
   bidirectional transformers for language understanding. In: Proceedings of the 2019
   Conference of the North American Chapter of the Association for Computational
   Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).
   pp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota
   (Jun 2019), https://aclanthology.org/N19-1423

9. Gao, T., Fisch, A., Chen, D.: Making pre-trained language models better few-shot
   learners. In: Zong, C., Xia, F., Li, W., Navigli, R. (eds.) Proceedings of the 59th
   Annual Meeting of the Association for Computational Linguistics and the 11th
   International Joint Conference on Natural Language Processing (Volume 1: Long
   Papers). pp. 3816–3830. Association for Computational Linguistics, Online (Aug
   2021), https://aclanthology.org/2021.acl-long.295

10. Glymour, C., Zhang, K., Spirtes, P.: Review of Causal Discovery Methods Based on
    Graphical Models. Frontiers in genetics 10(524) (2019), https://doi.org/10.3389/
    fgene.2019.00524

11. Gu, J., Qian, L., Zhou, G.: Chemical-induced disease relation extraction with
    various linguistic features. Database 2016 (04 2016), https://doi.org/10.1093/
    database/baw042, baw042""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk30> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk3> ex:chunkContent """| smoking |
|----------|
| genetics |
|    ↓     |
|   lung   |
|  cancer  |
|    ↓     |
| coughing |

Fig. 1. Example of a causal graph.

Typically, such metadata-based causal reasoning is performed by Subject Matter Experts (SMEs) as they construct a causal graph, drawing from their expertise in domain-specific subjects and common sense [21], or based on literature surveys on subjects related to the variables. The advancement in LLMs has simplified this formerly challenging process, as LLMs are now capable of providing the knowledge that previously can only be provided by SMEs. Recent works [18,39,42,45] also show promising results, notably, [21] explores causal capabilities of LLMs by experimenting on cause-effect pairs. Their finding suggests that LLM-based methods achieved state-of-the-art performance on several causal benchmarks. Similarly, [45] investigated the causal capability of LLMs by analyzing their behavior given a certain causal question. However, in contrast to [21], their result suggests that LLMs currently lack the capability to offer satisfactory answers for discovering new knowledge. Meanwhile, a work by [18] focused on investigating the LLMs' capability for causal association among events expressed in natural language. Thus, their study is more oriented towards extracting a causal diagram (e.g., a chain of events) from unstructured text instead of discovering new causal relations.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk4> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk30> ex:chunkContent """11. Gu, J., Qian, L., Zhou, G.: Chemical-induced disease relation extraction with
    various linguistic features. Database 2016 (04 2016), https://doi.org/10.1093/
    database/baw042, baw042

12. Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey,
    D., Smith, N.A.: Don't stop pretraining: Adapt language models to domains and
    tasks. In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (eds.) Proceedings of
    the 58th Annual Meeting of the Association for Computational Linguistics. pp.
    8342–8360. Association for Computational Linguistics, Online (Jul 2020), https:
    //aclanthology.org/2020.acl-main.740

13. Han, J., Zhao, S., Cheng, B., Ma, S., Lu, W.: Generative prompt tuning for re-
    lation classification. In: Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.) Findings of
    the Association for Computational Linguistics: EMNLP 2022. pp. 3170–3185. As-
    sociation for Computational Linguistics, Abu Dhabi, United Arab Emirates (Dec
    2022), https://aclanthology.org/2022.findings-emnlp.231

14. Han, X., Zhao, W., Ding, N., Liu, Z., Sun, M.: Ptr: Prompt tuning with rules for
    text classification. AI Open 3, 182–192 (2022), https://www.sciencedirect.com/
    science/article/pii/S2666651022000183
---
18        Y. Susanti and M. Färber

15. Hendrickx, I., Kim, S.N., Kozareva, Z., Nakov, P., Ó Séaghdha, D., Padó, S.,
    Pennacchiotti, M., Romano, L., Szpakowicz, S.: SemEval-2010 task 8: Multi-way
    classification of semantic relations between pairs of nominals. In: Proceedings of
    the 5th International Workshop on Semantic Evaluation. pp. 33–38. Association
    for Computational Linguistics, Uppsala, Sweden (Jul 2010), https://aclanthology.
    org/S10-1006""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk31> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk31> ex:chunkContent """16. Herrero-Zazo, M., Segura-Bedmar, I., Martínez, P., Declerck, T.: The ddi cor-
    pus: An annotated corpus with pharmacological substances and drug–drug inter-
    actions. Journal of Biomedical Informatics 46(5), 914–920 (2013), https://www.
    sciencedirect.com/science/article/pii/S1532046413001123

17. Himmelstein, D.S., Lizee, A., Hessler, C., Brueggeman, L., Chen, S.L., Hadley, D.,
    Green, A., Khankhanian, P., Baranzini, S.E.: Systematic integration of biomedical
    knowledge prioritizes drugs for repurposing. eLife 6, e26726 (sep 2017), https:
    //doi.org/10.7554/eLife.26726

18. Khetan, V., Rizvi, M.I., Huber, J., Bartusiak, P., Sacaleanu, B., Fano, A.: MIM-
    ICause: Representation and automatic extraction of causal relation types from
    clinical notes. In: Findings of the Association for Computational Linguistics: ACL
    2022. pp. 764–773. Association for Computational Linguistics, Dublin, Ireland
    (May 2022), https://aclanthology.org/2022.findings-acl.63

19. Khoo, C.S.G., Chan, S., Niu, Y.: Extracting causal knowledge from a medical
    database using graphical patterns. In: Proceedings of the 38th Annual Meeting on
    Association for Computational Linguistics. p. 336–343. ACL '00, Association for
    Computational Linguistics, USA (2000), https://doi.org/10.3115/1075218.1075261

20. Khoo, C.S.G., KORNFILT, J., ODDY, R.N., MYAENG, S.H.: Automatic Ex-
    traction of Cause-Effect Information from Newspaper Text Without Knowledge-
    based Inferencing. Literary and Linguistic Computing 13(4), 177–186 (12 1998),
    https://doi.org/10.1093/llc/13.4.177

21. Kiciman, E., Ness, R., Sharma, A., Tan, C.: Causal reasoning and large language
    models: Opening a new frontier for causality. CoRR abs/2305.00050 (2023),
    https://doi.org/10.48550/arXiv.2305.00050""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk32> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk32> ex:chunkContent """22. Lee, H.J., Shim, S.H., Song, M.R., Lee, H., Park, J.C.: Comagc: a corpus with
    multi-faceted annotations of gene-cancer relations. BMC Bioinformatics 14(1), 323
    (Nov 2013), https://doi.org/10.1186/1471-2105-14-323

23. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient
    prompt tuning. In: Moens, M.F., Huang, X., Specia, L., Yih, S.W.t. (eds.) Proceed-
    ings of the 2021 Conference on Empirical Methods in Natural Language Processing.
    pp. 3045–3059. Association for Computational Linguistics, Online and Punta Cana,
    Dominican Republic (Nov 2021), https://aclanthology.org/2021.emnlp-main.243

24. Li, H., Mo, T., Fan, H., Wang, J., Wang, J., Zhang, F., Li, W.: KiPT: Knowledge-
    injected prompt tuning for event detection. In: Proceedings of the 29th Inter-
    national Conference on Computational Linguistics. pp. 1943–1952. International
    Committee on Computational Linguistics, Gyeongju, Republic of Korea (Oct
    2022), https://aclanthology.org/2022.coling-1.169

25. Li, X.L., Liang, P.: Prefix-tuning: Optimizing continuous prompts for generation.
    In: Zong, C., Xia, F., Li, W., Navigli, R. (eds.) Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th Interna-
    tional Joint Conference on Natural Language Processing (Volume 1: Long Papers).
    pp. 4582–4597. Association for Computational Linguistics, Online (Aug 2021),
    https://aclanthology.org/2021.acl-long.353
---
KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery                         19""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk33> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk33> ex:chunkContent """26. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and
    predict: A systematic survey of prompting methods in natural language processing.
    ACM Comput. Surv. 55(9) (jan 2023), https://doi.org/10.1145/3560815
27. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
    Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized BERT pretraining
    approach. CoRR abs/1907.11692 (2019), http://arxiv.org/abs/1907.11692
28. Mihăilă, C., Ananiadou, S.: Semi-supervised learning of causal relations in biomed-
    ical scientific discourse. BioMedical Engineering OnLine 13(2), S1 (Dec 2014),
    https://doi.org/10.1186/1475-925X-13-S2-S1
29. Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T.L.,
    Bari, M.S., Shen, S., Yong, Z.X., Schoelkopf, H., Tang, X., Radev, D., Aji, A.F.,
    Almubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff, E., Raffel, C.: Crosslin-
    gual generalization through multitask finetuning. In: Rogers, A., Boyd-Graber,
    J.L., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Associa-
    tion for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,
    Canada, July 9-14, 2023. pp. 15991–16111. Association for Computational Linguis-
    tics (2023), https://doi.org/10.18653/v1/2023.acl-long.891
30. Noori, A., Li, M.M., Tan, A.L.M., Zitnik, M.: Metapaths: similarity search in
    heterogeneous knowledge graphs via meta-paths. Bioinformatics 39(5), btad297
    (05 2023), https://doi.org/10.1093/bioinformatics/btad297
31. OpenAI: Gpt-3.5-turbo. https://platform.openai.com/docs/models/
    gpt-3-5-turbo, [Accessed 11-04-2024]
32. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,
    Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-
    to-text transformer. Journal of Machine Learning Research 21(140), 1–67 (2020),
    http://jmlr.org/papers/v21/20-074.html""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk34> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk34> ex:chunkContent """to-text transformer. Journal of Machine Learning Research 21(140), 1–67 (2020),
    http://jmlr.org/papers/v21/20-074.html
33. Reklos, I., Meroño-Peñuela, A.: Medicause: Causal relation modelling and extrac-
    tion from medical publications. In: Tiwari, S., Mihindukulasooriya, N., Osborne,
    F., Kontokostas, D., D'Souza, J., Kejriwal, M., Bozzato, L., Carriero, V.A., Hah-
    mann, T., Zimmermann, A. (eds.) Proceedings of the 1st International Workshop
    on Knowledge Graph Generation From Text and the 1st International Work-
    shop on Modular Knowledge co-located with 19th Extended Semantic Conference
    (ESWC 2022), Hersonissos, Greece, May 30th, 2022. CEUR Workshop Proceed-
    ings, vol. 3184, pp. 1–18. CEUR-WS.org (2022), https://ceur-ws.org/Vol-3184/
    TEXT2KG_Paper_1.pdf
34. Schick, T., Schütze, H.: Exploiting cloze-questions for few-shot text classification
    and natural language inference. In: Merlo, P., Tiedemann, J., Tsarfaty, R. (eds.)
    Proceedings of the 16th Conference of the European Chapter of the Association for
    Computational Linguistics: Main Volume. pp. 255–269. Association for Computa-
    tional Linguistics, Online (Apr 2021), https://aclanthology.org/2021.eacl-main.20
35. Schick, T., Schütze, H.: It's not just size that matters: Small language models
    are also few-shot learners. In: Toutanova, K., Rumshisky, A., Zettlemoyer, L.,
    Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., Zhou,
    Y. (eds.) Proceedings of the 2021 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies.
    pp. 2339–2352. Association for Computational Linguistics, Online (Jun 2021),
    https://aclanthology.org/2021.naacl-main.185
36. Su, P., Vijay-Shanker, K.: Investigation of improving the pre-training and fine-
    tuning of bert model for biomedical relation extraction. BMC Bioinformatics 23(1),""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk35> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk35> ex:chunkContent """36. Su, P., Vijay-Shanker, K.: Investigation of improving the pre-training and fine-
    tuning of bert model for biomedical relation extraction. BMC Bioinformatics 23(1),
    120 (Apr 2022), https://doi.org/10.1186/s12859-022-04642-w
---
20         Y. Susanti and M. Färber""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk36> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk36> ex:chunkContent """37. Sun, Y., Han, J., Yan, X., Yu, P.S., Wu, T.: Pathsim: meta path-based top-k similarity search in heterogeneous information networks. Proc. VLDB Endow. 4(11), 992–1003 (aug 2011), https://doi.org/10.14778/3402707.3402736

38. Susanti, Y., Uchino, K.: Causal-evidence graph for causal relation classification. In: Hong, J., Park, J.W. (eds.) Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing, SAC 2024, Avila, Spain, April 8-12, 2024. pp. 714–722. ACM (2024), https://doi.org/10.1145/3605098.3635894

39. Tu, R., Ma, C., Zhang, C.: Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. CoRR abs/2301.13819 (2023), https://doi.org/10.48550/arXiv.2301.13819

40. Vrandečić, D., Krötzsch, M.: Wikidata: a free collaborative knowledgebase. Commun. ACM 57(10), 78–85 (sep 2014), https://doi.org/10.1145/2629489

41. Wang, H., Wang, X., Liu, W., Xie, X., Peng, S.: deepdga: Biomedical heterogeneous network-based deep learning framework for disease-gene association predictions. In: Adjeroh, D.A., Long, Q., Shi, X.M., Guo, F., Hu, X., Aluru, S., Narasimhan, G., Wang, J., Kang, M., Mondal, A., Liu, J. (eds.) IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2022, Las Vegas, NV, USA, December 6-8, 2022. pp. 601–606. IEEE (2022), https://doi.org/10.1109/BIBM55620.2022.9995651

42. Willig, M., Zecevic, M., Dhami, D.S., Kersting, K.: Can foundation models talk causality? CoRR abs/2206.10591 (2022), https://doi.org/10.48550/arXiv.2206.10591

43. Yao, W., Zhao, W., Jiang, X., Shen, X., He, T.: MPGNN-DSA: A meta-path-based graph neural network for drug-side effect association prediction. In: Adjeroh, D.A., Long, Q., Shi, X.M., Guo, F., Hu, X., Aluru, S., Narasimhan, G., Wang, J., Kang, M., Mondal, A., Liu, J. (eds.) IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2022, Las Vegas, NV, USA, December 6-8, 2022. pp. 627–632. IEEE (2022), https://doi.org/10.1109/BIBM55620.2022.9995486""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk37> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk37> ex:chunkContent """44. Ye, R., Zhang, C., Wang, R., Xu, S., Zhang, Y.: Language is all a graph needs. In: Graham, Y., Purver, M. (eds.) Findings of the Association for Computational Linguistics: EACL 2024. pp. 1955–1973. Association for Computational Linguistics, St. Julian's, Malta (Mar 2024), https://aclanthology.org/2024.findings-eacl.132

45. Zhang, C., Bauer, S., Bennett, P., Gao, J., Gong, W., Hilmkil, A., Jennings, J., Ma, C., Minka, T., Pawlowski, N., Vaughan, J.: Understanding causality with large language models: Feasibility and opportunities. CoRR abs/2304.05524 (2023), https://doi.org/10.48550/arXiv.2304.05524""" .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk4> ex:chunkContent """In this paper, we investigate the capabilities of language models for knowledge-based causal discovery between variable pairs given a textual context from text sources. Specifically, given a pair of variables e1 and e2, the task is to predict if a causal relation can be inferred between the variables. Therefore, similar to [18], our focus also lies in inferring causal relations from text rather than discovering new causal relations. In particular, we present "KG Structure as Prompt," a novel approach for integrating structural information from a Knowledge Graph (KG) into prompt-based learning with Small Language Models (SLMs). Prompt-based learning adapts LMs for specific tasks by incorporating prompts—task-specific instruction combined with the text input—to guide the models' output for the downstream tasks. Our approach enhances this method by incorporating additional information from KGs, leveraging the strengths of KGs in providing context and background knowledge. We opted for SLMs because a smaller model that can outperform larger models is more cost-effective and therefore preferable. We conduct experiments on three types of biomedical and an open-
---
KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery            3

domain datasets, and further evaluate the performance of the proposed approach
under three different architectures of language models.
     To summarize, our main contributions are as follows:""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk5> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk5> ex:chunkContent """domain datasets, and further evaluate the performance of the proposed approach
under three different architectures of language models.
     To summarize, our main contributions are as follows:

1. We present "KG Structure as Prompt", a novel approach for injecting struc-
   tural information from KGs into prompt-based learning. In experiments un-
   der few-shot settings, we demonstrate that our approach outperforms most
   of the no-KG baselines and achieves performance comparable to the conven-
   tional fine-tuning using a full dataset, even with limited samples.
2. We show that our approach is effective with different types of language model
   architectures and knowledge graphs, showcasing its flexibility and adaptabil-
   ity across various language models and knowledge graphs.
3. We demonstrate the robust capabilities of SLMs: fused with prompt-based
   learning and an access to a knowledge graph, SLMs are able to surpass an
   LLM with much larger number of parameters.²

## 2    Background and Related Work""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk6> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk6> ex:chunkContent """Small Language Models. Small Language Models (SLMs) refer to language
models with fewer parameters, resulting in a reduced capacity to process text
compared to larger-parameter LLMs. However, SLMs typically require less com-
putation resources, making them faster to train and deploy, and maintaining
them is generally more cost-effective. On the contrary, LLMs are trained on vast
amounts of diverse data, thus have significantly more parameters and are capa-
ble of handling more complex language tasks than SLMs. Nevertheless, LLMs
are expensive and difficult to train and deploy as they typically require more
computational resource. For instance, GPT-3 [4], which consists of 175 billion
parameters, is impractical to run on hardware with limited resources.
     In this work, we define SLMs as LMs with less than 1 billion parameters. We
explore the causal capability of SLMs with different architectures: (1) Masked
Language Model (MLM) especially the encoder-only model, (2) Causal Language
Model (CLM) or decoder-only language model, and (3) Sequence-to-Sequence
Language Model (Seq2SeqLM) or encoder-decoder model. We provide an overview
of each type of architecture below.
     MLMs, especially encoder-only models such as BERT [8], are a type of LM
that utilizes encoder blocks within the transformer architecture and are trained
to predict masked tokens based on the context provided by surrounding words.
They excel in natural language understanding (NLU) tasks, e.g., text classifica-
tion, as they are able to capture relationships between words in a text sequence.
CLMs, such as GPT-3 [4], use the decoder blocks within the transformer archi-
tecture and are trained to generate text one token at a time, by conditioning
each token on the preceding tokens in the sequence. Meanwhile, Seq2SeqLMs,
such as T5 [32], consist of both encoder and decoder blocks. The encoder trans-
forms the input sequence into vector representation, while the decoder produces""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk7> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk7> ex:chunkContent """² GPT-3.5-turbo model [31] with ICL [4] prompting method
---
4         Y. Susanti and M. Färber

the output based on the encoded vector. CLMs and Seq2SeqLMs generally work
well for natural language generation (NLG) and NLU tasks such as translation
and summarization, as they can produce coherent and grammatically accurate
sentences. We list our choice of language models in §5.

## Prompt-based Learning & Knowledge Injection

Research on classifying causal relations from text has predominantly occurred within supervised
settings, utilizing classical machine learning (ML) approaches [3,5,6,19,20,28]
or fine-tuning pre-trained language models [11,18,36,33,38]. Classical ML techniques often require extensive feature engineering and have shown inferior performance compared to fine-tuning language models such as BERT [8]. Therefore,
we evaluate our method against fine-tuning methods as baselines.

Meanwhile, prompt-based learning, also known as prompt-tuning, has recently emerged as a promising alternative to the conventional fine-tuning approach for a variety of Natural Language Processing (NLP) tasks [1,26,34,35].
Typically, a prompt is composed of discrete text (hard prompt); however, recent
work has introduced soft prompt, a continuous vector that can be optimized
through backpropagation [23,25]. In the relation classification task, promptbased learning often involves inserting a prompt template containing masked
tokens into the input, essentially converting the task into masked language modeling or text generation problems [7,13,14]. This approach is particularly wellsuited for few-shot or zero-shot scenarios, where only limited labeled data is
available [9,35]. This motivates us to investigate such prompt-based learning
under few-shot settings, given the scarcity of datasets for our causal relation
classification task.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk8> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk8> ex:chunkContent """Other works explore knowledge injection for the prompt construction, for
instance, KnowPrompt [7] injects latent knowledge contained in relation labels
into prompt construction with learnable virtual words. KAPING [2] retrieves
top-K similar triples of the target entities from Wikidata and further augments
them as a prompt. KiPT [24] uses WordNet to calculate semantic correlation
between the input and manually constructed core concepts to construct the
prompts. Our work differs from them since we focus on leveraging structural
information of knowledge graphs to construct the prompt (see §4).

## 3 Task Formulation

In this work, we focus on pairwise knowledge-based causal discovery: given a
pair of entities e1 and e2, i.e., variable or node pairs such as FGF6 and prostate
cancer, the task is to predict if a causal relation can be inferred between the pair.
We formulate the task as a binary classification task, classifying the relation as
causal or non-causal. We evaluate our approach on a dataset D = {X , Y}, where
X is a set of training instances and Y = {causal, non-causal} is a set of relation
labels. Each instance x ∈ X consists of a token sequence x = {w1, w2, ...w|n|}
and the spans of a marked variable pair, and is annotated with a label yx ∈ Y.
---
# KG Structure as Prompt: SLMs for Knowledge-based Causal Discovery

Figure 2

**Fig. 2. Overall framework of our KG Structure as Prompt with prompt-based learning**

## 4 Approach

We illustrate our proposed approach in Fig. 2. First, we generate a graph context, which is derived from the structural information of a knowledge graph with our KG Structure as Prompt method. Next, we feed the generated graph context and the inputs, i.e., the variable pair and its textual context, into the SLMs to train a prompt-based learning model.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk9> .

<https://dblp.org/rec/journals/corr/abs-2407-18752/chunk9> ex:chunkContent """We elaborate our proposed approach in the following subsections. We start with preliminaries (§4.1), followed by the design of the KG structure as Prompt for generating the graph context (§4.2), and the incorporation of the generated graph context into the SLMs architecture with prompt-based learning (§4.3).

### 4.1 Preliminaries

Formally, we define a directed graph G = (V, E) where V is a set of vertices or nodes, and E ⊆ V × V is a set of directed edges. A knowledge graph is a specific type of directed graph representing a network of entities and the relationships between them. Formally, we define a knowledge graph as a directed labeled graph KG = (N, E, R, F) where N is a set of nodes (entities), E ⊆ N × N is a set of edges (relations), R is a set of relation labels, and F : E → R, is a function assigning edges to relation labels. For instance, assignment label r to an edge e = (x, y) can be viewed as a triple (x, r, y), e.g., (Tokyo, IsCapitalOf, Japan).

### 4.2 Knowledge Graph Structure as Prompt

In the field of Graph Neural Networks (GNNs), [44] explores whether LLMs can replace GNNs as the foundation model for graphs by using natural language to describe the geometric structure of the graph. Their results on several graph datasets surpass traditional GNN-based methods, showing the potential
---
6         Y. Susanti and M. Färber

of LLMs as a new foundational model for graphs. Inspired by their success, we
similarly leverage the structural information of a specific type of graph, i.e., a
knowledge graph, to infer causal relationships between variable pairs. We select
knowledge graphs due to their rich structured information and their capability to express interconnected relationships. We call our approach "Knowledge
Graph Structure as Prompt".

For instance, we may infer a causal relationship by looking at multi-hop
relations between a node pair in a knowledge graph, as illustrated in Fig. 3.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-18752/chunk10> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk10> ex:chunkContent """## Domain Adaptation with LLMs

Few works of interest deal with the perfor-
mance of Large Language Models given domain-specific training corpora. Wan
et al. [28] propose a domain adaptation framework for very large language mod-
els (GPT-4 [21]) to address the scarcity of Chinese legal domain texts, using
an adapt-retrieve-revise process. With a smaller LLM trained on in-domain cor-
---
# Do LLMs Really Adapt to Domains? An Ontology Learning Perspective

## Experimental Protocol Overview

| CORPUS PREPARATION | PREPROCESSING | INFERENCE | EVALUATION |
|---------------------|----------------|------------|-------------|
| Domain Ontology from WordNet<br>Root<br><br>Child 1 Child 2<br><br>Parallel<br>Corpus<br>Synthesis<br>Gibberish Parallel Corpus<br>Root<br><br>Child 1 Child 2 | Real Dataset<br>Task-Specific Dataset<br>Extraction<br><br><br><br><br><br>Gibberish Dataset<br>Task-Specific Dataset<br>Extraction | Prompt<br><br><br><br>LARGE<br>LANGUAGE<br>MODEL<br><br><br>Prompt | Predictions on<br>Real Dataset<br><br>Output<br><br><br>vs.<br>Predictions on<br>Real Dataset<br>(Alignment)<br><br>Predictions on<br>Gibberish Dataset<br>Output | Real Targets<br><br>vs. Ground-Truth<br>(Performance)<br><br><br><br><br>Gib. Targets<br><br>vs. Ground-Truth<br>(Performance) |

Fig. 1: Overview of the off-the-shelf evaluation pipeline. The dataset extraction and the prompt depend on the task to perform.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk11> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk11> ex:chunkContent """Fig. 1: Overview of the off-the-shelf evaluation pipeline. The dataset extraction and the prompt depend on the task to perform.

pora, the authors generate a draft answer to retrieve evidence from an external knowledge base, both of which are given to GPT-4 to generate a final answer. Gururangan et al. [8] show that domain-adaptive pretraining (DAPT), i.e. continued pre-training on domain-specific text, allows one to adapt a language model to a domain at reduced cost. However, Cheng et al. [5] argue that while DAPT may improve specific task performance after fine-tuning, it overall hurts the ability of LLMs to perform question answering. Instead, they propose AdaptLLM, a scalable approach that aims to train a LLM on reading comprehension texts created from the raw domain-specific corpora. Finally, Shin et al. [24] show that the in-context learning ability of a LLM heavily depends on the corpus sources, and may emerge by combining multiple corpora, but the domain relevance of the corpus may not be indicative of the few-shot performance of the model.

## 3 Approach

Figure 1 illustrates an overview of the pipeline we employ to test the adaptability and generalizability of off-the-shelf LLMs (pretrained, and as is), which includes two main steps: corpus preparation and LLMs evaluation.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk12> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk12> ex:chunkContent """In particular, we use the English WordNet (2023 Edition) [16] to generate a parallel corpus of terms and definitions in the form of gibberish, which serve as our reference domain-specific setting. The process begins by choosing root concepts (for instance, sweets and desserts) in the WordNet taxonomy, then explores the graph through hyponymy, derivation and other (e.g. topic) relationships across concepts (with a set maximal exploration depth). The explored concepts form a domain in the real WordNet (e.g. sweets domain) that can be used to create a parallel corpus by propagating gibberish representations and definitions. More details about the algorithm can be found in Section 3.1.
---
6         Mai et al.

After obtaining a parallel corpus of a particular domain, we evaluate a LLM on two different tasks: relation extraction and taxonomy discovery, each on both versions of the corpus. Naturally, since the concepts remain the same up to an input-label mapping, we ideally expect the results to be analogous. More details can be found in Subsection 3.2. Additionally, we investigate the effect of fine-tuning on the in-domain performance of LLMs, as described in Subsection 3.3.

### 3.1 Parallel Corpus Synthesis

To simulate a domain that is unseen for the LLM, we generate another KG where the domain concepts have gibberish representations and definitions that do not collide (e.g. if "sugar" is turned into "arghl" then any definition that contains the word "sugar" will see it turned into the word "arghl" instead). For this purpose, we devise a procedure which includes three steps: concept mining, concept linking and gibberish generation. The code can be found online.³""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk13> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk13> ex:chunkContent """**Concept Mining.** The concept mining algorithm is a simple Breadth-First Search algorithm, starting from each of the root concepts C₀ and only going through user-selected relationships (hypernymy, sense derivation, and concept topic). We set a maximal exploration depth D, which is set to 5 during experiments. The explored concepts form a dataset D.

**Concept Linking.** The next step of this process is to establish the dependencies between concept definitions and other concepts. For example, if "sugar" is mentioned in the definition of concept c, then we will link all the concepts c_sugar which have "sugar" as a representation with c by introducing a blank node as follows, where {c_id} denotes the WordNet ID of c:

```
1 c       sct:definitionWord         _:_sugar_{c_id} .
2 _:_sugar_{c_id} rdf:value          "sugar"@en .
3 _:_sugar_{c_id} sct:references     c_sugar .
```

Listing 1.1: Example of linking triples.

**Gibberish Generation.** We assume that we have an algorithm T that creates a gibberish representation T(c) from a concept c based on its initial representation, definition and part-of-speech. A concept is fully processed when it has a gibberish definition AND a gibberish representation. A concept is partially processed when it has a gibberish representation (fully processed implies partially processed).

Denote D₀ the set of concepts in D that have no internal dependencies (i.e. for any c in D₀, the definition of c does not refer to any concept in D), as shown in Figure 2a. We create an initial representation T(c) for any c in D₀, and give them a gibberish definition identical to their original one. We will additionally

³ https://github.com/boschresearch/llm-vs-gibberish-ontologies
---
# Do LLMs Really Adapt to Domains? An Ontology Learning Perspective

Figure 2""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk14> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk14> ex:chunkContent """³ https://github.com/boschresearch/llm-vs-gibberish-ontologies
---
# Do LLMs Really Adapt to Domains? An Ontology Learning Perspective

Figure 2

Fig. 2: Algorithm to propagate gibberish representations and definitions. Squares are blank nodes that indicate that the preceding concept has a word that refers to one of the node's successors. Dashed circles represent the concepts eligible to be fully processed. Green circles represent (at least partially) processed concepts.

(a) Step 0 : Make a gibberish representation and (unchanged) definition for concepts with no dependencies.

(b) Step n + 1: Use at least partially processed concepts to resolve dependencies and create new gibberish representations and definitions.

add the homonyms of the previously processed concepts in D₀, with no gibberish representation. Moreover, set D₋₁ = ∅. We then repeat the following loop until we have fully processed all the concepts in D, as illustrated in Figure 2b.

Suppose we have obtained D_{n-1} and D_{n-2}. If |D_{n-1}| > |D_{n-2}|, we proceed as follows: obtain concepts c that are not fully labeled and which dependencies can all be resolved (i.e. for each dependency c sct:definitionWord x, there exists c' ∈ D_{n-1}, or c' = c such that x sct:references c'). We create gibberish representations T(c) if there is not one already, we resolve the dependencies using the gibberish representations in D_{n-1} to make a definition for c. All homonyms of processed concepts are also partially processed, and all the partially processed concepts are added to D_{n-1} to form D_n.

If |D_{n-1}| = |D_{n-2}|, we sample a random concept c of D that is not partially processed, assign a gibberish representation T(c) to it and add c to D_{n-1} to form D_n. Note that for i ≥ 0, D_i does not exclusively contain concepts in D, but D̄_i = D_i ∩ D does.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk15> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk15> ex:chunkContent """This pipeline yields a set of concepts D, where each concept c ∈ D has a gibberish representation T(c) as well as a gibberish definition consistent with the internal dependencies in D.

## Example

Consider the example depicted in Figure 2. In the first step, Sweet (adjective) and Fruit do not reference any other nodes in their definitions: they can be fully processed. In the second step, since Dessert refers to both Sweet (noun) and Sweet (adjective) and the latter was processed, it is eligible to be processed next. Moreover, since Sweet (adjective) is a homonym of Sweet (noun), it can also be processed. However, since Compote refers to both Dessert and Fruit, it may not be processed yet, since Dessert has not been processed yet. It will be processed in the following step.
---
8         Mai et al.

## 3.2     Off-the-Shelf Evaluation Methodology

For each dataset D, we perform an evaluation by comparing the performance of off-the-shelf LLMs in two different lexical semantic tasks: relation extraction and taxonomy discovery; and compare the outputs coming from two cases: on the original dataset, and its gibberish counterpart.

### Relation Extraction. 
Given a query concept in D, its lexical senses (i.e. written forms) and its definition, the goal of this task is to extract all relations between the concepts mentioned in the definition, including the query concept. To remain in the frame of ontology learning, we focus only on hypernymy and holonymy relationships. For instance, in Example 1, the extracted relations should be as follows: macaron is a subclass of confection, egg white is a part of macaron, icing sugar is a part of macaron, etc. Likewise, given the gibberish definition in 2 instead, we expect the same relations to be extracted, only with the concept names replaced with their gibberish counterpart. In order to retrieve a prediction, the LLM is prompted to output triples in the form: (A, r, B) where r is either is a subclass of or is a part of.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk16> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk16> ex:chunkContent """### Taxonomy Discovery. 
Given two concepts A and B in D, with their lexical senses and definitions, the goal of this task is to determine whether or not A is a subclass of B. Likewise, we expect that turning the lexical senses and definitions of A and B into gibberish will not change the outcome. WordNet hypernymy relations are used as a ground-truth: the predictions on the real (resp. gibberish counterpart of the) dataset are compared with the real (resp. gibberish counterpart of the) ground-truth. Indirect hypernymy relations (obtained with the transitive closure of the relation gwn:hypernym) are also used. Negative examples are produced by corrupting the hypernym once or twice per query (hyponym, subclass of, ?). This classification task can be evaluated with F1-score: a drop in performance should indicate that a LLM relies on the lexical senses to infer taxonomical relationships rather than textual semantic information.

### Prompting. 
For each task/model configuration, we follow general guidelines for prompting a LLM. (1) The return is in JSON format, (2) Chain-of-Thought (CoT) [30] can be used if it improves the performance of the model, (3) One or few exemplar(s) can be used, with example(s) outside of the dataset, if it improves the performance of the model. In the relation extraction task, for a concept C, its written form FC, its part-of-speech PC, and its definition DC, we construct a prompt p(C) as shown in Listing 1.2. In the taxonomy discovery task, for A, B two concepts, FA, FB their written forms, DA, DB their definition, we construct a prompt p(A, B) as shown in Listing 1.3. Prompt templates can be found here.

1 {FORMAT INSTRUCTIONS (Task and return format)}
2 {EXAMPLE(S) (zero, one or few exemplars)}
---
Do LLMs Really Adapt to Domains? An Ontology Learning Perspective    9

```
3
4 Concept: {F_C}
5 Part-of-speech: {P_C}
6 Definition: {D_C}
```

Listing 1.2: Relation extraction generic prompt p(C).""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk17> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk17> ex:chunkContent """```
3
4 Concept: {F_C}
5 Part-of-speech: {P_C}
6 Definition: {D_C}
```

Listing 1.2: Relation extraction generic prompt p(C).

```
1 {FORMAT INSTRUCTIONS (Task and return format)}
2 {EXAMPLE(S) (zero, one or few exemplars)}
3
4 Concept A: {F_A}
5 Definition: {D_A}
6
7 Concept B: {F_B}
8 Definition: {D_B}
```

Listing 1.3: Taxonomy discovery generic prompt p(A, B).

### 3.3 Fine-tuning Experiment

While it was previously verified that fine-tuning improves OL performance in existing domains [19], it is not clear if this statement still holds for arbitrary domains and unknown vocabularies where reasoning is required. To verify this question, after evaluating the zero/one/few-shot performance of LLMs in the gibberish domains, we propose to assess the effect of fine-tuning models on the inference performance for a specific task.

Data split. For each dataset D, we fine-tune a LLM for taxonomy discovery on a train split of hypernymy relations. Half of the concepts in the dataset, and their hypernymy relations, are used for training. The inverse relations are used as negatives (if A is a subclass of B, then B is not a subclass of A), alongside some randomly sampled negatives. The remaining relations are used for testing.

Prompting. We train LLMs on instructive datasets with the prefix prompt in Listing 1.4, completed using each pair of concepts in the hypernymy dataset.

```
1 ### HUMAN:
2 Identify whether the statement is true or false. Answer with
   only one word: 'True' or 'False'.
3
4 CONCEPT A: {term_a} ({pos_a})
5 Definition: {definition_a}
6
7 CONCEPT B: {term_b} ({pos_b})
8 Definition: {definition_b}
9
10 Statement: '{term_a}' is a subclass of '{term_b}'.
```
---
10        Mai et al.

Table 1: Dataset statistics. The 'Hypernyms' column indicates the number of retrieved hypernymy relationships within the dataset. The 'Depth' column indicates the exploration depth of the algorithm.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk18> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk18> ex:chunkContent """Table 1: Dataset statistics. The 'Hypernyms' column indicates the number of retrieved hypernymy relationships within the dataset. The 'Depth' column indicates the exploration depth of the algorithm.

| D | Concepts | Hypernyms | Depth | Root Concepts |
|---|----------|-----------|-------|----------------|
| WN-sweets | 244 | 418 | 5 | sweet (n), sweet (a), sugar |
| WN-football | 937 | 1401 | 5 | football, team, offensive (a), defensive (a) |
| WN-music | 1366 | 2497 | 5 | musical instrument |

Listing 1.4: Prefix prompt used for the fine-tuning experiment. The content between curly brackets is formatted for each pair of concepts in the dataset.

## 4 Experiments

In this section, we explore the off-the-shelf evaluation proposed in Subsection 3.2, and the fine-tuning experiments described in Subsection 3.3.

### 4.1 Experimental setup

Datasets. To assess the performance variation with domain specificity, we generate three synthetic domain-specific datasets as parallel corpora from the Open English WordNet [16], using the methodology described in Section 3.1. They are:

- Sweets: A collection of concepts related to sweets, desserts, sweet food or sugar. In this dataset, hypernyms are frequent, and concepts are relatively well constructed from their hypernyms.
- Football: A collection of concepts related to football. This dataset, created by browsing co-topic concepts, includes less taxonomic relationships, but has its own terminology and jargon.
- Music: A collection of concepts related to musical instruments. It is the largest of the three.

Table 1 shows, for each dataset, the number of concepts, the number of hypernymy relationships, the exploration depth and the root concepts. The translator T used to generate gibberish representations of concepts is available online.4.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk19> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk19> ex:chunkContent """Models. While this study is not comprehensive of all existing LLMs, our goal is to show a particular trend between them. In the off-the-shelf evaluation, we evaluate the following popular LLMs, with their number of parameters between parentheses: GPT-3.5 [3] (174B), GPT-4 [21] (≥ 1T), Falcon-40B [1] (40B),

4 https://github.com/htmai-880/gibberify
---
Do LLMs Really Adapt to Domains? An Ontology Learning Perspective    11

LLaMa2-13B [26] (13B), and Zephyr-7B-β [27] (7B). The former two, accessed
with a paid subscription, are closed-source, whereas the latter three are open-
source. In the fine-tuning experiment, we consider Zephyr-7B-β [27] and Falcon-
7B [1], which are both open-source. For the paid subscription models, We limit
our budget to 15€ per dataset.

Specificities. We henceforth consider the three following evaluation methods:

- Ground-truth (GT)(en) vs en we compare the answers on the original En-
  glish dataset against the ground-truth
- Ground-truth (GT)(gib) vs gib: we compare the answers on the gibberish
  dataset against the gibberish ground-truth
- en vs gib: using the answers on the original dataset as a ground-truth, we
  evaluate the consistency of the predictions on the gibberish dataset, regard-
  less of their correctness.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk20> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk2> ex:chunkContent """### Abstract. 
Large Language Models (LLMs) have demonstrated unprecedented prowess across various natural language processing tasks in various application domains. Recent studies show that LLMs can be leveraged to perform lexical semantic tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL). However, it has not effectively been verified whether their success is due to their ability to reason over unstructured or semi-structured data, or their effective learning of linguistic patterns and senses alone. This unresolved question is particularly crucial when dealing with domain-specific data, where the lexical senses and their meaning can completely differ from what a LLM has learned during its training stage. This paper investigates the following question: Do LLMs really adapt to domains and remain consistent in the extraction of structured knowledge, or do they only learn lexical senses instead of reasoning? To answer this question and, we devise a controlled experiment setup that uses WordNet to synthesize parallel corpora, with English and gibberish terms. We examine the differences in the outputs of LLMs for each corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical results show that, while adapting to the gibberish corpora, off-the-shelf LLMs do not consistently reason over semantic relationships between concepts, and instead leverage senses and their frame. However, fine-tuning improves the performance of LLMs on lexical semantic tasks even when the domain-specific terms are arbitrary and unseen during pre-training, hinting at the applicability of pre-trained LLMs for OL.

**Keywords:** ontology learning · LLMs · domain adaptation.

## 1 Introduction""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk3> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk20> ex:chunkContent """In the relation extraction (GT(X) vs X), due to the scarcely annotated holonymy
relationships in WordNet, we focus on hypernymy relationships to compute met-
rics. Moreover, a model prediction is processed by taking into account the in-
ferred hypernymy relationships, using the transitive property of the relation. For
instance, if a model outputs the triples (vanilla pudding, is a subclass of, custard-
like pudding), and (custard-like pudding, is a subclass of, pudding), we count
the triple (vanilla pudding, is a subclass of, pudding) as effectively predicted by
the model. In the taxonomy discovery task, predictions that are neither "True"
or "False" are ignored. Thus, macro-averaged F1-scores may not be between the
macro-averaged precisions and the macro-averaged recalls.

## 4.2 Off-the-Shelf Evaluation

Metrics. For each model/task configuration, we compute metrics in three set-
tings: GT(en) vs en, GT(gib) vs gib, and en vs gib. The goal is to see if the
predictions with gibberish terms align with the predictions with the real terms.
We compute the following metrics: precision, recall and F1-score.

Results. We first examine the results with respect the to ground-truths. It is
important to mention that the English WordNet is scarcely annotated in terms
of hypernymy and holonymy relationships. Consider the following example:

| Example 3 (toffee apple). an apple that is covered with a candy-like sub- |
| stance (usually caramelized sugar).                                       |

In this example, the definition obviously implies that a toffee apple is an
apple, but WordNet only considers sweet, confection to be valid hypernyms. The
incompleteness of WordNet explains why the observed performances are so low,
but because our goal is a relative comparison of performances on real corpora
---
12        Mai et al.

Table 2: Results on the relation extraction task in the GT(X) vs X setting, where X is either en or gib. Only hypernymy relations are considered.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk21> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk21> ex:chunkContent """Table 2: Results on the relation extraction task in the GT(X) vs X setting, where X is either en or gib. Only hypernymy relations are considered.

| Model | X | WN-sweets | | | WN-football | | | WN-music | | |
|---|---|---|---|---|---|---|---|---|---|---|
| | | Pre. | Rec. | F1 | Pre. | Rec. | F1 | Pre. | Rec. | F1 |
| GPT-3.5 | en | 0.478 | 0.150 | 0.228 | 0.383 | 0.056 | 0.097 | 0.397 | 0.060 | 0.104 |
| | gib | 0.336 | 0.069 | 0.115 | 0.371 | 0.035 | 0.065 | 0.307 | 0.029 | 0.053 |
| GPT-4 | en | 0.583 | 0.160 | 0.251 | - | - | - | - | - | - |
| | gib | 0.530 | 0.129 | 0.207 | - | - | - | - | - | - |
| Falcon-40B | en | 0.573 | 0.151 | 0.238 | 0.489 | 0.067 | 0.118 | 0.529 | 0.065 | 0.116 |
| | gib | 0.330 | 0.080 | 0.128 | 0.382 | 0.050 | 0.088 | 0.341 | 0.042 | 0.074 |
| LLaMa2-13B | en | 0.536 | 0.141 | 0.223 | 0.423 | 0.035 | 0.065 | 0.434 | 0.030 | 0.056 |
| | gib | 0.365 | 0.085 | 0.138 | 0.341 | 0.018 | 0.035 | 0.296 | 0.014 | 0.026 |
| Zephyr-7B-β | en | 0.441 | 0.158 | 0.233 | 0.399 | 0.067 | 0.115 | 0.374 | 0.063 | 0.108 |
| | gib | 0.243 | 0.095 | 0.137 | 0.313 | 0.052 | 0.089 | 0.261 | 0.044 | 0.075 |

and their gibberish counterpart, rather than absolute scores, it is not critical to have a high-quality ground-truth.

In both tasks, which results are reported in Table 2 and Table 3, a common trend occurs across all LLMs and in all synthetic domains: a significant performance decrease is observed when replacing real terms with gibberish. Although GPT-4 (tested on WN-sweets only because of the slowness and the costs) performs best on gibberish corpora, it suffers from the same performance drop.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk22> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk22> ex:chunkContent """While the performance on relation extraction is generally low across all LLMs with the real datasets, which we mainly attribute to the poor quality of the annotations, it is even lower with the gibberish datasets. In each case, the macro F1-score is practically halved, e.g. in WN-sweets, GPT-3.5 drops from 0.228 to 0.115, Falcon-40B drops from 0.238 to 0.128, LLaMa2-13B drops from 0.223 to 0.138, Zephyr-7B-β drops from 0.233 to 0.137. Note that the recall is low in comparison to the precision, which indicates that a LLM tends to overlook indicators that two concepts are hierarchically related. This observation aligns with the conclusion of LLMs4OL [7], according to which off-the-shelf LLMs are not sufficiently suitable for OL tasks, particularly in the case of relation extraction.

In the taxonomy discovery task, the performance of LLMs also plummets when using the gibberish dataset instead of the real one. The LLMs are relatively good at identifying whether two real concepts are hierarchically related (e.g. F1-score up to 0.940 on the WN-sweets dataset by GPT-3.5), but suffer from a large performance drop when confronted to unknown words which share the same semantic relations (e.g. the F1-score of GPT-3.5 drops from 0.940 to 0.446 when using the gibberish counterpart of WN-sweets; the same behavior is observed across all datasets and LLMs). We interpret this drop as the fact that LLMs are significantly better at leveraging semantic priors (i.e. lexical senses known from pre-training), to deduce that Concept A is a subclass of Concept B.
---
Do LLMs Really Adapt to Domains? An Ontology Learning Perspective    13

Table 3: Results (macro-average) on the taxonomy discovery task in the GT(X) vs X setting, where X is either en or gib.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk23> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk23> ex:chunkContent """Table 3: Results (macro-average) on the taxonomy discovery task in the GT(X) vs X setting, where X is either en or gib.

| Model | X | WN-sweets | | | WN-football | | | WN-music | | |
|---|---|---|---|---|---|---|---|---|---|---|
| | | Pre. | Rec. | F1 | Pre. | Rec. | F1 | Pre. | Rec. | F1 |
| GPT-3.5 | en | 0.944 | 0.937 | 0.940 | 0.758 | 0.701 | 0.648 | 0.829 | 0.858 | 0.818 |
| | gib | 0.783 | 0.539 | 0.446 | 0.640 | 0.505 | 0.333 | 0.687 | 0.537 | 0.361 |
| GPT-4 | en | 0.949 | 0.943 | 0.945 | - | - | - | - | - | - |
| | gib | 0.591 | 0.576 | 0.566 | - | - | - | - | - | - |
| Falcon-40B | en | 0.775 | 0.658 | 0.598 | 0.800 | 0.648 | 0.637 | 0.787 | 0.620 | 0.613 |
| | gib | 0.591 | 0.575 | 0.574 | 0.483 | 0.475 | 0.478 | 0.541 | 0.535 | 0.480 |
| LLaMa2-13B | en | 0.819 | 0.772 | 0.750 | 0.808 | 0.811 | 0.809 | 0.785 | 0.800 | 0.790 |
| | gib | 0.450 | 0.447 | 0.444 | 0.533 | 0.531 | 0.504 | 0.576 | 0.556 | 0.465 |
| Zephyr-7B-β | en | 0.899 | 0.897 | 0.898 | 0.813 | 0.751 | 0.759 | 0.821 | 0.762 | 0.778 |
| | gib | 0.691 | 0.634 | 0.621 | 0.500 | 0.500 | 0.469 | 0.530 | 0.524 | 0.523 |

Table 4: Alignment results (macro-average) on the relation extraction task in the en vs gib setting. Both hypernymy and holonymy relations are considered.

| Model | WN-sweets | | | WN-football | | | WN-music | | |
|---|---|---|---|---|---|---|---|---|---|
| | Pre. | Rec. | F1 | Pre. | Rec. | F1 | Pre. | Rec. | F1 |
| GPT-3.5 | 0.371 | 0.304 | 0.334 | 0.263 | 0.175 | 0.210 | 0.207 | 0.138 | 0.166 |
| GPT-4 | 0.504 | 0.527 | 0.515 | - | - | - | - | - | - |
| Falcon-40B | 0.310 | 0.303 | 0.306 | 0.236 | 0.238 | 0.237 | 0.225 | 0.224 | 0.225 |
| LLaMa2-13B | 0.347 | 0.340 | 0.344 | 0.386 | 0.225 | 0.284 | 0.374 | 0.215 | 0.273 |
| Zephyr-7B-β | 0.214 | 0.229 | 0.221 | 0.192 | 0.180 | 0.185 | 0.148 | 0.142 | 0.145 |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk24> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk24> ex:chunkContent """Although the general performance drop is expected because the tested LLM would then be dealing with words it has never seen during its training, we furthermore observe that the prediction alignment is very low. In spite of analogous concepts sharing the same semantic relations with each other in the parallel corpora, the model is unable to produce analogous outputs from analogous inputs, as shown by the low F1-scores in Tables 4 and 5. This is evidence that as is, LLMs do not reason over semantic relationships.

Our interpretation is that the attention mechanism heavily relies on the lexical sense and the frame of a token, instead of leveraging the semantic relationships that hold between tokens. In other words, the "reasoning" abilities of LLMs for Ontology Learning are mostly limited to entities and concepts that the models have already been trained on, i.e. prior semantics. However, such a quality is critical for Ontology Learning in arbitrary domains, where hypernymy relationships must be retrieved for unknown concepts, or new concepts that share the same lexical form as some existing word (for instance, if domain-specific jargon employs existing words with different meanings).
---
14        Mai et al.

Table 5: Alignment results (macro-average) on the taxonomy discovery task in the en vs gib setting.

| Model | WN-sweets | | | WN-football | | | WN-music | | |
|---|---|---|---|---|---|---|---|---|---|
| | Pre. | Rec. | F1 | Pre. | Rec. | F1 | Pre. | Rec. | F1 |
| GPT-3.5 | 0.789 | 0.541 | 0.465 | 0.700 | 0.517 | 0.488 | 0.738 | 0.544 | 0.459 |
| GPT-4 | 0.611 | 0.594 | 0.590 | - | - | - | - | - | - |
| Falcon-40B | 0.541 | 0.557 | 0.412 | 0.502 | 0.495 | 0.443 | 0.529 | 0.562 | 0.352 |
| LLaMa2-13B | 0.565 | 0.570 | 0.556 | 0.618 | 0.610 | 0.586 | 0.641 | 0.599 | 0.535 |
| Zephyr-7B-β | 0.818 | 0.727 | 0.716 | 0.557 | 0.544 | 0.545 | 0.570 | 0.572 | 0.570 |

Table 6: Hypernym train-test split for the fine-tuning experiment.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk25> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk25> ex:chunkContent """Table 6: Hypernym train-test split for the fine-tuning experiment.

| D | Train | | Test | |
|---|---|---|---|---|
| | Positives | Negatives | Positives | Negatives |
| WN-sweets | 189 | 393 | 229 | 284 |
| WN-football | 674 | 1043 | 727 | 567 |
| WN-music | 1367 | 1882 | 1130 | 851 |

This trend seems to hold true in both tasks, which confirms the fact that off-the-shelf LLMs are currently not suited for OL tasks on arbitrary domains.

### 4.3 Fine-Tuning Evaluation

Training details. Table 6 shows the number of hypernym pairs used for the fine-tuning experiment. We use instruction tuning specifically tailored towards taxonomy discovery. In the prefix prompt documented in Listing 1.4, given two concepts A and B, the model must return "True" if A is a subclass of B and "False" otherwise. We train the model on 20 epochs, with a batch size of 4 and a learning rate of 3 · 10^(-6). For efficient training, we quantize the model on 4 bits and use the LoRA [10] method with parameters α = 256 and r = 1024.

Metrics. Similarly to the first experiment, we use Precision, Recall and F1-score to evaluate the performance of the model on the testing set.

Results. The results of the fine-tuning experiment are reported in Table 7. Two general observations can be made about the experiment. Firstly, we notice that fine-tuning drastically improves the task-specific performance of the LLM regardless the domain and corpus version. For instance, while Falcon-7B seems to be initially worse-performing than Zephyr-7B-β overall, the F1-score improves up to almost threefold (real WN-music dataset, from 0.327 to 0.874). Although not surprising for real corpora, an improvement on gibberish corpora is non-trivial: the LLMs show signs of adaptation on gibberish corpora with improved
---
## Do LLMs Really Adapt to Domains? An Ontology Learning Perspective 15""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk26> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk26> ex:chunkContent """### Table 7: Taxonomy discovery results with fine-tuning in the GT(X) vs X setting, where X is either en or gib. Evaluation is performed before and after fine-tuning.

| Model (X) | When | WN-sweets ||| WN-football ||| WN-music |||
|---|---|---|---|---|---|---|---|---|---|---|
| | | Pre. | Rec. | F1 | Pre. | Rec. | F1 | Pre. | Rec. | F1 |
| Falcon-7B (en) | Before | 0.564 | 0.507 | 0.388 | 0.561 | 0.504 | 0.325 | 0.631 | 0.509 | 0.327 |
| | After | 0.923 | 0.902 | 0.907 | 0.867 | 0.871 | 0.868 | 0.874 | 0.881 | 0.874 |
| Falcon-7B (gib) | Before | 0.275 | 0.491 | 0.352 | 0.594 | 0.501 | 0.309 | 0.314 | 0.498 | 0.300 |
| | After | 0.725 | 0.663 | 0.655 | 0.685 | 0.687 | 0.679 | 0.708 | 0.703 | 0.683 |
| Zephyr-7B-β (en) | Before | 0.898 | 0.845 | 0.853 | 0.772 | 0.679 | 0.618 | 0.783 | 0.722 | 0.674 |
| | After | 0.905 | 0.906 | 0.897 | 0.940 | 0.939 | 0.939 | 0.941 | 0.939 | 0.940 |
| Zephyr-7B-β (gib) | Before | 0.746 | 0.572 | 0.500 | 0.740 | 0.621 | 0.535 | 0.723 | 0.589 | 0.479 |
| | After | 0.840 | 0.816 | 0.796 | 0.859 | 0.810 | 0.817 | 0.839 | 0.846 | 0.839 |

performance. Secondly, while the performance of a model on a gibberish corpus increases after fine-tuning, it never matches the performance of the same model fine-tuned on the real counterpart of the corpus. It is worth noting that this is a limitation of adaptation solely due to the reliance on prior semantics, since a corpus and its gibberish counterpart only differ in their input-label mapping.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk27> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk27> ex:chunkContent """### Transfer Learning. 
In spite of the two previous observations, we can hypothesize that the improvement in performance on the gibberish corpora may indicate signs of reasoning and generalization on unseen domains, given that gibberish words are most likely not contained in the vocabulary of LLMs. To elaborate this claim, we propose another experiment: we use Zephyr-7B-β, trained for taxonomy discovery in a domain, and test it on another domain. By only using gibberish corpora, we ensure that most of the domain-specific terminology is anonymized, preventing the LLM from effectively using prior semantics. Results are reported in Figure 3. We observe the following. Firstly, the F1-scores generally tend to increase after transfer, with the exception of the WN-music to WN-sweets case. The increase in F1-score is substantial, from 14% (WN-football to WN-sweets), up to 32% (WN-music to WN-football). This result is quite promising, as it points towards the possibility of OL on arbitrary domains with LLMs with effective pre-training. Secondly, the precision tends to drop in favor of the recall, indicating that fine-tuning makes the LLM more sensitive to syntactic clues of hypernymy relations at the cost of making them less precise. Due to the fact that both the training and the testing domains are made of gibberish words, we attribute the performance improvements of the LLM (with respect to its base version) to emerging reasoning capabilities: the fine-tuning LLM becomes more capable of abstraction and of focusing on semantic relationships between concepts, rather than the concepts themselves.
---
16           Mai et al.

| WN-sweets to WN-sweets | Before | After |
|-------------------------|--------|-------|
| Precision               | 0.746  | 0.746 |
| Recall                  | 0.572  | 0.572 |
| F1                      | 0.504  | 0.504 |""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk28> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk28> ex:chunkContent """| WN-sweets to WN-football | Before | After |
|--------------------------|--------|-------|
| Precision                | 0.740  | 0.720 |
| Recall                   | 0.621  | 0.664 |
| F1                       | 0.535  | 0.658 |

| WN-sweets to WN-music | Before | After |
|------------------------|--------|-------|
| Precision              | 0.723  | 0.715 |
| Recall                 | 0.589  | 0.638 |
| F1                     | 0.479  | 0.626 |

| WN-football to WN-sweets | Before | After |
|--------------------------|--------|-------|
| Precision                | 0.746  | 0.612 |
| Recall                   | 0.572  | 0.601 |
| F1                       | 0.504  | 0.578 |

| WN-football to WN-football | Before | After |
|----------------------------|--------|-------|
| Precision                  | 0.740  | 0.740 |
| Recall                     | 0.621  | 0.621 |
| F1                         | 0.535  | 0.535 |

| WN-football to WN-music | Before | After |
|--------------------------|--------|-------|
| Precision                | 0.723  | 0.627 |
| Recall                   | 0.589  | 0.627 |
| F1                       | 0.479  | 0.627 |

| WN-music to WN-sweets | Before | After |
|------------------------|--------|-------|
| Precision              | 0.746  | 0.512 |
| Recall                 | 0.572  | 0.508 |
| F1                     | 0.504  | 0.475 |

| WN-music to WN-football | Before | After |
|--------------------------|--------|-------|
| Precision                | 0.740  | 0.754 |
| Recall                   | 0.621  | 0.732 |
| F1                       | 0.535  | 0.706 |

| WN-music to WN-music | Before | After |
|----------------------|--------|-------|
| Precision            | 0.723  | 0.723 |
| Recall               | 0.589  | 0.589 |
| F1                   | 0.479  | 0.479 |

Fig. 3: Domain transfer experiment results with Zephyr-7B-β. Only gibberish
corpora are selected for both training and testing.

## 5 Conclusion""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk29> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk29> ex:chunkContent """Fig. 3: Domain transfer experiment results with Zephyr-7B-β. Only gibberish
corpora are selected for both training and testing.

## 5 Conclusion

We have explored and tested the limits of adaptability and generalizability of
LLMs, and observed that LLMs do not adapt well to arbitrary domains. By
creating gibberish datasets based on real data and real domains from Word-
Net, and using LLMs to perform ontology learning tasks on these data, it is
realized that LLMs are unable to consistently retrieve the same taxonomic re-
lationships between analogous concepts, which highlights their clear reliance on
priorly learned semantics, lexical senses, and the frame of the tokens. However,
we notice that after fine-tuning on gibberish data, LLMs improve at discovering
hierarchies, both on the domain they were trained on and other arbitrary do-
mains. We attribute this improvement to the emergence of reasoning with lexical
semantics. Our work serves as cautionary advice for the community that LLMs
do not adapt to arbitrary domains, and we hope that it can inspire future work
to leverage reasoning with LLMs for Ontology Learning.

Supplemental Material Statement: Generated datasets (real and gibberish for all
domains), source code for generating the synthetic datasets from the Open En-
glish WordNet, and for fine-tuning or evaluating LLMs on OL tasks are available
online.5

Acknowledgement. The work was partially supported by EU project: enRich-
MyData (HORIZON-CL4-2021-DATA-01 - GA 101070284).

5 https://github.com/boschresearch/llm-vs-gibberish-ontologies
---
Do LLMs Really Adapt to Domains? An Ontology Learning Perspective    17

## References

1. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune, B., Pannier, B., Penedo, G.: Falcon-40B: an open large language model with state-of-the-art performance (2023)""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk30> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk3> ex:chunkContent """**Keywords:** ontology learning · LLMs · domain adaptation.

## 1 Introduction

Knowledge Bases (KB) and ontologies play a key role in structuring and organizing knowledge across domains, and offer powerful solutions to link data that would otherwise remain unstructured (such as text). As of now, many sources of data can be used as ontologies, of varying specificity. For instance, WordNet [17], ConceptNet [14] and WebIsA [23] contain common knowledge, whereas KBs such
---
2         Mai et al.

as Unified Medical Language System (UMLS) [2] and GeoNames [6] have their own domain specificities (respectively, medical and geographic). For knowledge-intensive applications, access to structured data is of utmost importance, but creating it is a very tedious and time-consuming process that inevitably demands domain expertise. Ontology Learning (OL) is a field of artificial intelligence that concerns itself with automatically identifying terms, types and axioms between them from unstructured or structured information sources such as text or KBs. In particular, OL deals with identifying hyponymy (resp. hypernymy) relations in a KB. That is, for pairs of concepts (A, B) in the KB, one wants to infer whether or not Concept A is a subclass (resp. a superclass) of Concept B.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk4> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk30> ex:chunkContent """2. Bodenreider, O.: The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research 32 Database issue, D267–70 (2004)

3. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020)

4. Caufield, J.H., Hegde, H., Emonet, V., Harris, N.L., Joachimiak, M.P., Matentzoglu, N., Kim, H., Moxon, S.A.T., Reese, J.T., Haendel, M.A., Robinson, P.N., Mungall, C.J.: Structured prompt interrogation and recursive extraction of semantics (spires): A method for populating knowledge bases using zero-shot learning (2023)

5. Cheng, D., Huang, S., Wei, F.: Adapting large language models via reading comprehension (2024)

6. GeoNames: Geonames, https://www.geonames.org/

7. Giglou, H.B., D'Souza, J., Auer, S.: Llms4ol: Large language models for ontology learning (2023)

8. Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., Smith, N.A.: Don't stop pretraining: Adapt language models to domains and tasks. In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (eds.) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8342–8360. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.740

9. Hertling, S., Paulheim, H.: Olala: Ontology matching with large language models. In: Proceedings of the 12th Knowledge Capture Conference 2023. p. 131–139. K-CAP '23, Association for Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/3587259.3627571""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk31> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk31> ex:chunkContent """10. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models (2021)

11. Kauf, C., Ivanova, A.A., Rambelli, G., Chersoni, E., She, J.S., Chowdhury, Z., Fedorenko, E., Lenci, A.: Event knowledge in large language models: the gap between the impossible and the unlikely (2023)

12. Li, J., Cheng, X., Zhao, W.X., Nie, J.Y., Wen, J.R.: Halueval: A large-scale hallucination evaluation benchmark for large language models (2023)

13. Li, X.L., Kuncoro, A., Hoffmann, J., de Masson d'Autume, C., Blunsom, P., Nematzadeh, A.: A systematic investigation of commonsense knowledge in large language models. In: Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.) Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. pp. 11838–11855. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates (Dec 2022). https://doi.org/10.18653/v1/2022.emnlp-main.812

14. Liu, H., Singh, P.: Conceptnet — a practical commonsense reasoning tool-kit. BT Technology Journal 22(4), 211–226 (oct 2004). https://doi.org/10.1023/B:BTTJ.0000047600.45421.6d
---
18         Mai et al.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk32> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk32> ex:chunkContent """15. Mateiu, P., Groza, A.: Ontology engineering with large language models (2023)
16. McCrae, J.P., Rademaker, A., Bond, F., Rudnicka, E., Fellbaum, C.: English Word-
    Net 2019 – an open-source WordNet for English. In: Vossen, P., Fellbaum, C. (eds.)
    Proceedings of the 10th Global Wordnet Conference. pp. 245–252. Global Wordnet
    Association, Wroclaw, Poland (Jul 2019)
17. Miller, G.A.: Wordnet: a lexical database for english. Commun. ACM 38(11), 39–41
    (nov 1995). https://doi.org/10.1145/219717.219748
18. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., Zettlemoyer,
    L.: Rethinking the role of demonstrations: What makes in-context learning work?
    (2022)
19. Moskvoretskii, V., Neminova, E., Lobanova, A., Panchenko, A., Nikishina, I.: Tax-
    ollama: Wordnet-based model for solving multiple lexical sematic tasks (2024)
20. Norouzi, S.S., Mahdavinejad, M.S., Hitzler, P.: Conversational ontology alignment
    with chatgpt (2023)
21. OpenAI: Gpt-4 technical report (2024)
22. Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A.H., Riedel,
    S.: Language models as knowledge bases? (2019)
23. Seitner, J., Bizer, C., Eckert, K., Faralli, S., Meusel, R., Paulheim, H., Ponzetto,
    S.P.: A large DataBase of hypernymy relations extracted from the web. In: Calzo-
    lari, N., Choukri, K., Declerck, T., Goggi, S., Grobelnik, M., Maegaard, B., Mariani,
    J., Mazo, H., Moreno, A., Odijk, J., Piperidis, S. (eds.) Proceedings of the Tenth
    International Conference on Language Resources and Evaluation (LREC'16). pp.
    360–367. European Language Resources Association (ELRA), Portorož, Slovenia
    (May 2016)
24. Shin, S., Lee, S.W., Ahn, H., Kim, S., Kim, H., Kim, B., Cho, K., Lee, G., Park,
    W., Ha, J.W., Sung, N.: On the effect of pretraining corpora on in-context learning
    by a large-scale language model. In: Carpuat, M., de Marneffe, M.C., Meza Ruiz,""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk33> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk33> ex:chunkContent """W., Ha, J.W., Sung, N.: On the effect of pretraining corpora on in-context learning
    by a large-scale language model. In: Carpuat, M., de Marneffe, M.C., Meza Ruiz,
    I.V. (eds.) Proceedings of the 2022 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies.
    pp. 5168–5186. Association for Computational Linguistics, Seattle, United States
    (Jul 2022). https://doi.org/10.18653/v1/2022.naacl-main.380
25. Sun, K., Xu, Y.E., Zha, H., Liu, Y., Dong, X.L.: Head-to-tail: How knowledgeable
    are large language models (llm)? a.k.a. will llms replace knowledge graphs? (2023)
26. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
    lykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C.,
    Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao,
    C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kar-
    das, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux,
    M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov,
    T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Sal-
    adi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang,
    B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan,
    A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom,
    T.: Llama 2: Open foundation and fine-tuned chat models (2023)
27. Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang,
    S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A.M.,
    Wolf, T.: Zephyr: Direct distillation of lm alignment (2023)
28. wan, Z., Zhang, Y., Wang, Y., Cheng, F., Kurohashi, S.: Reformulating domain
    adaptation of large language models as adapt-retrieve-revise (2023)
---""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk34> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk34> ex:chunkContent """28. wan, Z., Zhang, Y., Wang, Y., Cheng, F., Kurohashi, S.: Reformulating domain
    adaptation of large language models as adapt-retrieve-revise (2023)
---
Do LLMs Really Adapt to Domains? An Ontology Learning Perspective    19""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk35> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk35> ex:chunkContent """29. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models (2022)

30. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models (2023)

31. Wei, J., Hou, L., Lampinen, A., Chen, X., Huang, D., Tay, Y., Chen, X., Lu, Y., Zhou, D., Ma, T., Le, Q.V.: Symbol tuning improves in-context learning in language models (2023)

32. Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., Ma, T.: Larger language models do in-context learning differently (2023)

33. Zhao, Z., Lee, W.S., Hsu, D.: Large language models as commonsense knowledge for large-scale task planning (2023)""" .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk4> ex:chunkContent "In the past few years, Large Language Models (LLMs), such as GPT-3 [3], GPT-4, LLaMa2 [26] or Falcon-40B [1] have displayed unprecedented prowess in many NLP applications across various domains. LLMs are language models with a very large parameter count that are trained with enormous amounts of textual data. Hence, they are equipped with common knowledge and have shown remarkable success in generating text. Furthermore, LLMs have made it possible to capture the meaning of text and reason about it, providing a promising alternative for knowledge-intensive tasks such as KB completion and ontology-related tasks like OL and OM (Ontology Matching). Recent studies have shown that LLMs could be viewed as Knowledge Bases [22], storing knowledge incorporated in their parameters (e.g. factual knowledge [25], event knowledge [11], commonsense knowledge [13,33], etc). In particular, LLMs4OL [7] introduces a novel approach to OL using LLMs: the work provides comprehensive empirical evidence that, although requiring fine-tuning for better performance, LLMs can work as effective ontology learners on specialized datasets." ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk5> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk5> ex:chunkContent """However, there are several challenges that come with LLMs, and that have been left relatively unexplored in the frame of ontology learning. They include: (1) LLMs are susceptible to hallucinate [12], i.e. provide generate text that is syntactically sound but factually incorrect. (2) LLMs are trained on massive corpora of textual data and acquire common knowledge, but their few - or zero - shot generalizability and adaptability to unknown domains remains relatively undiscussed. Studying these two aspects is crucial to better understand the current limits of LLMs, and is ever so needed since they are being become increasingly prominent for domain-specific uses. On the one hand, it is effectively possible to get LLMs to adapt to a domain by fine-tuning, but such a process requires labeled data in this domain and can be computationally expensive. On the other hand, generalizability is also an extremely valuable quality for OL in domain-specific settings. To illustrate this, consider the Examples 1 and 2.

In OL, a LLM may be able to identify that a macaron is a subclass of confection that is made from many ingredients such as egg white, icing sugar, granulated sugar, and so on. Given Example 2, obtained by turning the words of Example 1 into gibberish, a model capable of generalization should retrieve the analogous concepts as hypernyms or meronyms (i.e., twiglomptoroa is a subclass of becsverdecoroal etc.). However, it was not explicitly verified if LLMs would do so, or more broadly, if they are able to generalize taxonomic axioms over text
---
Do LLMs Really Adapt to Domains? An Ontology Learning Perspective    3""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk6> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk6> ex:chunkContent """| Example 1 (macaron). a sweet meringue-based confection made with egg white, icing sugar, granulated sugar, almond powder or ground almond, and food colouring. |
|---|
| Example 2 (twiglomptoroa). a becsverdecoroa meringue-based becsverdecoroal made with egg white, icing shifbousadu stintsobroa, granulated shifbousadu stintsobroa, almond powder or ground almond, and food colouring. |

(e.g. textual patterns indicating that A is a subclass of B) rather than learn the concepts themselves during the pre-training process.

This paper presents comprehensive experiments to study the generalizability and domain adaptability of LLMs, with perspective of ontology learning. By synthesizing three new domain corpora from the Open English WordNet, and creating a gibberish counterpart each, we assess the performance of LLMs on two main tasks: relation extraction and taxonomy discovery. We conduct the experiments on popular LLMs, including closed- and open-source, off-the-shelf and fine-tuned, and evaluate them on both in-domain and across-domain setups. The novel contributions of this paper are as follows:

- We create three synthetic datasets as parallel corpora from the Open English WordNet, by turning domain-specific terms into gibberish.
- We conduct experiments to simulate the adaptability and generalizability of LLMs in unseen domains, with or without backpropagation.
- We provide empirical evidence that there is a limit to the generalization capability of off-the-shelf LLMs, with OL tasks that leverage lexical semantics such as relation extraction and taxonomy discovery.
- We show that in-domain fine-tuning improves in-domain task-specific performance, and that the improvements are transferable to new domains.

## 2 Related Work""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk7> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk7> ex:chunkContent """## 2 Related Work

Ontology Learning with LLMs. OL is the (semi)automatic acquisition of T-Box and/or A-Box data from various data sources. In the context of this work, we look at OL from unstructured text or semi-structured data such as Knowledge Graphs paired with textual descriptions. More generally, recent studies show that LLMs can be leveraged to perform ontology related tasks, such as OM (Ontology Matching) or OL. For instance, Norouzi et al. [20] use a naive approach that uses ChatGPT for ontology alignment, by providing the entire source and target ontologies. Mateiu et al. [15] use a fine-tuned GPT-3 to convert natural language into OWL Functional Syntax for ontology enrichment. Hertling et al. [9] use few-shot prompting to enhance the performance of open-source LLMs on OM tasks.

In LLMs4OL [7], the authors argue that with sufficient formulation, all tasks pertinent to OL fall within one of the three categories: A) Term Typing (determining a generalized type for a lexical term), B) Taxonomy Discovery (determin-
---
4         Mai et al.

ing the hierarchy between a pair of concepts), C) Non-Taxonomic Relation Ex-
traction (finding non-hierarchic relations between concepts). This task paradigm
allows them to evaluate LLMs in OL with a zero-shot prompting method. In the
work, the authors show that although LLMs may not be suitable for OL as is,
they may still be helpful when effectively fine-tuned for ontology construction.

SPIRES [4] is a successful application of LLMs to populate ontologies. It
leverages Zero-Shot Learning to extract relations between concepts in textual
corpora, then grounds the concepts using other existing ontologies in the target
domain (e.g. FoodOn or Wikidata). However, in a domain-specific setting, it is
not granted that public ontologies of the domain exist and are of high quality.""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk8> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk8> ex:chunkContent """Moskvoretskii et al. [19] use LLaMa [26] fine-tuned on WordNet, to perform
OL tasks such as taxonomy discovery, taxonomy enrichment, taxonomy construc-
tion and lexical entailment. Specifically, they provide further empirical evidence
that fine-tuning LLMs for taxonomy discovery on domains drastically increases
their performance, making them suitable for the task. The method was tested
on real domains such as the food, music and medical domains. In comparison,
our work seeks to establish whether or not domain adaptation would hold in
arbitrary domains, where the terminology is unknown to the model.

## In-Context Learning with LLMs""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk9> .

<https://dblp.org/rec/journals/corr/abs-2407-19998/chunk9> ex:chunkContent """## In-Context Learning with LLMs

It was previously observed and verified
that LLMs can learn from a few in-context examples in the form of demon-
stration. In fact, to better answer a given query, a LLM can leverage previous
examples to estimate the distribution of input-output pairs. This emergent be-
havior of LLMs [29] has become a successful learning paradigm because it no
longer requires the expensive optimization of model parameters. With respect
to our paper, four particular works pertaining to In-Context Learning (ICL)
are of high interest. Firstly, Chain-of-Thought prompting [30] forces the model
to generate intermediate steps before returning an output, which was shown to
elicit reasoning in very large language models and improve their symbolic rea-
soning performance. Secondly, Min et al. [18] show that ground-truth labels do
not significantly hurt the performance of LLMs on downstream tasks, suggesting
that models implicitly learn input-label mappings from the language modelling
objective alone. Thirdly, symbol tuning [31], the process of fine-tuning by re-
placing original labels to semantically unrelated ones, was shown to improve the
in-context learning performance of very large LMs, and effectively override prior
semantic knowledge. Finally, Wei et al. [32] show that"smaller" LLMs greatly
suffer from semantically unrelated labels in comparison to larger ones, heavily
implying that they overly rely on semantic priors of targets instead of effectively
reasoning over them. The contributions of these works justify our necessity to
verify the in-context adaptation of LLMs to domains by extending this verifica-
tion to semantically unrelated inputs (e.g. gibberish input-label mappings).

## Domain Adaptation with LLMs""" ;
    ex:next <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk10> .

<https://dblp.org/rec/journals/corr/abs-2407-10430> a dblp:Informal,
        dblp:Publication ;
    rdfs:label "Zhoutian Shao et al.: Expanding the Scope: Inductive Knowledge Graph Reasoning with Multi-Starting Progressive Propagation. (2024)" ;
    ex:hasChunk <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk1>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk10>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk11>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk12>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk13>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk14>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk15>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk16>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk17>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk18>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk19>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk2>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk20>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk21>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk22>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk23>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk24>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk25>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk26>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk27>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk28>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk29>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk3>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk30>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk31>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk32>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk33>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk4>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk5>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk6>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk7>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk8>,
        <https://dblp.org/rec/journals/corr/abs-2407-10430/chunk9> ;
    datacite:hasIdentifier [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:dblp-record ;
            litre:hasLiteralValue "journals/corr/abs-2407-10430" ],
        [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:doi ;
            litre:hasLiteralValue "10.48550/ARXIV.2407.10430" ] ;
    schema1:abstract "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new entities are constantly emerging in the real world. Inductive KG reasoning aims to predict missing facts for these new entities. Among existing models, graph neural networks (GNNs) based ones have shown promising performance for this task. However, they are still challenged by inefficient message propagation due to the distance and scalability issues. In this paper, we propose a new inductive KG reasoning model, MStar, by leveraging conditional message passing neural networks (C-MPNNs). Our key insight is to select multiple query-specific starting entities to expand the scope of progressive propagation. To propagate query-related messages to a farther area within limited steps, we subsequently design a highway layer to propagate information toward these selected starting entities. Moreover, we introduce a training strategy called LinkVerify to mitigate the impact of noisy training samples. Experimental results validate that MStar achieves superior performance compared with state-of-the-art models, especially for distant entities." ;
    schema1:keywords "Conditional message passing",
        "Inductive reasoning",
        "Knowledge graphs" ;
    owl:sameAs <http://dx.doi.org/10.48550/ARXIV.2407.10430>,
        <https://doi.org/10.48550/ARXIV.2407.10430> ;
    dblp:authoredBy <https://dblp.org/pid/291/6757>,
        <https://dblp.org/pid/383/1089>,
        <https://dblp.org/pid/52/173> ;
    dblp:bibtexType bibtex:Article ;
    dblp:createdBy <https://dblp.org/pid/291/6757>,
        <https://dblp.org/pid/383/1089>,
        <https://dblp.org/pid/52/173> ;
    dblp:documentPage <https://doi.org/10.48550/ARXIV.2407.10430> ;
    dblp:doi <https://doi.org/10.48550/ARXIV.2407.10430> ;
    dblp:hasSignature [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/383/1089> ;
            dblp:signatureDblpName "Zhoutian Shao" ;
            dblp:signatureOrdinal 1 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-10430> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/291/6757> ;
            dblp:signatureDblpName "Yuanning Cui" ;
            dblp:signatureOrdinal 2 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-10430> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/52/173> ;
            dblp:signatureDblpName "Wei Hu" ;
            dblp:signatureOrdinal 3 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-10430> ] ;
    dblp:listedOnTocPage <https://dblp.org/db/journals/corr/corr2407> ;
    dblp:numberOfCreators 3 ;
    dblp:primaryDocumentPage <https://doi.org/10.48550/ARXIV.2407.10430> ;
    dblp:publishedIn "CoRR" ;
    dblp:publishedInJournal "CoRR" ;
    dblp:publishedInJournalVolume "abs/2407.10430" ;
    dblp:publishedInStream <https://dblp.org/streams/journals/corr> ;
    dblp:title "Expanding the Scope: Inductive Knowledge Graph Reasoning with Multi-Starting Progressive Propagation." ;
    dblp:yearOfPublication "2024"^^xsd:gYear .

<https://dblp.org/rec/journals/corr/abs-2407-19998> a dblp:Informal,
        dblp:Publication ;
    rdfs:label "Huu Tan Mai et al.: Do LLMs Really Adapt to Domains? An Ontology Learning Perspective. (2024)" ;
    ex:hasChunk <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk1>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk10>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk11>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk12>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk13>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk14>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk15>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk16>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk17>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk18>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk19>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk2>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk20>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk21>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk22>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk23>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk24>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk25>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk26>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk27>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk28>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk29>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk3>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk30>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk31>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk32>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk33>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk34>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk35>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk4>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk5>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk6>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk7>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk8>,
        <https://dblp.org/rec/journals/corr/abs-2407-19998/chunk9> ;
    datacite:hasIdentifier [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:dblp-record ;
            litre:hasLiteralValue "journals/corr/abs-2407-19998" ],
        [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:doi ;
            litre:hasLiteralValue "10.48550/ARXIV.2407.19998" ] ;
    schema1:abstract "Large Language Models (LLMs) have demonstrated unprecedented prowess across various natural language processing tasks in various application domains. Recent studies show that LLMs can be leveraged to perform lexical semantic tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL). However, it has not effectively been verified whether their success is due to their ability to reason over unstructured or semi-structured data, or their effective learning of linguistic patterns and senses alone. This unresolved question is particularly crucial when dealing with domain-specific data, where the lexical senses and their meaning can completely differ from what a LLM has learned during its training stage. This paper investigates the following question: Do LLMs really adapt to domains and remain consistent in the extraction of structured knowledge, or do they only learn lexical senses instead of reasoning? To answer this question and, we devise a controlled experiment setup that uses WordNet to synthesize parallel corpora, with English and gibberish terms. We examine the differences in the outputs of LLMs for each corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical results show that, while adapting to the gibberish corpora, off-the-shelf LLMs do not consistently reason over semantic relationships between concepts, and instead leverage senses and their frame. However, fine-tuning improves the performance of LLMs on lexical semantic tasks even when the domain-specific terms are arbitrary and unseen during pre-training, hinting at the applicability of pre-trained LLMs for OL." ;
    schema1:keywords "LLMs",
        "domain adaptation",
        "ontology learning" ;
    owl:sameAs <http://dx.doi.org/10.48550/ARXIV.2407.19998>,
        <https://doi.org/10.48550/ARXIV.2407.19998> ;
    dblp:authoredBy <https://dblp.org/pid/198/3329>,
        <https://dblp.org/pid/367/5185>,
        <https://dblp.org/pid/39/4064> ;
    dblp:bibtexType bibtex:Article ;
    dblp:createdBy <https://dblp.org/pid/198/3329>,
        <https://dblp.org/pid/367/5185>,
        <https://dblp.org/pid/39/4064> ;
    dblp:documentPage <https://doi.org/10.48550/ARXIV.2407.19998> ;
    dblp:doi <https://doi.org/10.48550/ARXIV.2407.19998> ;
    dblp:hasSignature [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/367/5185> ;
            dblp:signatureDblpName "Huu Tan Mai" ;
            dblp:signatureOrdinal 1 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-19998> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/198/3329> ;
            dblp:signatureDblpName "Cuong Xuan Chu" ;
            dblp:signatureOrdinal 2 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-19998> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/39/4064> ;
            dblp:signatureDblpName "Heiko Paulheim" ;
            dblp:signatureOrdinal 3 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-19998> ] ;
    dblp:listedOnTocPage <https://dblp.org/db/journals/corr/corr2407> ;
    dblp:numberOfCreators 3 ;
    dblp:primaryDocumentPage <https://doi.org/10.48550/ARXIV.2407.19998> ;
    dblp:publishedIn "CoRR" ;
    dblp:publishedInJournal "CoRR" ;
    dblp:publishedInJournalVolume "abs/2407.19998" ;
    dblp:publishedInStream <https://dblp.org/streams/journals/corr> ;
    dblp:title "Do LLMs Really Adapt to Domains? An Ontology Learning Perspective." ;
    dblp:yearOfPublication "2024"^^xsd:gYear .

<https://dblp.org/rec/journals/corr/abs-2407-16127> a dblp:Informal,
        dblp:Publication ;
    rdfs:label "Yang Liu et al.: Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion. (2024)" ;
    ex:hasChunk <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk1>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk10>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk11>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk12>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk13>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk14>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk15>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk16>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk17>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk18>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk19>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk2>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk20>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk21>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk22>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk23>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk24>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk25>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk26>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk27>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk28>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk29>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk3>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk30>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk31>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk32>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk33>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk34>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk35>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk36>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk4>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk5>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk6>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk7>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk8>,
        <https://dblp.org/rec/journals/corr/abs-2407-16127/chunk9> ;
    datacite:hasIdentifier [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:dblp-record ;
            litre:hasLiteralValue "journals/corr/abs-2407-16127" ],
        [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:doi ;
            litre:hasLiteralValue "10.48550/ARXIV.2407.16127" ] ;
    schema1:abstract "Traditional knowledge graph (KG) completion models learn embeddings to predict missing facts. Recent works attempt to complete KGs in a text-generation manner with large language models (LLMs). However, they need to ground the output of LLMs to KG entities, which inevitably brings errors. In this paper, we present a finetuning framework, DIFT, aiming to unleash the KG completion ability of LLMs and avoid grounding errors. Given an incomplete fact, DIFT employs a lightweight model to obtain candidate entities and finetunes an LLM with discrimination instructions to select the correct one from the given candidates. To improve performance while reducing instruction data, DIFT uses a truncated sampling method to select useful facts for finetuning and injects KG embeddings into the LLM. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed framework." ;
    schema1:keywords "Instruction tuning",
        "Knowledge graph completion",
        "Large language model" ;
    owl:sameAs <http://dx.doi.org/10.48550/ARXIV.2407.16127>,
        <https://doi.org/10.48550/ARXIV.2407.16127> ;
    dblp:authoredBy <https://dblp.org/pid/186/9718>,
        <https://dblp.org/pid/311/4971>,
        <https://dblp.org/pid/51/3710>,
        <https://dblp.org/pid/52/173> ;
    dblp:bibtexType bibtex:Article ;
    dblp:createdBy <https://dblp.org/pid/186/9718>,
        <https://dblp.org/pid/311/4971>,
        <https://dblp.org/pid/51/3710>,
        <https://dblp.org/pid/52/173> ;
    dblp:documentPage <https://doi.org/10.48550/ARXIV.2407.16127> ;
    dblp:doi <https://doi.org/10.48550/ARXIV.2407.16127> ;
    dblp:hasSignature [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/51/3710> ;
            dblp:signatureDblpName "Yang Liu" ;
            dblp:signatureOrdinal 1 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-16127> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/311/4971> ;
            dblp:signatureDblpName "Xiaobin Tian" ;
            dblp:signatureOrdinal 2 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-16127> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/186/9718> ;
            dblp:signatureDblpName "Zequn Sun" ;
            dblp:signatureOrdinal 3 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-16127> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/52/173> ;
            dblp:signatureDblpName "Wei Hu" ;
            dblp:signatureOrdinal 4 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2407-16127> ] ;
    dblp:listedOnTocPage <https://dblp.org/db/journals/corr/corr2407> ;
    dblp:numberOfCreators 4 ;
    dblp:primaryDocumentPage <https://doi.org/10.48550/ARXIV.2407.16127> ;
    dblp:publishedIn "CoRR" ;
    dblp:publishedInJournal "CoRR" ;
    dblp:publishedInJournalVolume "abs/2407.16127" ;
    dblp:publishedInStream <https://dblp.org/streams/journals/corr> ;
    dblp:title "Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion." ;
    dblp:yearOfPublication "2024"^^xsd:gYear .

<https://dblp.org/rec/journals/corr/abs-2401-07237> a dblp:Informal,
        dblp:Publication ;
    rdfs:label "Somin Wadhwa et al.: Distilling Event Sequence Knowledge From Large Language Models. (2024)" ;
    ex:hasChunk <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk1>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk10>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk11>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk12>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk13>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk14>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk15>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk16>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk17>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk18>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk19>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk2>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk20>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk21>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk22>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk23>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk24>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk25>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk26>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk27>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk28>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk29>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk3>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk30>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk31>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk32>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk33>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk34>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk35>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk36>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk37>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk38>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk39>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk4>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk40>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk5>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk6>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk7>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk8>,
        <https://dblp.org/rec/journals/corr/abs-2401-07237/chunk9> ;
    datacite:hasIdentifier [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:dblp-record ;
            litre:hasLiteralValue "journals/corr/abs-2401-07237" ],
        [ a datacite:Identifier,
                datacite:ResourceIdentifier ;
            datacite:usesIdentifierScheme datacite:doi ;
            litre:hasLiteralValue "10.48550/ARXIV.2401.07237" ] ;
    schema1:abstract "Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex structured knowledge from pattern mining and probabilistic event models. We release our sequence generation code and evaluation framework, as well as corpus of event sequence data." ;
    schema1:keywords "Knowledge Distillation",
        "Knowledge Graphs",
        "Large Language Models" ;
    owl:sameAs <http://dx.doi.org/10.48550/ARXIV.2401.07237>,
        <https://doi.org/10.48550/ARXIV.2401.07237> ;
    dblp:authoredBy <https://dblp.org/pid/265/5925>,
        <https://dblp.org/pid/75/2470>,
        <https://dblp.org/pid/98/5604>,
        <https://dblp.org/pid/b/KenBarker2>,
        <https://dblp.org/pid/h/OktieHassanzadeh> ;
    dblp:bibtexType bibtex:Article ;
    dblp:createdBy <https://dblp.org/pid/265/5925>,
        <https://dblp.org/pid/75/2470>,
        <https://dblp.org/pid/98/5604>,
        <https://dblp.org/pid/b/KenBarker2>,
        <https://dblp.org/pid/h/OktieHassanzadeh> ;
    dblp:documentPage <https://doi.org/10.48550/ARXIV.2401.07237> ;
    dblp:doi <https://doi.org/10.48550/ARXIV.2401.07237> ;
    dblp:hasSignature [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/265/5925> ;
            dblp:signatureDblpName "Somin Wadhwa" ;
            dblp:signatureOrdinal 1 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2401-07237> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/h/OktieHassanzadeh> ;
            dblp:signatureDblpName "Oktie Hassanzadeh" ;
            dblp:signatureOrcid <https://orcid.org/0000-0001-5307-9857> ;
            dblp:signatureOrdinal 2 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2401-07237> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/98/5604> ;
            dblp:signatureDblpName "Debarun Bhattacharjya" ;
            dblp:signatureOrdinal 3 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2401-07237> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/b/KenBarker2> ;
            dblp:signatureDblpName "Ken Barker 0002" ;
            dblp:signatureOrdinal 4 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2401-07237> ],
        [ a dblp:AuthorSignature,
                dblp:Signature ;
            dblp:signatureCreator <https://dblp.org/pid/75/2470> ;
            dblp:signatureDblpName "Jian Ni" ;
            dblp:signatureOrdinal 5 ;
            dblp:signaturePublication <https://dblp.org/rec/journals/corr/abs-2401-07237> ] ;
    dblp:listedOnTocPage <https://dblp.org/db/journals/corr/corr2401> ;
    dblp:numberOfCreators 5 ;
    dblp:primaryDocumentPage <https://doi.org/10.48550/ARXIV.2401.07237> ;
    dblp:publishedIn "CoRR" ;
    dblp:publishedInJournal "CoRR" ;
    dblp:publishedInJournalVolume "abs/2401.07237" ;
    dblp:publishedInStream <https://dblp.org/streams/journals/corr> ;
    dblp:title "Distilling Event Sequence Knowledge From Large Language Models." ;
    dblp:yearOfPublication "2024"^^xsd:gYear .

